{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LCA of Germany's grid expansion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bw2data as bd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd.projects.set_current(\"paper_plca_grid_expansion_rev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do quite a few calculations here. To save some time, e.g., if only interested in the plots, set the \"recalculate\" variable to False.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = ('IPCC 2021', 'climate change', 'GWP 100a, incl. H and bio CO2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recalculate = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GWI of static vs prospective expansion (Figure 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_expansion_nodes = sorted([node for node in bd.Database(\"grid_expansion_static\")])\n",
    "prospective_expansion_nodes_base = sorted(\n",
    "    [\n",
    "        node\n",
    "        for node in bd.Database(\"grid_expansion_prospective\")\n",
    "        if \"NPi\" in node[\"name\"]\n",
    "    ]\n",
    ")\n",
    "prospective_expansion_nodes_PkBudg1000 = sorted(\n",
    "    [\n",
    "        node\n",
    "        for node in bd.Database(\"grid_expansion_prospective\")\n",
    "        if \"PkBudg1000\" in node[\"name\"]\n",
    "    ]\n",
    ")\n",
    "prospective_expansion_nodes_PkBudg650 = sorted(\n",
    "    [\n",
    "        node\n",
    "        for node in bd.Database(\"grid_expansion_prospective\")\n",
    "        if \"PkBudg650\" in node[\"name\"]\n",
    "    ]\n",
    ")\n",
    "prospective_expansion_nodes = (\n",
    "    prospective_expansion_nodes_base\n",
    "    + prospective_expansion_nodes_PkBudg1000\n",
    "    + prospective_expansion_nodes_PkBudg650\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the LCA results for the static and prospective cases:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bw2calc as bc\n",
    "import json\n",
    "\n",
    "if recalculate:\n",
    "    # static expansion\n",
    "    results_components = {}\n",
    "    lca = bc.LCA(demand={static_expansion_nodes[0]: 1}, method=method)\n",
    "    lca.lci(factorize=True)\n",
    "    for node in static_expansion_nodes:\n",
    "        component_results = {}\n",
    "        for exc in node.technosphere():\n",
    "            lca.lcia(demand={exc.input.id: exc.amount})\n",
    "            component_results[exc.input[\"name\"]] = lca.score\n",
    "        results_components[node[\"name\"]] = component_results\n",
    "\n",
    "    # prospective expansion\n",
    "    lca = bc.LCA(demand={prospective_expansion_nodes[0]: 1}, method=method)\n",
    "    lca.lci(factorize=True)\n",
    "    for node in prospective_expansion_nodes:\n",
    "        component_results = {}\n",
    "        for exc in node.technosphere():\n",
    "            lca.lcia(demand={exc.input.id: exc.amount})\n",
    "            component_results[exc.input[\"name\"]] = lca.score\n",
    "        results_components[node[\"name\"]] = component_results\n",
    "\n",
    "    json.dump(results_components, open(\"data/results/expansion_component_results_rev1.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results_components = json.load(open(\"data/results/expansion_component_results_rev1.json\"))\n",
    "df_components = pd.DataFrame(results_components)\n",
    "\n",
    "df_components.columns = pd.MultiIndex.from_tuples(\n",
    "    [(col.split(\"_\", 2)[1], col.rsplit(\"_\", 1)[1]) for col in df_components.columns]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting setup:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RWTHColors import ColorManager\n",
    "\n",
    "cm = ColorManager()\n",
    "\n",
    "plot_colors = []\n",
    "plot_colors_light = []\n",
    "color_list = [\n",
    "    cm.RWTHBlau,\n",
    "    #   cm.RWTHPetrol,\n",
    "    cm.RWTHTuerkis,\n",
    "    cm.RWTHGruen,\n",
    "    cm.RWTHMaiGruen,\n",
    "    cm.RWTHOrange,\n",
    "    cm.RWTHRot,\n",
    "    # cm.RWTHBordeaux,\n",
    "    # cm.RWTHViolett,\n",
    "    cm.RWTHLila,\n",
    "]\n",
    "intensities = [100, 50, 10]\n",
    "\n",
    "for intensity in intensities:\n",
    "    for color in color_list:\n",
    "        plot_colors.append(color.p(intensity))\n",
    "\n",
    "for intensity in [75, 50, 10]:\n",
    "    for color in color_list:\n",
    "        plot_colors_light.append(color.p(intensity))\n",
    "\n",
    "plot_colors_expansion = [\n",
    "    cm.RWTHBlau.p(100),\n",
    "    \"#2069AB\",\n",
    "    cm.RWTHBlau.p(75),\n",
    "    cm.RWTHBlau.p(50),\n",
    "    cm.RWTHBlau.p(25),\n",
    "]\n",
    "\n",
    "plot_colors_components = [\n",
    "    cm.RWTHPetrol.p(100),  # overhead lines\n",
    "    cm.RWTHGruen.p(100),  # cables\n",
    "    cm.RWTHMagenta.p(75),  # switchgears\n",
    "    cm.RWTHLila.p(100),  # transformers\n",
    "    cm.RWTHViolett.p(100),  # substations\n",
    "]\n",
    "\n",
    "plot_colors_materials = [\n",
    "    cm.RWTHOrange.p(100),  # aluminium\n",
    "    cm.RWTHTuerkis.p(100),  # iron & steel\n",
    "    cm.RWTHRot.p(75),  # copper\n",
    "    cm.RWTHMaiGruen.p(100),  # SF6\n",
    "    cm.RWTHGelb.p(75),  # plastics\n",
    "    cm.RWTHPetrol.p(50),  # concrete\n",
    "    cm.RWTHSchwarz.p(50),  # other materials\n",
    "]\n",
    "\n",
    "plot_colors_processes = [\n",
    "    cm.RWTHGelb.p(100),  # electricity\n",
    "    cm.RWTHTuerkis.p(50),  # iron & steel (process emissions)\n",
    "    cm.RWTHRot.p(100),  # heat\n",
    "    cm.RWTHBlau.p(50),  # transport\n",
    "    cm.RWTHSchwarz.p(75),  # coal\n",
    "    cm.RWTHMaiGruen.p(100),  # SF6\n",
    "    cm.RWTHOrange.p(50),  # aluminium (process emissions)\n",
    "    cm.RWTHViolett.p(75),  # clinker\n",
    "    cm.RWTHSchwarz.p(50),  # other processes\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sankey = pd.read_csv(\"data/results/sankey_data.csv\")\n",
    "score_grid_status_quo = df_sankey[df_sankey[\"target\"] == \"grid status quo\"][\n",
    "    \"value\"\n",
    "].sum()\n",
    "data_base = {\"Score\": [score_grid_status_quo]}\n",
    "df_base = pd.DataFrame(data_base)\n",
    "\n",
    "data_static = {\"Score\": df_components[\"static\"].sum().values / 1e9}\n",
    "df_static = pd.DataFrame(data_static)\n",
    "\n",
    "data_NPi = {\"Score\": df_components[\"NPi\"].sum().values / 1e9}\n",
    "df_NPi = pd.DataFrame(data_NPi)\n",
    "\n",
    "data_PkBudg1000 = {\"Score\": df_components[\"PkBudg1000\"].sum().values / 1e9}\n",
    "df_PkBudg1000 = pd.DataFrame(data_PkBudg1000)\n",
    "\n",
    "data_PkBudg650 = {\"Score\": df_components[\"PkBudg650\"].sum().values / 1e9}\n",
    "df_PkBudg650 = pd.DataFrame(data_PkBudg650)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GWI contributions on the expansion period, component, material and process level (Figure 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components & Expansion Periods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "df_sankey = pd.read_csv(\"data/results/sankey_data.csv\")\n",
    "score_grid_status_quo = df_sankey[df_sankey[\"target\"] == \"grid status quo\"][\n",
    "    \"value\"\n",
    "].sum()\n",
    "data_base = {\"Score\": [score_grid_status_quo]}\n",
    "df_base = pd.DataFrame(data_base)\n",
    "\n",
    "data_static = {\"Score\": df_components[\"static\"].sum().values / 1e9}\n",
    "df_static = pd.DataFrame(data_static)\n",
    "\n",
    "data_NPi = {\"Score\": df_components[\"NPi\"].sum().values / 1e9}\n",
    "df_NPi = pd.DataFrame(data_NPi)\n",
    "\n",
    "data_PkBudg1000 = {\"Score\": df_components[\"PkBudg1000\"].sum().values / 1e9}\n",
    "df_PkBudg1000 = pd.DataFrame(data_PkBudg1000)\n",
    "\n",
    "data_PkBudg650 = {\"Score\": df_components[\"PkBudg650\"].sum().values / 1e9}\n",
    "df_PkBudg650 = pd.DataFrame(data_PkBudg650)\n",
    "\n",
    "# Aggregate expansion to 2037 & 2040\n",
    "for df in [df_static, df_NPi, df_PkBudg1000, df_PkBudg650]:\n",
    "    df.at[3, \"Score\"] = df[\"Score\"].iloc[3:5].sum()\n",
    "    df.drop(index=4, inplace=True)\n",
    "    \n",
    "df_expansion_periods = pd.DataFrame({\n",
    "    \"BAU\": df_static[\"Score\"].values,\n",
    "    \"3.0°C scenario\": df_NPi[\"Score\"].values,\n",
    "    \"2.0°C scenario\": df_PkBudg1000[\"Score\"].values,\n",
    "    \"1.5°C scenario\": df_PkBudg650[\"Score\"].values,\n",
    "})\n",
    "\n",
    "df_expansion_periods.loc[\"SPACING\"] = [\n",
    "    0,\n",
    "    df_expansion_periods[\"BAU\"].sum() - df_expansion_periods[\"3.0°C scenario\"].sum(),\n",
    "    df_expansion_periods[\"BAU\"].sum() - df_expansion_periods[\"2.0°C scenario\"].sum(),\n",
    "    df_expansion_periods[\"BAU\"].sum() - df_expansion_periods[\"1.5°C scenario\"].sum(),\n",
    "]\n",
    "\n",
    "df_expansion_periods.index = [\"2023 → 2025\", \"2025 → 2030\", \"2030 → 2035\", \"2035 → 2040\", \"2040 → 2045\", \"SPACING\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expansion_periods[df_expansion_periods.index != \"SPACING\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = pd.DataFrame(\n",
    "    {\n",
    "        \"static\": df_components[\"static\"].T.sum().values,\n",
    "        \"Base\": df_components[\"NPi\"].T.sum().values,\n",
    "        \"PkBudg1000\": df_components[\"PkBudg1000\"].T.sum().values,\n",
    "        \"PkBudg650\": df_components[\"PkBudg650\"].T.sum().values,\n",
    "    },\n",
    "    index=df_components.index,\n",
    ")\n",
    "\n",
    "component_groups = {\n",
    "    \"Overhead line\": \"overhead lines\",\n",
    "    \"cable\": \"cables\",\n",
    "    \"Transformer\": \"transformers\",\n",
    "    \"switchgear\": \"switchgears\",\n",
    "    \"Substation\": \"substations\",\n",
    "}\n",
    "\n",
    "\n",
    "def map_labels(labels, groups, other_label=\"other\"):\n",
    "    new_labels = {}\n",
    "    for label in labels:\n",
    "        for key, group in groups.items():\n",
    "            if key in label:\n",
    "                new_labels[label] = group\n",
    "                break\n",
    "        else:\n",
    "            new_labels[label] = other_label\n",
    "    return new_labels\n",
    "\n",
    "\n",
    "components.index = components.index.map(\n",
    "    map_labels(components.index, component_groups, other_label=\"other components\")\n",
    ")\n",
    "components = components.groupby(level=0).sum()\n",
    "\n",
    "# sorting\n",
    "components[\"sort_key\"] = components.index != \"other components\"\n",
    "components = components.sort_values(by=[\"sort_key\", \"static\"], ascending=[True, True])\n",
    "components = components.drop(columns=\"sort_key\")\n",
    "\n",
    "components.loc[\"SPACING\"] = [\n",
    "    0,\n",
    "    components[\"static\"].sum() - components[\"Base\"].sum(),\n",
    "    components[\"static\"].sum() - components[\"PkBudg1000\"].sum(),\n",
    "    components[\"static\"].sum() - components[\"PkBudg650\"].sum(),\n",
    "]\n",
    "components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Materials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bw2calc as bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if recalculate:\n",
    "    results_materials = {}\n",
    "    bioflows = {}\n",
    "    material_sums = {}\n",
    "\n",
    "    for node in static_expansion_nodes + prospective_expansion_nodes:\n",
    "        component_results = {}\n",
    "        lca = bc.LCA({node: 1}, method=method)\n",
    "        lca.lci(factorize=True)\n",
    "        for component in node.technosphere():\n",
    "            material_results = {}\n",
    "            for material in list(component.input.technosphere()):\n",
    "                lca.lcia(demand={material.input.id: material.amount * component.amount})\n",
    "                material_results[material.input[\"name\"]] = lca.score\n",
    "            for bioflow in component.input.biosphere():\n",
    "                bioflows[component.input[\"name\"]] = bioflow\n",
    "            component_results[component.input[\"name\"]] = material_results\n",
    "\n",
    "        results_materials[node[\"name\"]] = component_results\n",
    "\n",
    "    # Sum up materials\n",
    "    for scenario, comp in results_materials.items():\n",
    "        for component, materials in comp.items():\n",
    "            for material, value in materials.items():\n",
    "                if material not in material_sums:\n",
    "                    material_sums[material] = {}\n",
    "                if scenario not in material_sums[material]:\n",
    "                    material_sums[material][scenario] = 0\n",
    "                material_sums[material][scenario] += value\n",
    "\n",
    "    with open(\"data/results/expansion_materials_results_remind-eu_premise_gwp.json\", \"w\") as f:\n",
    "        json.dump(material_sums, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_materials = json.load(open(\"data/results/expansion_materials_results_remind-eu_premise_gwp.json\"))\n",
    "materials = pd.DataFrame.from_dict(results_materials, orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add SF6 leakage\n",
    "distributed_components = json.load(open(\"data/results/distributed_components.json\"))\n",
    "\n",
    "sf6_row = {}\n",
    "\n",
    "for col, year in zip(\n",
    "    materials.columns,\n",
    "    [\n",
    "        2023,\n",
    "        2025,\n",
    "        2030,\n",
    "        2035,\n",
    "        2037,\n",
    "        2040,\n",
    "        2023,\n",
    "        2025,\n",
    "        2030,\n",
    "        2035,\n",
    "        2037,\n",
    "        2040,\n",
    "        2023,\n",
    "        2025,\n",
    "        2030,\n",
    "        2035,\n",
    "        2037,\n",
    "        2040,\n",
    "        2023,\n",
    "        2025,\n",
    "        2030,\n",
    "        2035,\n",
    "        2037,\n",
    "        2040,\n",
    "    ],\n",
    "):\n",
    "    sf6_impact = (\n",
    "        distributed_components[str(year)][\"gas_insulated_switchgear_420kv\"]\n",
    "        * 28.6\n",
    "        * 25200\n",
    "    )  # emissions and CF of sf6\n",
    "    sf6_row[col] = sf6_impact\n",
    "\n",
    "materials.loc[\"sulfur hexafluoride\"] = sf6_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "material_groups = {\n",
    "    \"aluminium\": \"aluminum\",\n",
    "    \"copper\": \"copper\",\n",
    "    \"iron\": \"iron & steel\",\n",
    "    \"steel\": \"iron & steel\",\n",
    "    \"concrete\": \"concrete\",\n",
    "    \"cement\": \"concrete\",\n",
    "    \"sulfur hexafluoride\": \"SF6\",\n",
    "    \"polyethylene\": \"plastics\",\n",
    "    \"polypropylene\": \"plastics\",\n",
    "    \"plastic\": \"plastics\",\n",
    "}\n",
    "\n",
    "materials.index = materials.index.map(\n",
    "    map_labels(materials.index, material_groups, other_label=\"other materials\")\n",
    ")\n",
    "materials = materials.groupby(level=0).sum()\n",
    "\n",
    "materials.columns = pd.MultiIndex.from_tuples(\n",
    "    [(col.split(\"_\", 2)[1], col.rsplit(\"_\", 1)[1]) for col in materials.columns]\n",
    ")\n",
    "\n",
    "materials = pd.DataFrame(\n",
    "    {\n",
    "        \"static\": materials[\"static\"].T.sum().values,\n",
    "        \"Base\": materials[\"NPi\"].T.sum().values,\n",
    "        \"PkBudg1000\": materials[\"PkBudg1000\"].T.sum().values,\n",
    "        \"PkBudg650\": materials[\"PkBudg650\"].T.sum().values,\n",
    "    },\n",
    "    index=materials.index,\n",
    ")\n",
    "\n",
    "# sorting\n",
    "materials[\"sort_key\"] = materials.index != \"other materials\"\n",
    "materials = materials.sort_values(by=[\"sort_key\", \"static\"], ascending=[True, True])\n",
    "materials = materials.drop(columns=\"sort_key\")\n",
    "\n",
    "materials.loc[\"SPACING\"] = [\n",
    "    0,\n",
    "    materials[\"static\"].sum() - materials[\"Base\"].sum(),\n",
    "    materials[\"static\"].sum() - materials[\"PkBudg1000\"].sum(),\n",
    "    materials[\"static\"].sum() - materials[\"PkBudg650\"].sum(),\n",
    "]\n",
    "materials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bw2analyzer as ba\n",
    "\n",
    "if recalculate:\n",
    "    process_results = {}\n",
    "    product_impacts_static = {}\n",
    "    product_impacts_base = {}\n",
    "    product_impacts_PkBudg1000 = {}\n",
    "    product_impacts_PkBudg650 = {}\n",
    "\n",
    "    lca = bc.LCA(\n",
    "        {static_expansion_nodes[0]: 1}, method=method\n",
    "    )  # just to build the matrices\n",
    "    lca.lci(factorize=True)\n",
    "    for node in static_expansion_nodes:\n",
    "        lca.lcia(demand={node.id: 1})\n",
    "        top_processes = ba.ContributionAnalysis().annotated_top_processes(\n",
    "            lca, limit=1000\n",
    "        )\n",
    "        for process in top_processes:\n",
    "            if process[2][\"reference product\"] in product_impacts_static:\n",
    "                product_impacts_static[process[2][\"reference product\"]] += process[0]\n",
    "            else:\n",
    "                product_impacts_static[process[2][\"reference product\"]] = process[0]\n",
    "    process_results[\"static\"] = product_impacts_static\n",
    "\n",
    "    for node in prospective_expansion_nodes_base:\n",
    "        lca = bc.LCA({node: 1}, method=method)\n",
    "        lca.lci()\n",
    "        lca.lcia()\n",
    "        top_processes = ba.ContributionAnalysis().annotated_top_processes(\n",
    "            lca, limit=1000\n",
    "        )\n",
    "        for process in top_processes:\n",
    "            if process[2][\"reference product\"] in product_impacts_base:\n",
    "                product_impacts_base[process[2][\"reference product\"]] += process[0]\n",
    "            else:\n",
    "                product_impacts_base[process[2][\"reference product\"]] = process[0]\n",
    "    process_results[\"Base\"] = product_impacts_base\n",
    "\n",
    "    for node in prospective_expansion_nodes_PkBudg1000:\n",
    "        lca = bc.LCA({node: 1}, method=method)\n",
    "        lca.lci()\n",
    "        lca.lcia()\n",
    "        top_processes = ba.ContributionAnalysis().annotated_top_processes(\n",
    "            lca, limit=1000\n",
    "        )\n",
    "        for process in top_processes:\n",
    "            if process[2][\"reference product\"] in product_impacts_PkBudg1000:\n",
    "                product_impacts_PkBudg1000[process[2][\"reference product\"]] += process[0]\n",
    "            else:\n",
    "                product_impacts_PkBudg1000[process[2][\"reference product\"]] = process[0]\n",
    "    process_results[\"PkBudg1000\"] = product_impacts_PkBudg1000\n",
    "\n",
    "    for node in prospective_expansion_nodes_PkBudg650:\n",
    "        lca = bc.LCA({node: 1}, method=method)\n",
    "        lca.lci()\n",
    "        lca.lcia()\n",
    "        top_processes = ba.ContributionAnalysis().annotated_top_processes(\n",
    "            lca, limit=1000\n",
    "        )\n",
    "        for process in top_processes:\n",
    "            if process[2][\"reference product\"] in product_impacts_PkBudg650:\n",
    "                product_impacts_PkBudg650[process[2][\"reference product\"]] += process[0]\n",
    "            else:\n",
    "                product_impacts_PkBudg650[process[2][\"reference product\"]] = process[0]\n",
    "    process_results[\"PkBudg650\"] = product_impacts_PkBudg650\n",
    "\n",
    "    with open(\"data/results/expansion_process_results_remind-eu_premise_gwp.json\", \"w\") as f:\n",
    "        json.dump(process_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "process_results = json.load(open(\"data/results/expansion_process_results_remind-eu_premise_gwp.json\"))\n",
    "processes = pd.DataFrame(process_results)\n",
    "\n",
    "product_groups = {\n",
    "    \"electricity\": \"electricity\",\n",
    "    \"heat\": \"heat\",\n",
    "    \"transport\": \"transport\",\n",
    "    \"aluminium\": \"aluminum (process emissions)\",\n",
    "    \"iron\": \"iron & steel (process emissions)\",\n",
    "    \"steel\": \"iron & steel (process emissions)\",\n",
    "    \"coal\": \"coal\",\n",
    "    \"coke\": \"coal\",\n",
    "    \"clinker\": \"clinker\",\n",
    "    \"diesel\": \"transport\",\n",
    "    \"Gas insulated switchgear\": \"SF6\",\n",
    "    \"sulfur hexafluoride\": \"SF6\",\n",
    "}\n",
    "\n",
    "processes.index = processes.index.map(\n",
    "    map_labels(processes.index, product_groups, other_label=\"other processes\")\n",
    ")\n",
    "\n",
    "processes = processes.groupby(level=0).sum()\n",
    "\n",
    "# sorting\n",
    "processes[\"sort_key\"] = processes.index != \"other processes\"\n",
    "processes = processes.sort_values(by=[\"sort_key\", \"static\"], ascending=[True, True])\n",
    "processes = processes.drop(columns=\"sort_key\")\n",
    "\n",
    "processes.loc[\"SPACING\"] = [\n",
    "    0,\n",
    "    processes[\"static\"].sum() - processes[\"Base\"].sum(),\n",
    "    processes[\"static\"].sum() - processes[\"PkBudg1000\"].sum(),\n",
    "    processes[\"static\"].sum() - processes[\"PkBudg650\"].sum(),\n",
    "]\n",
    "processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def round_to_percentages(df):\n",
    "    def adjust_to_100(absolute_values):\n",
    "        total = absolute_values.sum()\n",
    "        percentages = (absolute_values / total * 100).round()\n",
    "        difference = 100 - percentages.sum()\n",
    "        if difference != 0:\n",
    "            idx = absolute_values.index[\n",
    "                absolute_values.index.str.contains(\"other\")\n",
    "            ].tolist()\n",
    "            if not idx:  # If no \"other\" index, use the largest decimal part\n",
    "                # Assign the difference to the index with the highest fractional part\n",
    "                fractions = (absolute_values / total * 100) % 1\n",
    "                idx = fractions.idxmax()\n",
    "            else:\n",
    "                idx = idx[0]\n",
    "            percentages.at[idx] += difference\n",
    "\n",
    "        return percentages\n",
    "\n",
    "    rounded_df = df.apply(adjust_to_100, axis=0).astype(int)\n",
    "    assert (\n",
    "        rounded_df.sum() == np.array([100, 100, 100, 100])\n",
    "    ).all()  # check rounding errors\n",
    "    return rounded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "plt.style.use(\"rwth\")\n",
    "\n",
    "expansion_plot = round_to_percentages(df_expansion_periods)\n",
    "components_plot = round_to_percentages(components)\n",
    "materials_plot = round_to_percentages(materials)\n",
    "processes_plot = round_to_percentages(processes)\n",
    "\n",
    "scenario_label_mapping = {\n",
    "    \"static\": \"BAU\",\n",
    "    \"Base\": \"3.0°C scenario\",\n",
    "    \"PkBudg1000\": \"2.0°C scenario\",\n",
    "    \"PkBudg650\": \"1.5°C scenario\",\n",
    "}\n",
    "for df in [expansion_plot, components_plot, materials_plot, processes_plot]:\n",
    "    df.columns = df.columns.map(lambda x: scenario_label_mapping.get(x, x))\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=4, ncols=1, figsize=(14, 10), sharex=True)\n",
    "plt.subplots_adjust(hspace=0.6)\n",
    "\n",
    "general_plot_kwargs = dict(\n",
    "    kind=\"barh\",\n",
    "    stacked=True,\n",
    "    width=0.7,\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=0.5,\n",
    ")\n",
    "\n",
    "expansion_colors = plot_colors_expansion + [\"none\"]\n",
    "expansion_plot.T.plot(ax=axes[0], color=expansion_colors, **general_plot_kwargs)\n",
    "\n",
    "component_colors = [\"none\"] + plot_colors_components\n",
    "components_plot.T.plot(ax=axes[1], color=component_colors[::-1], **general_plot_kwargs)\n",
    "\n",
    "material_colors = [\"none\"] + plot_colors_materials\n",
    "materials_plot.T.plot(ax=axes[2], color=material_colors[::-1], **general_plot_kwargs)\n",
    "\n",
    "process_colors = [\"none\"] + plot_colors_processes\n",
    "processes_plot.T.plot(ax=axes[3], color=process_colors[::-1], **general_plot_kwargs)\n",
    "\n",
    "for idx, (ax, data) in enumerate(\n",
    "    zip(axes, [expansion_plot, components_plot, materials_plot, processes_plot])\n",
    "):\n",
    "    ax.invert_yaxis()\n",
    "    ax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    handles, labels = zip(*[(h, l) for h, l in zip(handles, labels) if l != \"SPACING\"])\n",
    "    ax.legend(\n",
    "        handles[::-1],\n",
    "        labels[::-1],\n",
    "        loc=\"center left\",\n",
    "        bbox_to_anchor=(1, 0.5),\n",
    "        frameon=False,\n",
    "    )\n",
    "\n",
    "    ax.xaxis.grid(True)\n",
    "    ax.set_axisbelow(True)\n",
    "\n",
    "    totals = data.sum(axis=1)\n",
    "\n",
    "    # Adding percentage labels to each bar\n",
    "    for bar in ax.patches:\n",
    "        label_x_pos = bar.get_x() + bar.get_width() / 2 \n",
    "        label_y_pos = bar.get_y() + bar.get_height() / 2\n",
    "        total_width = sum(\n",
    "            [p.get_width() for p in ax.patches if p.get_y() == bar.get_y()]\n",
    "        )  # Sum width of all bars in this row\n",
    "        percentage = bar.get_width()\n",
    "        percentage_text = f\"{round(percentage)}\"\n",
    "        if percentage > 0:\n",
    "            rotation = 0\n",
    "            fontsize = 10\n",
    "            bbox_props = dict(\n",
    "                boxstyle=\"round,pad=0.2\", ec=\"none\", fc=\"white\", alpha=0.6\n",
    "            )\n",
    "            if bar in ax.patches[-4:]:\n",
    "                bbox_props = dict(\n",
    "                    boxstyle=\"round,pad=0.2\", ec=\"none\", fc=\"white\", alpha=1\n",
    "                )\n",
    "                if percentage < 5:\n",
    "                    label_x_pos = label_x_pos + 0.5\n",
    "\n",
    "            ax.text(\n",
    "                label_x_pos,\n",
    "                label_y_pos,\n",
    "                percentage_text,\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                rotation=rotation,\n",
    "                fontsize=fontsize,\n",
    "                bbox=bbox_props,\n",
    "            )\n",
    "\n",
    "    # arrows from right end of bars\n",
    "    for bar in ax.patches[-3:]:\n",
    "        bar.set_edgecolor(\"none\")\n",
    "\n",
    "        x_end = bar.get_x() + bar.get_width() \n",
    "        y_position = (\n",
    "            bar.get_y() + bar.get_height() / 2\n",
    "        )\n",
    "\n",
    "        head_width = bar.get_height() / 9\n",
    "        head_length = 1\n",
    "        ax.arrow(\n",
    "            x_end,\n",
    "            y_position,\n",
    "            -bar.get_width() + head_length,\n",
    "            0,\n",
    "            head_width=head_width,\n",
    "            head_length=head_length,\n",
    "            width=0.01,\n",
    "            fc=\"black\",\n",
    "            ec=\"black\",\n",
    "            lw=0.5,\n",
    "        )\n",
    "        ax.plot(x_end, y_position, marker=\"|\", color=\"black\", markersize=8)\n",
    "\n",
    "axes[-1].xaxis.set_major_formatter(FuncFormatter(lambda x, _: f\"{int(x)}%\"))\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlim(0, 102)\n",
    "\n",
    "subplot_labels = [\"a) expansion periods\", \"b) grid components\", \"c) first-tier materials\", \"d) emitting processes\"]\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.text(-0.14, 1.15, subplot_labels[i], transform=ax.transAxes,\n",
    "    # ax.text(-0.1175, 1.15, subplot_labels[i], transform=ax.transAxes,\n",
    "            fontsize=12, va='top', ha='left')\n",
    "\n",
    "plt.subplots_adjust(hspace=0.8)\n",
    "\n",
    "plt.tight_layout(h_pad=0.5)\n",
    "\n",
    "plt.savefig(\"figs/Fig4.svg\", format=\"svg\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Burden Shifting (Figure 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if recalculate:\n",
    "    functional_units = {\n",
    "        \"static\": {act.id: 1 for act in static_expansion_nodes},\n",
    "        \"Base\": {act.id: 1 for act in prospective_expansion_nodes_base},\n",
    "        \"PkBudg1000\": {act.id: 1 for act in prospective_expansion_nodes_PkBudg1000},\n",
    "        \"PkBudg650\": {act.id: 1 for act in prospective_expansion_nodes_PkBudg650},\n",
    "    }\n",
    "\n",
    "    chosen_methods = [m for m in bd.methods if \"EF v3.1 no LT\" in m[1]]\n",
    "\n",
    "    mlca_config = {\n",
    "        \"impact_categories\": chosen_methods,\n",
    "    }\n",
    "\n",
    "    data_objs = bd.get_multilca_data_objs(\n",
    "        functional_units=functional_units, method_config=mlca_config\n",
    "    )\n",
    "\n",
    "    mlca = bc.MultiLCA(\n",
    "        demands=functional_units, method_config=mlca_config, data_objs=data_objs\n",
    "    )\n",
    "    mlca.lci()\n",
    "    mlca.lcia()\n",
    "\n",
    "    df = pd.DataFrame(mlca.scores.items(), columns=[\"MethodProcess\", \"Score\"])\n",
    "    df[[\"Method\", \"Process\"]] = pd.DataFrame(\n",
    "        df[\"MethodProcess\"].tolist(), index=df.index\n",
    "    )\n",
    "\n",
    "    df_mlca = df.pivot(index=\"Process\", columns=\"Method\", values=\"Score\")\n",
    "\n",
    "    def extract_ia_name(method_tuple):\n",
    "        return method_tuple[2][:-6]  # remove \"no LT\" from the method name\n",
    "\n",
    "    df_mlca.columns = df_mlca.columns.map(extract_ia_name)\n",
    "\n",
    "    json.dump(df_mlca.to_dict(), open(\"data/results/expansion_all_impact_categories.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mlca = pd.DataFrame(json.load(open(\"data/results/expansion_all_impact_categories.json\")))\n",
    "\n",
    "df_mlca = df_mlca[\n",
    "    [\n",
    "        \"acidification\",\n",
    "        \"climate change\",\n",
    "        \"ecotoxicity: freshwater\",\n",
    "        \"energy resources: non-renewable\",\n",
    "        \"eutrophication: freshwater\",\n",
    "        \"eutrophication: marine\",\n",
    "        \"eutrophication: terrestrial\",\n",
    "        \"human toxicity: carcinogenic\",\n",
    "        \"human toxicity: non-carcinogenic\",\n",
    "        \"ionising radiation: human health\",\n",
    "        \"land use\",\n",
    "        \"material resources: metals/minerals\",\n",
    "        \"ozone depletion\",\n",
    "        \"particulate matter formation\",\n",
    "        \"photochemical oxidant formation: human health\",\n",
    "        \"water use\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize with respect to the static results:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mlca.loc[\"Base\"] = df_mlca.loc[\"Base\"] / df_mlca.loc[\"static\"]\n",
    "df_mlca.loc[\"PkBudg1000\"] = df_mlca.loc[\"PkBudg1000\"] / df_mlca.loc[\"static\"]\n",
    "df_mlca.loc[\"PkBudg650\"] = df_mlca.loc[\"PkBudg650\"] / df_mlca.loc[\"static\"]\n",
    "df_mlca.loc[\"static\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import LinearSegmentedColormap, Normalize\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "# Assume df_mlca and cm are already defined\n",
    "\n",
    "values_base = df_mlca.loc[\"Base\"].sort_values(ascending=True)\n",
    "values_r26 = df_mlca.loc[\"PkBudg1000\"].sort_values(ascending=True)\n",
    "values_r19 = df_mlca.loc[\"PkBudg650\"].sort_values(ascending=True)\n",
    "\n",
    "bordeaux = cm.RWTHBordeaux.p(100)\n",
    "red = cm.RWTHRot.p(100)\n",
    "orange = cm.RWTHOrange.p(100)\n",
    "yellow = cm.RWTHGelb.p(100)\n",
    "maigreen = cm.RWTHMaiGruen.p(100)\n",
    "green = cm.RWTHGruen.p(100)\n",
    "\n",
    "cmap = LinearSegmentedColormap.from_list(\n",
    "    \"custom_red_green\", [green, maigreen, orange, red]\n",
    ")\n",
    "\n",
    "class MidpointNormalize(Normalize):\n",
    "    def __init__(self, vmin=None, vmax=None, midpoint=0, clip=False):\n",
    "        self.midpoint = midpoint\n",
    "        Normalize.__init__(self, vmin, vmax, clip)\n",
    "\n",
    "    def __call__(self, value, clip=None):\n",
    "        result, is_scalar = self.process_value(value)\n",
    "        self.autoscale_None(result)\n",
    "        vmin, vmax, midpoint = self.vmin, self.vmax, self.midpoint\n",
    "        x, y = [vmin, midpoint, vmax], [0, 0.5, 1]\n",
    "        return np.ma.masked_array(np.interp(result, x, y))\n",
    "\n",
    "shifted_base = values_base - 1\n",
    "shifted_r26 = values_r26 - 1\n",
    "shifted_r19 = values_r19 - 1\n",
    "\n",
    "all_values = np.concatenate([shifted_r26, shifted_r19, shifted_base])\n",
    "norm = MidpointNormalize(vmin=all_values.min(), vmax=all_values.max(), midpoint=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "index = np.arange(len(values_r26)) * 2.5\n",
    "bar_height = 0.5\n",
    "\n",
    "bars_base = ax.bar(\n",
    "    index,\n",
    "    shifted_base,\n",
    "    bar_height,\n",
    "    color=cmap(norm(shifted_base.values)),\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=0.5,\n",
    ")\n",
    "bars_r19 = ax.bar(\n",
    "    index + 2.5 * bar_height,\n",
    "    shifted_r19,\n",
    "    bar_height,\n",
    "    color=cmap(norm(shifted_r19.values)),\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=0.5,\n",
    ")\n",
    "bars_r26 = ax.bar(\n",
    "    index + 1.25 * bar_height,\n",
    "    shifted_r26,\n",
    "    bar_height,\n",
    "    color=cmap(norm(shifted_r26.values)),\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=0.5,\n",
    ")\n",
    "\n",
    "ax.set_ylabel(\"relative environmental impact \\ncompared to BAU\")\n",
    "ax.set_ylim(-0.32, 0.32)\n",
    "ax.axhline(y=0, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "ax.set_xticks(index + 1.25 * bar_height)\n",
    "ax.set_xticklabels(labels=values_r26.index, rotation=35, ha=\"right\")\n",
    "\n",
    "datasets = [shifted_r26, shifted_r19, shifted_base]\n",
    "bar_positions = [index + 1.25 * bar_height, index + 2.5 * bar_height, index]\n",
    "\n",
    "for dataset, bars, position, scenario_label in zip(\n",
    "    datasets,\n",
    "    [bars_r26, bars_r19, bars_base],\n",
    "    bar_positions,\n",
    "    [\"2.0°C scenario\", \"1.5°C scenario\", \"3.0°C scenario\"],\n",
    "):\n",
    "    for idx, height in enumerate(dataset):\n",
    "        percentage_change = height * 100\n",
    "        sign = \"+\" if percentage_change > 0 else \"\"\n",
    "        text = f\"{sign}{percentage_change:.0f}%\"\n",
    "        ax.text(\n",
    "            position[idx],\n",
    "            height + 0.005 if height > 0 else height - 0.005,\n",
    "            text,\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\" if height > 0 else \"top\",\n",
    "            rotation=90,\n",
    "        )\n",
    "        if idx == 0 or idx == len(dataset) - 1:\n",
    "            ax.text(\n",
    "                position[idx],\n",
    "                -0.01 if height > 0 else 0.01,\n",
    "                scenario_label,\n",
    "                ha=\"center\",\n",
    "                va=\"top\" if height > 0 else \"bottom\",\n",
    "                rotation=90,\n",
    "                style=\"italic\",\n",
    "            )\n",
    "\n",
    "ax.axvline(x=-0.5, color=\"grey\", linestyle=\"-\", linewidth=0.25)\n",
    "for x_pos in index + 3.75 * bar_height:\n",
    "    ax.axvline(x=x_pos, color=\"grey\", linestyle=\"-\", linewidth=0.25)\n",
    "\n",
    "ax.yaxis.set_major_formatter(PercentFormatter(1, decimals=0))\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figs/Fig5.svg\", format=\"svg\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SI: Sensitivities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo results\n",
    "\n",
    "Obtained from Activity Browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Load the data\n",
    "# Note: Use pd.read_excel if loading the original .xlsx file\n",
    "file_path = 'data/results/uncertainty_Monte-Carlo-results_IPCC-2021-climate-change-GWP-100a-incl-H-and-bio-CO2.xlsx'\n",
    "df = pd.read_excel(file_path, index_col=0)\n",
    "\n",
    "# 2. Clean up and Rename Scenarios\n",
    "# First, extract the short names from the long strings\n",
    "df.columns = [c.split('|')[0].strip() for c in df.columns]\n",
    "\n",
    "# Map the old names to the new scenario names\n",
    "name_map = {\n",
    "    'expansion_PkBudg650': '1.5°C scenario',\n",
    "    'expansion_PkBudg1000': '2°C scenario',\n",
    "    'expansion_NPi': '3°C scenario'\n",
    "}\n",
    "df = df.rename(columns=name_map)\n",
    "\n",
    "# Update color_dict with the NEW names\n",
    "color_dict = {\n",
    "    '1.5°C scenario': '#57AB27',   \n",
    "    '2°C scenario': '#F6A800', \n",
    "    '3°C scenario': '#00549F'\n",
    "}\n",
    "\n",
    "# 3. Create the plots\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Boxplot for side-by-side comparison\n",
    "sns.boxplot(data=df, ax=ax[0], palette=color_dict)\n",
    "ax[0].set_title('Comparison of Scenarios (GWP 100a)')\n",
    "ax[0].set_ylabel('GWP100 [kg CO2-eq]')\n",
    "ax[0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# KDE Plot for distribution shapes\n",
    "for col in df.columns:\n",
    "    color = color_dict[col]\n",
    "    mean_val = df[col].mean()\n",
    "    sns.kdeplot(df[col], label=f'{col}', fill=True, ax=ax[1], color=color, alpha=0.4)\n",
    "    # Using ax[1].axvline instead of plt.axvline for better control in subplots\n",
    "    ax[1].axvline(x=mean_val, color=color, linestyle='--', linewidth=1.5, \n",
    "                label=f'Mean {col}: {mean_val:.2e}')\n",
    "\n",
    "ax[1].set_title('Probability Distribution of Results')\n",
    "ax[1].set_xlabel('GWP100 [kg CO2-eq]')\n",
    "ax[1].set_ylabel('Density')\n",
    "ax[1].grid(axis='both', linestyle='--', alpha=0.5)\n",
    "\n",
    "# 4. Extract handles and labels from the KDE axis for a shared legend\n",
    "handles, labels = ax[1].get_legend_handles_labels()\n",
    "\n",
    "# 5. Place the legend at the bottom of the figure\n",
    "fig.legend(handles, labels, loc='lower center', ncol=3, bbox_to_anchor=(0.5, -0.05))\n",
    "\n",
    "# 6. Adjust layout to make room for the legend\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(bottom=0.15) \n",
    "\n",
    "fig.savefig(\"figs/uncertainty.svg\", format=\"svg\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Load the data\n",
    "file_path = 'data/results/uncertainty_Monte-Carlo-results_IPCC-2021-climate-change-GWP-100a-incl-H-and-bio-CO2.xlsx'\n",
    "df = pd.read_excel(file_path, index_col=0)\n",
    "\n",
    "# 2. Clean up and Rename Scenarios\n",
    "df.columns = [c.split('|')[0].strip() for c in df.columns]\n",
    "\n",
    "name_map = {\n",
    "    'expansion_PkBudg650': '1.5°C scenario',\n",
    "    'expansion_PkBudg1000': '2°C scenario',\n",
    "    'expansion_NPi': '3°C scenario'\n",
    "}\n",
    "df = df.rename(columns=name_map)\n",
    "\n",
    "color_dict = {\n",
    "    '1.5°C scenario': '#57AB27',   \n",
    "    '2°C scenario': '#F6A800', \n",
    "    '3°C scenario': '#00549F'\n",
    "}\n",
    "\n",
    "# 3. Create the plot (Single Plot)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# KDE Plot for distribution shapes\n",
    "for col in df.columns:\n",
    "    color = color_dict.get(col, '#333333') # Fallback color\n",
    "    mean_val = df[col].mean()\n",
    "    \n",
    "    sns.kdeplot(df[col], label=f'{col}', fill=True, ax=ax, color=color, alpha=0.4)\n",
    "    ax.axvline(x=mean_val, color=color, linestyle='--', linewidth=1.5, \n",
    "                label=f'Mean {col}: {mean_val:.2e}')\n",
    "\n",
    "# Formatting\n",
    "ax.set_xlabel('GWP100 [kg CO2-eq]', fontsize=12)\n",
    "ax.set_ylabel('Density', fontsize=12)\n",
    "ax.grid(axis='both', linestyle='--', alpha=0.5)\n",
    "\n",
    "# 4. Legend and Layout\n",
    "# Moving the legend to the right or bottom for clarity\n",
    "ax.legend(loc='upper right', fontsize=9, frameon=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# 5. Save and Show\n",
    "fig.savefig(\"figs/uncertainty_distribution.svg\", format=\"svg\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GSA Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Define filenames and scenario names\n",
    "gsa_files = {\n",
    "    '1.5°C scenario': 'data/results/gsa_output_uncertainty_5000_expansion_PkBudg650_IPCC-2021-climate-change-GWP-100a-incl-H-and-bio-CO2xlsx.xlsx',\n",
    "    '2°C scenario': 'data/results/gsa_output_uncertainty_5000_expansion_PkBudg1000_IPCC-2021-climate-change-GWP-100a-incl-H-and-bio-CO2xlsx.xlsx',\n",
    "    '3°C scenario': 'data/results/gsa_output_uncertainty_5000_expansion_NPi_IPCC-2021-climate-change-GWP-100a-incl-H-and-bio-CO2xlsx.xlsx'\n",
    "}\n",
    "\n",
    "def extract_year_robust(row):\n",
    "    \"\"\"\n",
    "    Seeks the year (_YYYY) across multiple potential columns.\n",
    "    Priority: to name -> output code -> GSA technical name -> input code.\n",
    "    \"\"\"\n",
    "    # 1. Check 'to name' (The destination activity name usually has the year)\n",
    "    to_name = str(row.get('to name', \"\"))\n",
    "    match = re.search(r\"_(\\d{4})\", to_name)\n",
    "    if match: return match.group(1)\n",
    "    \n",
    "    # 2. Check 'output' column (The tuple from the database)\n",
    "    out_val = str(row.get('output', \"\"))\n",
    "    match = re.search(r\"_(\\d{4})\", out_val)\n",
    "    if match: return match.group(1)\n",
    "    \n",
    "    # 3. Check technical GSA name (after 'TO')\n",
    "    gsa_name = str(row.get('GSA name', \"\"))\n",
    "    if ' TO ' in gsa_name:\n",
    "        dest_part = gsa_name.split(' TO ')[1]\n",
    "        match = re.search(r\"_(\\d{4})\", dest_part)\n",
    "        if match: return match.group(1)\n",
    "        \n",
    "    # 4. Fallback to 'from name' or 'input' (for biosphere or specific material vintages)\n",
    "    from_name = str(row.get('from name', \"\"))\n",
    "    match = re.search(r\"_(\\d{4})\", from_name)\n",
    "    if match: return match.group(1)\n",
    "    \n",
    "    in_val = str(row.get('input', \"\"))\n",
    "    match = re.search(r\"_(\\d{4})\", in_val)\n",
    "    if match: return match.group(1)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def clean_lca_name(name):\n",
    "    \"\"\"\n",
    "    Cleans technical exchange names for readability.\n",
    "    \"\"\"\n",
    "    # Remove technical prefixes/suffixes\n",
    "    name = name.replace('T:  FROM ', '').replace('agg', '').replace(' [GLO]', '').replace('()', '')\n",
    "    \n",
    "    parts = name.split(' TO ')\n",
    "    if len(parts) == 2:\n",
    "        src = parts[0].strip()\n",
    "        dest = parts[1].strip()\n",
    "        # Simplify destination for grid expansion\n",
    "        dest = re.sub(r'grid_.*_\\d{4}', 'Grid Expansion', dest)\n",
    "        return f\"{src} → {dest}\"\n",
    "    return name.strip()\n",
    "\n",
    "# Process the data\n",
    "scenario_results = []\n",
    "\n",
    "for scenario, file_path in gsa_files.items():\n",
    "    if not os.path.exists(file_path):\n",
    "        continue\n",
    "        \n",
    "    # Load data (assuming excel based on filenames provided)\n",
    "    df = pd.read_excel(file_path, index_col=0)\n",
    "    \n",
    "    def get_full_exchange_name(row):\n",
    "        base_name = clean_lca_name(row['GSA name'])\n",
    "        year = extract_year_robust(row)\n",
    "        # Only append year if found and not already in the cleaned name\n",
    "        if year and year not in base_name:\n",
    "            return f\"{base_name} ({year})\"\n",
    "        return base_name\n",
    "\n",
    "    df['Exchange'] = df.apply(get_full_exchange_name, axis=1)\n",
    "    \n",
    "    # Sort and drop duplicates\n",
    "    unique_df = df[['Exchange', 'delta']].drop_duplicates().sort_values('delta', ascending=False)\n",
    "    \n",
    "    # Prepare top 25\n",
    "    top25 = unique_df.head(25).copy()\n",
    "    top25['Rank'] = range(1, len(top25) + 1)\n",
    "    top25['Scenario'] = scenario\n",
    "    scenario_results.append(top25)\n",
    "\n",
    "# 3. Create the side-by-side comparison table\n",
    "if scenario_results:\n",
    "    all_tops = pd.concat(scenario_results)\n",
    "    pivot_list = []\n",
    "\n",
    "    for scenario in gsa_files.keys():\n",
    "        if scenario in all_tops['Scenario'].unique():\n",
    "            s_df = all_tops[all_tops['Scenario'] == scenario][['Rank', 'Exchange', 'delta']].copy()\n",
    "            s_df.columns = ['Rank', f'{scenario} Exchange', f'{scenario} $\\delta$']\n",
    "            pivot_list.append(s_df.set_index('Rank'))\n",
    "\n",
    "    final_table = pd.concat(pivot_list, axis=1).round(4)\n",
    "    final_table.to_excel('data/results/gsa_top25_comparison_table.xlsx')\n",
    "    print(\"Table generated.\")\n",
    "    \n",
    "final_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra sensitivities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AL / CU ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bw2data as bd\n",
    "\n",
    "hv_cable_al = \"land_cable_vpe_al_50kv\"\n",
    "hv_cable_cu = \"land_cable_oil_cu_150kv\"\n",
    "mv_cable_al = \"land_cable_vpe_al_10kv\"\n",
    "mv_cable_cu = \"land_cable_epr_cu_11kv\"\n",
    "lv_cable_al = \"land_cable_vpe_al_04kv\"\n",
    "lv_cable_cu = \"land_cable_vpe_cu_1kv\"\n",
    "\n",
    "hv_cables = [hv_cable_al, hv_cable_cu]\n",
    "mv_cables = [mv_cable_al, mv_cable_cu]\n",
    "lv_cables = [lv_cable_al, lv_cable_cu]\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Map voltage levels to your variable names for easier lookup\n",
    "cable_map = {\n",
    "    \"HV\": {\"al_code\": hv_cable_al, \"all_codes\": hv_cables},\n",
    "    \"MV\": {\"al_code\": mv_cable_al, \"all_codes\": mv_cables},\n",
    "    \"LV\": {\"al_code\": lv_cable_al, \"all_codes\": lv_cables}\n",
    "}\n",
    "\n",
    "for scenario, scenario_name in [(prospective_expansion_nodes_base, \"Base\"), (prospective_expansion_nodes_PkBudg1000, \"PkBudg1000\"), (prospective_expansion_nodes_PkBudg650, \"PkBudg650\")]:\n",
    "    grid_ids_and_shares = {}\n",
    "    \n",
    "    for grid_node in scenario:\n",
    "        # Initialize node entry if not exists\n",
    "        grid_ids_and_shares[grid_node.id] = {}\n",
    "        \n",
    "        # First pass: Collect what is already there\n",
    "        for exc in grid_node.technosphere():\n",
    "            for level, config in cable_map.items():\n",
    "                if exc.input[\"code\"] in config[\"all_codes\"]:\n",
    "                    # Ensure level dictionary exists (e.g., grid_ids_and_shares[id][\"HV\"])\n",
    "                    grid_ids_and_shares[grid_node.id].setdefault(level, {})\n",
    "                    \n",
    "                    metal = \"AL\" if exc.input[\"code\"] == config[\"al_code\"] else \"CU\"\n",
    "                    grid_ids_and_shares[grid_node.id][level][metal] = {exc.input.id: exc.amount}\n",
    "\n",
    "        # Second pass: \"Inject\" missing AL/CU placeholders so the sensitivity study has indices to target\n",
    "        for level, config in cable_map.items():\n",
    "            if level in grid_ids_and_shares[grid_node.id]:\n",
    "                level_data = grid_ids_and_shares[grid_node.id][level]\n",
    "                \n",
    "                # If one metal is present but the other isn't, find the missing one in the same DB\n",
    "                for current_metal, target_metal, target_code in [(\"CU\", \"AL\", config[\"al_code\"]), (\"AL\", \"CU\", \"placeholder\")]:\n",
    "                    # We mainly care about adding AL if only CU exists\n",
    "                    if current_metal in level_data and target_metal not in level_data:\n",
    "                        # Get the database name from the existing metal exchange\n",
    "                        existing_id = list(level_data[current_metal].keys())[0]\n",
    "                        db_name = bd.get_activity(existing_id)[\"database\"]\n",
    "                        \n",
    "                        try:\n",
    "                            # Find the corresponding cable in the SAME database\n",
    "                            missing_act = bd.get_activity((db_name, target_code))\n",
    "                            level_data[target_metal] = {missing_act.id: 0.0}\n",
    "                        except bd.errors.UnknownObject:\n",
    "                            # Fallback if the other metal doesn't exist in that specific database\n",
    "                            continue\n",
    "                        \n",
    "    import numpy as np\n",
    "    import bw_processing as bwp\n",
    "\n",
    "    sensitivity_shares = np.linspace(0, 1, 11)\n",
    "\n",
    "    # Generate scenario names\n",
    "    scenario_names = [\"Baseline\"] + [f\"Al_Share_{int(s*100)}pct\" for s in sensitivity_shares]\n",
    "\n",
    "    # Reference baseline shares (from your existing study)\n",
    "    BASE_SHARES = {\n",
    "        \"HV\": 0.5,\n",
    "        \"MV\": 0.75,\n",
    "        \"LV\": 0.7\n",
    "    }\n",
    "\n",
    "    indices = []\n",
    "    data_rows = []\n",
    "\n",
    "    for grid_id, levels in grid_ids_and_shares.items():\n",
    "        for level, metals in levels.items():\n",
    "            # Identify IDs and current values\n",
    "            al_info = metals.get(\"AL\", {})\n",
    "            cu_info = metals.get(\"CU\", {})\n",
    "            \n",
    "            al_id = list(al_info.keys())[0] if al_info else None\n",
    "            cu_id = list(cu_info.keys())[0] if cu_info else None\n",
    "            \n",
    "            # Calculate total length (Al + Cu) to keep it constant\n",
    "            total_len = sum(al_info.values()) + sum(cu_info.values())\n",
    "            \n",
    "            # --- Scenario Calculation ---\n",
    "            al_scenario_values = []\n",
    "            cu_scenario_values = []\n",
    "            \n",
    "            # 1. First entry: Baseline\n",
    "            base_al_share = BASE_SHARES[level]\n",
    "            al_scenario_values.append(total_len * base_al_share)\n",
    "            cu_scenario_values.append(total_len * (1 - base_al_share))\n",
    "            \n",
    "            # 2. Subsequent entries: 0% to 100% steps\n",
    "            for share in sensitivity_shares:\n",
    "                al_scenario_values.append(total_len * share)\n",
    "                cu_scenario_values.append(total_len * (1 - share))\n",
    "                \n",
    "            # Append to the datapackage arrays\n",
    "            if al_id:\n",
    "                indices.append((al_id, grid_id))\n",
    "                data_rows.append(al_scenario_values)\n",
    "            if cu_id:\n",
    "                indices.append((cu_id, grid_id))\n",
    "                data_rows.append(cu_scenario_values)\n",
    "\n",
    "    # Convert to final arrays\n",
    "    indices_array = np.array(indices, dtype=bwp.INDICES_DTYPE)\n",
    "    data_array = np.array(data_rows)\n",
    "    flip_array = np.ones(len(indices), dtype=bool)\n",
    "\n",
    "    dp = bwp.create_datapackage(name=\"absolute-share-sensitivity\", sequential=True)\n",
    "\n",
    "    dp.add_persistent_array(\n",
    "        matrix=\"technosphere_matrix\",\n",
    "        indices_array=indices_array,\n",
    "        data_array=data_array,\n",
    "        flip_array=flip_array,\n",
    "    )\n",
    "\n",
    "    fu, datapackages, _ = bd.prepare_lca_inputs({grid_node: 1 for grid_node in scenario}, method=method)\n",
    "    datapackages.append(dp)\n",
    "\n",
    "    import bw2calc as bc\n",
    "\n",
    "    lca = bc.LCA(\n",
    "        demand=fu,\n",
    "        method=method,\n",
    "        data_objs=datapackages,\n",
    "        use_arrays=True,\n",
    "    )\n",
    "    lca.lci()\n",
    "    lca.lcia()\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    lca.keep_first_iteration() # keep the base values\n",
    "\n",
    "    results_list = []\n",
    "\n",
    "    for _, label in zip(lca, scenario_names):\n",
    "        results_list.append((label, lca.score))\n",
    "        print(scenario_name, label, lca.score)\n",
    "\n",
    "    results[scenario_name] = results_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# 1. Create the DataFrame\n",
    "# (Using a dictionary for clarity, but you can also load your csv here)\n",
    "data = {\n",
    "    'Scenario': ['Default']*12 + ['PkBudg1000']*12 + ['PkBudg650']*12,\n",
    "    'Share': [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None] * 3,\n",
    "    'Type': (['Variation']*11 + ['Baseline']) * 3,\n",
    "    'Value': [\n",
    "        # Default\n",
    "        24179211492.94, 24266610307.96, 24354009122.98, 24441407938.00, 24528806753.02, \n",
    "        24616205568.04, 24703604383.06, 24791003198.08, 24878402013.11, 24965800828.13, \n",
    "        25053199643.15, 24740376488.57,\n",
    "        # PkBudg1000\n",
    "        22440140170.83, 22520846320.98, 22601552471.13, 22682258621.28, 22762964771.43, \n",
    "        22843670921.58, 22924377071.73, 23005083221.88, 23085789372.03, 23166495522.18, \n",
    "        23247201672.33, 22961545193.67,\n",
    "        # PkBudg650\n",
    "        21133716951.43, 21234858831.71, 21336000711.99, 21437142592.27, 21538284472.54, \n",
    "        21639426352.82, 21740568233.10, 21841710113.38, 21942851993.66, 22043993873.93, \n",
    "        22145135754.21, 21805206388.09\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 2. Setup the Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Define styling per scenario\n",
    "scenarios = {\n",
    "    'Default': {'name': '3 °C scenario', 'color': '#d62728', 'marker': 'o'},\n",
    "    'PkBudg1000': {'name': '2 °C scenario', 'color': '#1f77b4', 'marker': 's'},\n",
    "    'PkBudg650': {'name': '1.5 °C scenario', 'color': '#ff7f0e', 'marker': '^'}\n",
    "}\n",
    "\n",
    "# 3. Plotting loop\n",
    "for sc, style in scenarios.items():\n",
    "    # Filter variation data (the line)\n",
    "    subset = df[(df['Scenario'] == sc) & (df['Type'] == 'Variation')].sort_values('Share')\n",
    "    # Filter baseline (the horizontal line)\n",
    "    baseline_val = df[(df['Scenario'] == sc) & (df['Type'] == 'Baseline')]['Value'].values[0]\n",
    "    \n",
    "    # Plot sensitivity line\n",
    "    plt.plot(subset['Share'], subset['Value'], \n",
    "             label=f\"{style['name']} Sensitivity\", \n",
    "             color=style['color'], \n",
    "             marker=style['marker'], \n",
    "             markersize=5, \n",
    "             linewidth=2)\n",
    "    \n",
    "    # Plot baseline reference line\n",
    "    plt.axhline(y=baseline_val, \n",
    "                color=style['color'], \n",
    "                linestyle='--', \n",
    "                alpha=0.6, \n",
    "                label=f\"{style['name']} Main Paper Assumption\")\n",
    "\n",
    "# 4. Final Formatting\n",
    "plt.xlabel('Share of aluminium cables across voltage levels (%)')\n",
    "plt.ylabel('Climate change impact of grid expansion through 2045 (kg CO2-eq)')\n",
    "\n",
    "# Place legend outside of the plot area\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cable vs overhead line ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bw2data as bd\n",
    "import numpy as np\n",
    "import bw_processing as bwp\n",
    "import bw2calc as bc\n",
    "\n",
    "# Define overhead line and cable codes for each voltage level\n",
    "# EHV DC\n",
    "ehv_dc_overhead = \"overhead_line_HVDC\"\n",
    "ehv_dc_cable = \"land_cable_oil_cu_HVDC\"\n",
    "\n",
    "# HV (cables split by AL/CU, so we need both)\n",
    "hv_overhead = \"overhead_line_150kv\"\n",
    "hv_cable_al = \"land_cable_vpe_al_50kv\"\n",
    "hv_cable_cu = \"land_cable_oil_cu_150kv\"\n",
    "\n",
    "# MV\n",
    "mv_overhead = \"overhead_line_10kv\"\n",
    "mv_cable_al = \"land_cable_vpe_al_10kv\"\n",
    "mv_cable_cu = \"land_cable_epr_cu_11kv\"\n",
    "\n",
    "# LV\n",
    "lv_overhead = \"overhead_line_04kv\"\n",
    "lv_cable_al = \"land_cable_vpe_al_04kv\"\n",
    "lv_cable_cu = \"land_cable_vpe_cu_1kv\"\n",
    "\n",
    "# Map voltage levels to component codes\n",
    "line_map = {\n",
    "    \"EHV_DC\": {\"overhead_code\": ehv_dc_overhead, \"cable_codes\": [ehv_dc_cable]},\n",
    "    \"HV\": {\"overhead_code\": hv_overhead, \"cable_codes\": [hv_cable_al, hv_cable_cu]},\n",
    "    \"MV\": {\"overhead_code\": mv_overhead, \"cable_codes\": [mv_cable_al, mv_cable_cu]},\n",
    "    \"LV\": {\"overhead_code\": lv_overhead, \"cable_codes\": [lv_cable_al, lv_cable_cu]},\n",
    "}\n",
    "\n",
    "# Baseline cable shares (1 - overhead share)\n",
    "BASE_CABLE_SHARES = {\n",
    "    \"EHV_DC\": 1 - 0.05,   # 95% cable\n",
    "    \"HV\": 1 - 0.9474,     # 5.26% cable\n",
    "    \"MV\": 1 - 0.1847,     # 81.53% cable\n",
    "    \"LV\": 1 - 0.0652,     # 93.48% cable\n",
    "}\n",
    "\n",
    "results_cable_share = {}\n",
    "sensitivity_shares = np.linspace(0, 1, 11)\n",
    "voltage_levels = list(line_map.keys())\n",
    "\n",
    "for scenario, scenario_name in [\n",
    "    (prospective_expansion_nodes_base, \"Base\"),\n",
    "    (prospective_expansion_nodes_PkBudg1000, \"PkBudg1000\"),\n",
    "    (prospective_expansion_nodes_PkBudg650, \"PkBudg650\")\n",
    "]:\n",
    "    print(f\"\\n--- Processing Scenario: {scenario_name} ---\")\n",
    "    \n",
    "    # 1. Collect Data & Totals\n",
    "    grid_ids_and_shares = {}\n",
    "    for grid_node in scenario:\n",
    "        grid_ids_and_shares[grid_node.id] = {}\n",
    "        for exc in grid_node.technosphere():\n",
    "            for level, config in line_map.items():\n",
    "                if exc.input[\"code\"] == config[\"overhead_code\"]:\n",
    "                    grid_ids_and_shares[grid_node.id].setdefault(level, {\"overhead\": {}, \"cables\": {}})\n",
    "                    grid_ids_and_shares[grid_node.id][level][\"overhead\"][exc.input.id] = exc.amount\n",
    "                if exc.input[\"code\"] in config[\"cable_codes\"]:\n",
    "                    grid_ids_and_shares[grid_node.id].setdefault(level, {\"overhead\": {}, \"cables\": {}})\n",
    "                    grid_ids_and_shares[grid_node.id][level][\"cables\"][exc.input.id] = exc.amount\n",
    "\n",
    "    # 2. Define Scenario Labels\n",
    "    # Order: [Baseline, EHV_0, ..., EHV_100, HV_0, ..., HV_100, etc.]\n",
    "    scenario_labels = [\"Baseline\"]\n",
    "    for level in voltage_levels:\n",
    "        for s in sensitivity_shares:\n",
    "            scenario_labels.append(f\"{level}_{int(s*100)}pct\")\n",
    "    \n",
    "    num_columns = len(scenario_labels)\n",
    "    indices = []\n",
    "    data_matrix = []\n",
    "\n",
    "    print(f\"Building data matrix for {num_columns} sensitivity steps...\")\n",
    "\n",
    "    # 3. Build the Data Matrix\n",
    "    for grid_id, levels in grid_ids_and_shares.items():\n",
    "        for level_name, components in levels.items():\n",
    "            overhead_dict = components.get(\"overhead\", {})\n",
    "            cable_dict = components.get(\"cables\", {})\n",
    "            \n",
    "            # Identify IDs (handles multiple cables per level like AL/CU)\n",
    "            o_id = list(overhead_dict.keys())[0] if overhead_dict else None\n",
    "            c_ids = list(cable_dict.keys())\n",
    "            \n",
    "            total_len = sum(overhead_dict.values()) + sum(cable_dict.values())\n",
    "            if total_len == 0: continue\n",
    "\n",
    "            # Cable mix (how much AL vs CU within the cable share)\n",
    "            total_c = sum(cable_dict.values())\n",
    "            c_props = {cid: (amt/total_c if total_c > 0 else 1.0/len(c_ids)) for cid, amt in cable_dict.items()}\n",
    "\n",
    "            # Create the rows for this specific component set\n",
    "            # Initialize with 0s, then fill\n",
    "            o_row = np.zeros(num_columns)\n",
    "            c_rows = {cid: np.zeros(num_columns) for cid in c_ids}\n",
    "\n",
    "            for col_idx, label in enumerate(scenario_labels):\n",
    "                if label == \"Baseline\":\n",
    "                    share = BASE_CABLE_SHARES[level_name]\n",
    "                else:\n",
    "                    active_level = label.split('_')[0] if \"EHV\" not in label else \"EHV_DC\"\n",
    "                    active_share = int(label.split('_')[-1].replace('pct', '')) / 100.0\n",
    "                    \n",
    "                    # If THIS component belongs to the level being varied\n",
    "                    if level_name == active_level:\n",
    "                        share = active_share\n",
    "                    else:\n",
    "                        # Keep at baseline!\n",
    "                        share = BASE_CABLE_SHARES[level_name]\n",
    "\n",
    "                o_row[col_idx] = total_len * (1 - share)\n",
    "                for cid in c_ids:\n",
    "                    c_rows[cid][col_idx] = total_len * share * c_props[cid]\n",
    "\n",
    "            # Add to master lists\n",
    "            if o_id:\n",
    "                indices.append((o_id, grid_id))\n",
    "                data_matrix.append(o_row)\n",
    "            for cid in c_ids:\n",
    "                indices.append((cid, grid_id))\n",
    "                data_matrix.append(c_rows[cid])\n",
    "\n",
    "    # 4. Run LCA\n",
    "    print(f\"Calculating LCA for {scenario_name}...\")\n",
    "    dp = bwp.create_datapackage(name=\"isolated-sensitivity\", sequential=True)\n",
    "    dp.add_persistent_array(\n",
    "        matrix=\"technosphere_matrix\",\n",
    "        indices_array=np.array(indices, dtype=bwp.INDICES_DTYPE),\n",
    "        data_array=np.array(data_matrix),\n",
    "        flip_array=np.ones(len(indices), dtype=bool),\n",
    "    )\n",
    "\n",
    "    fu, datapackages, _ = bd.prepare_lca_inputs({grid_node: 1 for grid_node in scenario}, method=method)\n",
    "    datapackages.append(dp)\n",
    "\n",
    "    lca = bc.LCA(demand=fu, method=method, data_objs=datapackages, use_arrays=True)\n",
    "    lca.lci()\n",
    "    lca.lcia()\n",
    "\n",
    "    results_list = []\n",
    "    for label in scenario_labels:\n",
    "        results_list.append((label, lca.score))\n",
    "        try:\n",
    "            next(lca)\n",
    "        except StopIteration:\n",
    "            break\n",
    "            \n",
    "    results_cable_share[scenario_name] = results_list\n",
    "    print(f\"Finished {scenario_name}. Baseline: {results_list[0][1]:.2e}\")\n",
    "\n",
    "print(\"\\nAll scenarios complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_rows = []\n",
    "for scenario_name, results_list in results_cable_share.items():\n",
    "    for label, value in results_list:\n",
    "        if label == \"Baseline\":\n",
    "            # Tag baseline as applicable to all levels for plotting reference\n",
    "            data_rows.append({\n",
    "                'Scenario': scenario_name,\n",
    "                'Level': 'Baseline',\n",
    "                'Share': None,\n",
    "                'Value': value\n",
    "            })\n",
    "        else:\n",
    "            # New parsing: \"MV_50pct\" -> Level: \"MV\", Share: 50\n",
    "            parts = label.split('_')\n",
    "            level = parts[0] if len(parts) == 2 else f\"{parts[0]}_{parts[1]}\" # Handles EHV_DC\n",
    "            share = int(parts[-1].replace('pct', ''))\n",
    "            \n",
    "            data_rows.append({\n",
    "                'Scenario': scenario_name,\n",
    "                'Level': level,\n",
    "                'Share': share,\n",
    "                'Value': value\n",
    "            })\n",
    "\n",
    "df_cable = pd.DataFrame(data_rows)\n",
    "\n",
    "# Define styling per scenario (consistent with your previous plot)\n",
    "scenarios_style = {\n",
    "    'Base': {'name': '3 °C scenario', 'color': '#d62728', 'marker': 'o'},\n",
    "    'PkBudg1000': {'name': '2 °C scenario', 'color': '#1f77b4', 'marker': 's'},\n",
    "    'PkBudg650': {'name': '1.5 °C scenario', 'color': '#ff7f0e', 'marker': '^'}\n",
    "}\n",
    "\n",
    "# Get the list of unique voltage levels (excluding 'Baseline')\n",
    "levels = [l for l in df_cable['Level'].unique() if l != 'Baseline']\n",
    "\n",
    "# Create subplots: 1 row, 4 columns\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 6), sharey=True)\n",
    "\n",
    "for i, level in enumerate(levels):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    for sc, style in scenarios_style.items():\n",
    "        # Filter variation data for this specific level and scenario\n",
    "        subset = df_cable[(df_cable['Scenario'] == sc) & (df_cable['Level'] == level)].sort_values('Share')\n",
    "        \n",
    "        # Filter baseline for this scenario\n",
    "        baseline_val = df_cable[(df_cable['Scenario'] == sc) & (df_cable['Level'] == 'Baseline')]['Value'].values[0]\n",
    "        \n",
    "        # Plot sensitivity line\n",
    "        ax.plot(subset['Share'], subset['Value'], \n",
    "                 label=f\"{style['name']}\", \n",
    "                 color=style['color'], \n",
    "                 marker=style['marker'], \n",
    "                 markersize=4, \n",
    "                 linewidth=1.5)\n",
    "        \n",
    "        # Plot baseline reference line\n",
    "        ax.axhline(y=baseline_val, \n",
    "                    color=style['color'], \n",
    "                    linestyle='--', \n",
    "                    alpha=0.5)\n",
    "\n",
    "    ax.set_title(f\"Sensitivity: {level}\")\n",
    "    ax.set_xlabel('Cable Share (%)')\n",
    "    if i == 0:\n",
    "        ax.set_ylabel('Impact (kg CO2-eq)')\n",
    "\n",
    "# Create a single legend for the entire figure\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 1.05), ncol=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figs/sensitivity_cable_share_split.svg\", format=\"svg\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "premise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
