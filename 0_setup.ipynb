{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "In this notebook, we set up the databases we need for the LCA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bw2data as bd\n",
    "import bw2io as bi\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd.projects.set_current(\"paper_plca_grid_expansion_rev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating background databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For creating the background databases, we need access to the ecoinvent database (commercial) and to the default premise scenarios (free, to be asked from premise maintainers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecoinvent_username = \"xxx\" # fill in your data here\n",
    "ecoinvent_password = \"xxx\"\n",
    "premise_key = \"xxx\" # to be asked from premise maintainers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting ecoinvent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying strategy: normalize_units\n",
      "Applying strategy: drop_unspecified_subcategories\n",
      "Applying strategy: ensure_categories_are_tuples\n",
      "Applied 3 strategies in 0.00 seconds\n",
      "Graph statistics for `ecoinvent-3.10.1-biosphere` importer:\n",
      "4362 graph nodes:\n",
      "\temission: 4000\n",
      "\tnatural resource: 344\n",
      "\tinventory indicator: 13\n",
      "\teconomic: 5\n",
      "0 graph edges:\n",
      "0 edges to the following databases:\n",
      "0 unique unlinked edges (0 total):\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4362/4362 [00:00<00:00, 24702.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m07:15:05+0100\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mVacuuming database            \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created database: ecoinvent-3.10.1-biosphere\n",
      "Extracting XML data from 23523 datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timodiepers/anaconda3/envs/premise/lib/python3.11/site-packages/scikits/__init__.py:1: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __import__('pkg_resources').declare_namespace(__name__)\n",
      "/Users/timodiepers/anaconda3/envs/premise/lib/python3.11/site-packages/scikits/__init__.py:1: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __import__('pkg_resources').declare_namespace(__name__)\n",
      "/Users/timodiepers/anaconda3/envs/premise/lib/python3.11/site-packages/scikits/__init__.py:1: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __import__('pkg_resources').declare_namespace(__name__)\n",
      "/Users/timodiepers/anaconda3/envs/premise/lib/python3.11/site-packages/scikits/__init__.py:1: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __import__('pkg_resources').declare_namespace(__name__)\n",
      "/Users/timodiepers/anaconda3/envs/premise/lib/python3.11/site-packages/scikits/__init__.py:1: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __import__('pkg_resources').declare_namespace(__name__)\n",
      "/Users/timodiepers/anaconda3/envs/premise/lib/python3.11/site-packages/scikits/__init__.py:1: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __import__('pkg_resources').declare_namespace(__name__)\n",
      "/Users/timodiepers/anaconda3/envs/premise/lib/python3.11/site-packages/scikits/__init__.py:1: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __import__('pkg_resources').declare_namespace(__name__)\n",
      "/Users/timodiepers/anaconda3/envs/premise/lib/python3.11/site-packages/scikits/__init__.py:1: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __import__('pkg_resources').declare_namespace(__name__)\n",
      "/Users/timodiepers/anaconda3/envs/premise/lib/python3.11/site-packages/scikits/__init__.py:1: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __import__('pkg_resources').declare_namespace(__name__)\n",
      "/Users/timodiepers/anaconda3/envs/premise/lib/python3.11/site-packages/scikits/__init__.py:1: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __import__('pkg_resources').declare_namespace(__name__)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m07:15:10+0100\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mCan't import `SimaProBlockCSVImporter` - please install `bw2io` with `pip install bw2io[multifunctional]` or install `multifunctional` and `bw_simapro_csv` manually.\u001b[0m\n",
      "\u001b[2m07:15:10+0100\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mCan't import `SimaProBlockCSVImporter` - please install `bw2io` with `pip install bw2io[multifunctional]` or install `multifunctional` and `bw_simapro_csv` manually.\u001b[0m\n",
      "\u001b[2m07:15:10+0100\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mCan't import `SimaProBlockCSVImporter` - please install `bw2io` with `pip install bw2io[multifunctional]` or install `multifunctional` and `bw_simapro_csv` manually.\u001b[0m\n",
      "\u001b[2m07:15:10+0100\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mCan't import `SimaProBlockCSVImporter` - please install `bw2io` with `pip install bw2io[multifunctional]` or install `multifunctional` and `bw_simapro_csv` manually.\u001b[0m\n",
      "\u001b[2m07:15:10+0100\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mCan't import `SimaProBlockCSVImporter` - please install `bw2io` with `pip install bw2io[multifunctional]` or install `multifunctional` and `bw_simapro_csv` manually.\u001b[0m\n",
      "\u001b[2m07:15:10+0100\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mCan't import `SimaProBlockCSVImporter` - please install `bw2io` with `pip install bw2io[multifunctional]` or install `multifunctional` and `bw_simapro_csv` manually.\u001b[0m\n",
      "\u001b[2m07:15:10+0100\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mCan't import `SimaProBlockCSVImporter` - please install `bw2io` with `pip install bw2io[multifunctional]` or install `multifunctional` and `bw_simapro_csv` manually.\u001b[0m\n",
      "\u001b[2m07:15:10+0100\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mCan't import `SimaProBlockCSVImporter` - please install `bw2io` with `pip install bw2io[multifunctional]` or install `multifunctional` and `bw_simapro_csv` manually.\u001b[0m\n",
      "\u001b[2m07:15:10+0100\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mCan't import `SimaProBlockCSVImporter` - please install `bw2io` with `pip install bw2io[multifunctional]` or install `multifunctional` and `bw_simapro_csv` manually.\u001b[0m\n",
      "\u001b[2m07:15:10+0100\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mCan't import `SimaProBlockCSVImporter` - please install `bw2io` with `pip install bw2io[multifunctional]` or install `multifunctional` and `bw_simapro_csv` manually.\u001b[0m\n",
      "\u001b[2m07:20:23+0100\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mExtracted 23523 datasets in 317.64 seconds\u001b[0m\n",
      "Applying strategy: normalize_units\n",
      "Applying strategy: update_ecoinvent_locations\n",
      "Applying strategy: remove_zero_amount_coproducts\n",
      "Applying strategy: remove_zero_amount_inputs_with_no_activity\n",
      "Applying strategy: remove_unnamed_parameters\n",
      "Applying strategy: es2_assign_only_product_with_amount_as_reference_product\n",
      "Applying strategy: assign_single_product_as_activity\n",
      "Applying strategy: create_composite_code\n",
      "Applying strategy: drop_unspecified_subcategories\n",
      "Applying strategy: fix_ecoinvent_flows_pre35\n",
      "Applying strategy: drop_temporary_outdated_biosphere_flows\n",
      "Applying strategy: link_biosphere_by_flow_uuid\n",
      "Applying strategy: link_internal_technosphere_by_composite_code\n",
      "Applying strategy: delete_exchanges_missing_activity\n",
      "Applying strategy: delete_ghost_exchanges\n",
      "Applying strategy: remove_uncertainty_from_negative_loss_exchanges\n",
      "Applying strategy: fix_unreasonably_high_lognormal_uncertainties\n",
      "Applying strategy: convert_activity_parameters_to_list\n",
      "Applying strategy: add_cpc_classification_from_single_reference_product\n",
      "Applying strategy: delete_none_synonyms\n",
      "Applying strategy: update_social_flows_in_older_consequential\n",
      "Applying strategy: set_lognormal_loc_value\n",
      "Applied 22 strategies in 10.37 seconds\n",
      "Graph statistics for `ecoinvent-3.10.1-cutoff` importer:\n",
      "23523 graph nodes:\n",
      "\tprocess: 23523\n",
      "743409 graph edges:\n",
      "\tbiosphere: 449622\n",
      "\ttechnosphere: 270264\n",
      "\tproduction: 23523\n",
      "743409 edges to the following databases:\n",
      "\tecoinvent-3.10.1-biosphere: 449622\n",
      "\tecoinvent-3.10.1-cutoff: 293787\n",
      "0 unique unlinked edges (0 total):\n",
      "\n",
      "\n",
      "\u001b[2m07:20:35+0100\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mNot able to determine geocollections for all datasets. This database is not ready for regionalization.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23523/23523 [00:40<00:00, 573.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m07:21:16+0100\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mVacuuming database            \u001b[0m\n",
      "Created database: ecoinvent-3.10.1-cutoff\n"
     ]
    }
   ],
   "source": [
    "bi.import_ecoinvent_release(\n",
    "    version=\"3.10.1\",\n",
    "    system_model=\"cutoff\",\n",
    "    username=ecoinvent_username,\n",
    "    password=ecoinvent_password,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating prospective databases with `premise`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from premise import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "premise v.(2, 3, 0, 'dev1')\n",
      "+------------------------------------------------------------------+\n",
      "| Warning                                                          |\n",
      "+------------------------------------------------------------------+\n",
      "| Because some of the scenarios can yield LCI databases            |\n",
      "| containing net negative emission technologies (NET),             |\n",
      "| it is advised to account for biogenic CO2 flows when calculating |\n",
      "| Global Warming potential indicators.                             |\n",
      "| `premise_gwp` provides characterization factors for such flows.  |\n",
      "| It also provides factors for hydrogen emissions to air.          |\n",
      "|                                                                  |\n",
      "| Within your Brightway project:                                   |\n",
      "| from premise_gwp import add_premise_gwp                          |\n",
      "| add_premise_gwp()                                                |\n",
      "+------------------------------------------------------------------+\n",
      "+--------------------------------+----------------------------------+\n",
      "| Utils functions                | Description                      |\n",
      "+--------------------------------+----------------------------------+\n",
      "| clear_cache()                  | Clears the cache folder. Useful  |\n",
      "|                                | when updating `premise`or        |\n",
      "|                                | encountering issues with         |\n",
      "|                                | inventories.                     |\n",
      "| get_regions_definition(model)  | Retrieves the list of countries  |\n",
      "|                                | for each region of the model.    |\n",
      "| ndb.NewDatabase(...)           | Generates a summary of the most  |\n",
      "| ndb.generate_scenario_report() | important scenarios' variables.  |\n",
      "+--------------------------------+----------------------------------+\n",
      "Keep uncertainty data?\n",
      "NewDatabase(..., keep_source_db_uncertainty=True), keep_imports_uncertainty=True)\n",
      "\n",
      "Hide these messages?\n",
      "NewDatabase(..., quiet=True)\n",
      "- Extracting source database\n",
      "Cannot find cached database. Will create one now for next time...\n",
      "Getting activity data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23523/23523 [00:00<00:00, 451747.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding exchange data to activities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 743409/743409 [00:29<00:00, 25160.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling out exchange data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23523/23523 [00:02<00:00, 8833.73it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set missing location of datasets to global scope.\n",
      "Set missing location of production exchanges to scope of dataset.\n",
      "Correct missing location of technosphere exchanges.\n",
      "Correct missing flow categories for biosphere exchanges\n",
      "Remove empty exchanges.\n",
      "Remove uncertainty data.\n",
      "- Extracting inventories\n",
      "Cannot find cached inventories. Will create them now for next time...\n",
      "Importing default inventories...\n",
      "\n",
      "Extracted 1 worksheets in 0.07 seconds\n",
      "Migrating from 3.5 to 3.8 first\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Migrating from 3.8 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.01 seconds\n",
      "Migrating from 3.5 to 3.8 first\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Migrating from 3.8 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.00 seconds\n",
      "Migrating from 3.5 to 3.8 first\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Migrating from 3.8 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.02 seconds\n",
      "Migrating from 3.9 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "WARNING: missing classification for carbon dioxide, captured and stored, from a biomass fermentation plant | carbon dioxide, captured\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.01 seconds\n",
      "Migrating from 3.6 to 3.8 first\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Migrating from 3.8 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 4 worksheets in 0.12 seconds\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.01 seconds\n",
      "Migrating from 3.9 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 7 worksheets in 0.02 seconds\n",
      "Migrating from 3.8 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.06 seconds\n",
      "Migrating from 3.8 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.01 seconds\n",
      "Migrating from 3.8 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.01 seconds\n",
      "Migrating from 3.8 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "The following datasets to import already exist in the source database. They will not be imported\n",
      "+--------------------------------+--------------------------------+----------+--------------------------------+\n",
      "|              Name              |       Reference product        | Location |              File              |\n",
      "+--------------------------------+--------------------------------+----------+--------------------------------+\n",
      "| market for battery management  | battery management system, for |   GLO    | lci-batteries-NMC622-NMC532.xl |\n",
      "| battery management system prod | battery management system, for |   GLO    | lci-batteries-NMC622-NMC532.xl |\n",
      "+--------------------------------+--------------------------------+----------+--------------------------------+\n",
      "Extracted 1 worksheets in 0.01 seconds\n",
      "Migrating from 3.8 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "The following datasets to import already exist in the source database. They will not be imported\n",
      "+--------------------------------+--------------------------------+----------+-------------------------------+\n",
      "|              Name              |       Reference product        | Location |              File             |\n",
      "+--------------------------------+--------------------------------+----------+-------------------------------+\n",
      "| market for battery management  | battery management system, for |   GLO    | lci-batteries-NMC955-LTO.xlsx |\n",
      "| battery management system prod | battery management system, for |   GLO    | lci-batteries-NMC955-LTO.xlsx |\n",
      "| packaging production, for Li-i | battery module packaging, Li-i |   GLO    | lci-batteries-NMC955-LTO.xlsx |\n",
      "| management sytem production, f | management system, for Li-ion  |   GLO    | lci-batteries-NMC955-LTO.xlsx |\n",
      "| high voltage system, for Li-io | high voltage system, for Li-io |   GLO    | lci-batteries-NMC955-LTO.xlsx |\n",
      "| low voltage system, for Li-ion | low voltage system, for Li-ion |   GLO    | lci-batteries-NMC955-LTO.xlsx |\n",
      "| integrated interface system pr | management system, for Li-ion  |   GLO    | lci-batteries-NMC955-LTO.xlsx |\n",
      "| electrolyte, for Li-ion batter | electrolyte, for Li-ion batter |   GLO    | lci-batteries-NMC955-LTO.xlsx |\n",
      "| copper collector foil, for Li- | copper collector foil, for Li- |   GLO    | lci-batteries-NMC955-LTO.xlsx |\n",
      "| aluminium collector foil, for  | aluminium collector foil, for  |   GLO    | lci-batteries-NMC955-LTO.xlsx |\n",
      "|       battery separator        |       battery separator        |   GLO    | lci-batteries-NMC955-LTO.xlsx |\n",
      "|        vinyl carbonate         |      disodium disulphite       |   GLO    | lci-batteries-NMC955-LTO.xlsx |\n",
      "|         cell container         |         cell container         |   GLO    | lci-batteries-NMC955-LTO.xlsx |\n",
      "|        multilayer pouch        |        multilayer pouch        |   GLO    | lci-batteries-NMC955-LTO.xlsx |\n",
      "|          tab Aluminum          |          tab Aluminum          |   GLO    | lci-batteries-NMC955-LTO.xlsx |\n",
      "|           tab Copper           |           tab Copper           |   GLO    | lci-batteries-NMC955-LTO.xlsx |\n",
      "+--------------------------------+--------------------------------+----------+-------------------------------+\n",
      "Extracted 1 worksheets in 0.02 seconds\n",
      "Migrating from 3.9 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.01 seconds\n",
      "Migrating from 3.9 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.02 seconds\n",
      "Migrating from 3.9 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "The following datasets to import already exist in the source database. They will not be imported\n",
      "+--------------------------------+--------------------------------+----------+-----------------------------+\n",
      "|              Name              |       Reference product        | Location |             File            |\n",
      "+--------------------------------+--------------------------------+----------+-----------------------------+\n",
      "| market for styrene butadiene r | styrene butadiene rubber (SBR) |   RER    | lci-batteries-vanadium.xlsx |\n",
      "+--------------------------------+--------------------------------+----------+-----------------------------+\n",
      "Extracted 1 worksheets in 0.01 seconds\n",
      "Migrating from 3.9 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.02 seconds\n",
      "Migrating from 3.9 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.01 seconds\n",
      "Migrating from 3.9 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 2 worksheets in 0.01 seconds\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.01 seconds\n",
      "Migrating from 3.9 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.00 seconds\n",
      "Migrating from 3.11 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.27 seconds\n",
      "Migrating from 3.7 to 3.8 first\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Migrating from 3.8 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "The following datasets to import already exist in the source database. They will not be imported\n",
      "+--------------------------------+--------------------------------+----------+-------------+\n",
      "|              Name              |       Reference product        | Location |     File    |\n",
      "+--------------------------------+--------------------------------+----------+-------------+\n",
      "| electricity production, photov |    electricity, low voltage    |    CH    | lci-PV.xlsx |\n",
      "| electricity production, photov |    electricity, low voltage    |    CH    | lci-PV.xlsx |\n",
      "| electricity production, photov |    electricity, low voltage    |    CH    | lci-PV.xlsx |\n",
      "| electricity production, photov |    electricity, low voltage    |    CH    | lci-PV.xlsx |\n",
      "| electricity production, photov |    electricity, low voltage    |    CH    | lci-PV.xlsx |\n",
      "| electricity production, photov |    electricity, low voltage    |    CH    | lci-PV.xlsx |\n",
      "| fluorspar production, 97% puri |     fluorspar, 97% purity      |   GLO    | lci-PV.xlsx |\n",
      "| metallization paste production | metallization paste, back side |   RER    | lci-PV.xlsx |\n",
      "| metallization paste production | metallization paste, back side |   RER    | lci-PV.xlsx |\n",
      "| metallization paste production | metallization paste, front sid |   RER    | lci-PV.xlsx |\n",
      "|   multi-Si wafer production    |         multi-Si wafer         |   RER    | lci-PV.xlsx |\n",
      "| photovoltaic facade installati | photovoltaic facade installati |    CH    | lci-PV.xlsx |\n",
      "| photovoltaic facade installati | photovoltaic facade installati |    CH    | lci-PV.xlsx |\n",
      "| photovoltaic facade installati | photovoltaic facade installati |    CH    | lci-PV.xlsx |\n",
      "| photovoltaic facade installati | photovoltaic facade installati |    CH    | lci-PV.xlsx |\n",
      "| photovoltaic flat-roof install | photovoltaic flat-roof install |    CH    | lci-PV.xlsx |\n",
      "| photovoltaic flat-roof install | photovoltaic flat-roof install |    CH    | lci-PV.xlsx |\n",
      "| photovoltaic laminate producti | photovoltaic laminate, multi-S |   RER    | lci-PV.xlsx |\n",
      "| photovoltaic laminate producti | photovoltaic laminate, single- |   RER    | lci-PV.xlsx |\n",
      "| photovoltaic module production | photovoltaic module, building- |   RER    | lci-PV.xlsx |\n",
      "| photovoltaic module production | photovoltaic module, building- |   RER    | lci-PV.xlsx |\n",
      "| photovoltaic mounting system p | photovoltaic mounting system,  |   RER    | lci-PV.xlsx |\n",
      "| photovoltaic mounting system p | photovoltaic mounting system,  |   RER    | lci-PV.xlsx |\n",
      "| photovoltaic mounting system p | photovoltaic mounting system,  |   RER    | lci-PV.xlsx |\n",
      "| photovoltaic panel factory con |   photovoltaic panel factory   |   GLO    | lci-PV.xlsx |\n",
      "| photovoltaic panel production, | photovoltaic panel, multi-Si w |   RER    | lci-PV.xlsx |\n",
      "| photovoltaic panel production, | photovoltaic panel, single-Si  |   RER    | lci-PV.xlsx |\n",
      "| photovoltaic slanted-roof inst | photovoltaic slanted-roof inst |    CH    | lci-PV.xlsx |\n",
      "| photovoltaic slanted-roof inst | photovoltaic slanted-roof inst |    CH    | lci-PV.xlsx |\n",
      "| photovoltaic slanted-roof inst | photovoltaic slanted-roof inst |    CH    | lci-PV.xlsx |\n",
      "| photovoltaic slanted-roof inst | photovoltaic slanted-roof inst |    CH    | lci-PV.xlsx |\n",
      "| photovoltaic slanted-roof inst | photovoltaic slanted-roof inst |    CH    | lci-PV.xlsx |\n",
      "| photovoltaic slanted-roof inst | photovoltaic slanted-roof inst |    CH    | lci-PV.xlsx |\n",
      "|  polyvinylfluoride production  |       polyvinylfluoride        |    US    | lci-PV.xlsx |\n",
      "| polyvinylfluoride production,  | polyvinylfluoride, dispersion  |    US    | lci-PV.xlsx |\n",
      "| polyvinylfluoride, film produc |    polyvinylfluoride, film     |    US    | lci-PV.xlsx |\n",
      "| silicon production, metallurgi |  silicon, metallurgical grade  |    NO    | lci-PV.xlsx |\n",
      "| silicon production, multi-Si,  |   silicon, multi-Si, casted    |   RER    | lci-PV.xlsx |\n",
      "| silicon production, single cry | silicon, single crystal, Czoch |   RER    | lci-PV.xlsx |\n",
      "| silicon production, solar grad |      silicon, solar grade      |   RER    | lci-PV.xlsx |\n",
      "| single-Si wafer production, ph | single-Si wafer, photovoltaic  |   RER    | lci-PV.xlsx |\n",
      "|   vinyl fluoride production    |         vinyl fluoride         |    US    | lci-PV.xlsx |\n",
      "|   wafer factory construction   |         wafer factory          |    DE    | lci-PV.xlsx |\n",
      "+--------------------------------+--------------------------------+----------+-------------+\n",
      "Extracted 1 worksheets in 0.01 seconds\n",
      "Migrating from 3.8 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.03 seconds\n",
      "Migrating from 3.9 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.01 seconds\n",
      "Migrating from 3.9 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.01 seconds\n",
      "Migrating from 3.9 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.01 seconds\n",
      "Migrating from 3.7 to 3.8 first\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Migrating from 3.8 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.01 seconds\n",
      "Migrating from 3.7 to 3.8 first\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Migrating from 3.8 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.00 seconds\n",
      "Migrating from 3.7 to 3.8 first\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Migrating from 3.8 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "The following datasets to import already exist in the source database. They will not be imported\n",
      "+--------------------------------+--------------------------------+----------+--------------------------------+\n",
      "|              Name              |       Reference product        | Location |              File              |\n",
      "+--------------------------------+--------------------------------+----------+--------------------------------+\n",
      "| hydrogen production, coal gasi | hydrogen, gaseous, low pressur |   RoW    | lci-hydrogen-coal-gasification |\n",
      "| methanol production, coal gasi |            methanol            |   RoW    | lci-hydrogen-coal-gasification |\n",
      "+--------------------------------+--------------------------------+----------+--------------------------------+\n",
      "Extracted 1 worksheets in 0.00 seconds\n",
      "Migrating from 3.7 to 3.8 first\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Migrating from 3.8 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.01 seconds\n",
      "Migrating from 3.7 to 3.8 first\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Migrating from 3.8 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.01 seconds\n",
      "Migrating from 3.7 to 3.8 first\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Migrating from 3.8 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.01 seconds\n",
      "Migrating from 3.7 to 3.8 first\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Migrating from 3.8 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.01 seconds\n",
      "Migrating from 3.7 to 3.8 first\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Migrating from 3.8 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.01 seconds\n",
      "Migrating from 3.7 to 3.8 first\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Migrating from 3.8 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "The following datasets to import already exist in the source database. They will not be imported\n",
      "+--------------------------------+--------------------------------+----------+--------------------------------+\n",
      "|              Name              |       Reference product        | Location |              File              |\n",
      "+--------------------------------+--------------------------------+----------+--------------------------------+\n",
      "| hydrogen production, steam met | hydrogen, gaseous, low pressur |   RER    | lci-hydrogen-smr-atr-natgas.xl |\n",
      "+--------------------------------+--------------------------------+----------+--------------------------------+\n",
      "Extracted 1 worksheets in 0.00 seconds\n",
      "Migrating from 3.7 to 3.8 first\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Migrating from 3.8 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.00 seconds\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.00 seconds\n",
      "Migrating from 3.9 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.01 seconds\n",
      "Migrating from 3.9 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.01 seconds\n",
      "Migrating from 3.7 to 3.8 first\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Migrating from 3.8 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "The following datasets to import already exist in the source database. They will not be imported\n",
      "+--------------------------------+--------------------------------+----------+--------------------------------+\n",
      "|              Name              |       Reference product        | Location |              File              |\n",
      "+--------------------------------+--------------------------------+----------+--------------------------------+\n",
      "| methanol production facility,  | methanol production facility,  |   RER    | lci-synfuels-from-methanol-fro |\n",
      "+--------------------------------+--------------------------------+----------+--------------------------------+\n",
      "Extracted 1 worksheets in 0.02 seconds\n",
      "Migrating from 3.9 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.00 seconds\n",
      "Migrating from 3.7 to 3.8 first\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Migrating from 3.8 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 5 worksheets in 0.09 seconds\n",
      "Migrating from 3.7 to 3.8 first\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Migrating from 3.8 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.01 seconds\n",
      "Migrating from 3.7 to 3.8 first\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Migrating from 3.8 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.01 seconds\n",
      "Migrating from 3.7 to 3.8 first\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Migrating from 3.8 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.01 seconds\n",
      "Migrating from 3.7 to 3.8 first\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Migrating from 3.8 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.01 seconds\n",
      "Migrating from 3.7 to 3.8 first\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Migrating from 3.8 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.01 seconds\n",
      "Migrating from 3.7 to 3.8 first\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Migrating from 3.8 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.01 seconds\n",
      "Migrating from 3.7 to 3.8 first\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Migrating from 3.8 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.02 seconds\n",
      "Migrating from 3.6 to 3.8 first\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Migrating from 3.8 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.00 seconds\n",
      "Migrating from 3.8 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.05 seconds\n",
      "Migrating from 3.8 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.00 seconds\n",
      "Migrating from 3.8 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.01 seconds\n",
      "Migrating from 3.8 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.01 seconds\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.00 seconds\n",
      "Migrating from 3.9 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.01 seconds\n",
      "Migrating from 3.9 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.01 seconds\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.01 seconds\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.03 seconds\n",
      "Migrating from 3.9 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.01 seconds\n",
      "Migrating from 3.9 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 19 worksheets in 0.27 seconds\n",
      "Migrating from 3.7 to 3.8 first\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Migrating from 3.8 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.08 seconds\n",
      "Migrating from 3.7 to 3.8 first\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Migrating from 3.8 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.10 seconds\n",
      "Migrating from 3.7 to 3.8 first\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Migrating from 3.8 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "The following datasets to import already exist in the source database. They will not be imported\n",
      "+--------------------------------+--------------------------------+----------+----------------+\n",
      "|              Name              |       Reference product        | Location |      File      |\n",
      "+--------------------------------+--------------------------------+----------+----------------+\n",
      "| EV charger, level 3, plugin, 2 | EV charger, level 3, plugin, 2 |   RER    | lci-buses.xlsx |\n",
      "+--------------------------------+--------------------------------+----------+----------------+\n",
      "Extracted 1 worksheets in 0.68 seconds\n",
      "Migrating from 3.7 to 3.8 first\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Migrating from 3.8 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "The following datasets to import already exist in the source database. They will not be imported\n",
      "+-----------------+-------------------+----------+--------------------+\n",
      "|       Name      | Reference product | Location |        File        |\n",
      "+-----------------+-------------------+----------+--------------------+\n",
      "| HDPE tank liner |  HDPE tank liner  |   RER    | lci-pass_cars.xlsx |\n",
      "+-----------------+-------------------+----------+--------------------+\n",
      "Extracted 1 worksheets in 0.02 seconds\n",
      "Migrating from 3.9 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.01 seconds\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.02 seconds\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.00 seconds\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.01 seconds\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.00 seconds\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.08 seconds\n",
      "Remove uncertainty data.\n",
      "The following datasets to import already exist in the source database. They will not be imported\n",
      "+--------------------------------+--------------------------------+----------+-----------------------+\n",
      "|              Name              |       Reference product        | Location |          File         |\n",
      "+--------------------------------+--------------------------------+----------+-----------------------+\n",
      "| EV charger, level 3, plugin, 2 | EV charger, level 3, plugin, 2 |   RER    | lci-final-energy.xlsx |\n",
      "+--------------------------------+--------------------------------+----------+-----------------------+\n",
      "Extracted 3 worksheets in 0.00 seconds\n",
      "Migrating from 3.8 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Extracted 1 worksheets in 0.01 seconds\n",
      "Remove uncertainty data.\n",
      "Extracted 8 worksheets in 0.04 seconds\n",
      "Migrating from 3.9 to 3.10\n",
      "Applying strategy: migrate_datasets\n",
      "Applying strategy: migrate_exchanges\n",
      "Remove uncertainty data.\n",
      "Data cached. It is advised to restart your workflow at this point.\n",
      "This allows premise to use the cached data instead, which results in\n",
      "a faster workflow.\n",
      "- Fetching IAM data\n",
      "Found file: remind-eu_SSP2-PkBudg1000\n",
      "Reading remind-eu_SSP2-PkBudg1000 as CSV file\n",
      "Found file: remind-eu_SSP2-PkBudg1000\n",
      "Reading remind-eu_SSP2-PkBudg1000 as CSV file\n",
      "Found file: remind-eu_SSP2-PkBudg1000\n",
      "Reading remind-eu_SSP2-PkBudg1000 as CSV file\n",
      "Found file: remind-eu_SSP2-PkBudg1000\n",
      "Reading remind-eu_SSP2-PkBudg1000 as CSV file\n",
      "Found file: remind-eu_SSP2-PkBudg1000\n",
      "Reading remind-eu_SSP2-PkBudg1000 as CSV file\n",
      "Found file: remind-eu_SSP2-PkBudg1000\n",
      "Reading remind-eu_SSP2-PkBudg1000 as CSV file\n",
      "Found file: remind-eu_SSP2-PkBudg1000\n",
      "Reading remind-eu_SSP2-PkBudg1000 as CSV file\n",
      "Found file: remind-eu_SSP2-PkBudg650\n",
      "Reading remind-eu_SSP2-PkBudg650 as CSV file\n",
      "Found file: remind-eu_SSP2-PkBudg650\n",
      "Reading remind-eu_SSP2-PkBudg650 as CSV file\n",
      "Found file: remind-eu_SSP2-PkBudg650\n",
      "Reading remind-eu_SSP2-PkBudg650 as CSV file\n",
      "Found file: remind-eu_SSP2-PkBudg650\n",
      "Reading remind-eu_SSP2-PkBudg650 as CSV file\n",
      "Found file: remind-eu_SSP2-PkBudg650\n",
      "Reading remind-eu_SSP2-PkBudg650 as CSV file\n",
      "Found file: remind-eu_SSP2-PkBudg650\n",
      "Reading remind-eu_SSP2-PkBudg650 as CSV file\n",
      "Found file: remind-eu_SSP2-PkBudg650\n",
      "Reading remind-eu_SSP2-PkBudg650 as CSV file\n",
      "Found file: remind-eu_SSP2-NPi\n",
      "Reading remind-eu_SSP2-NPi as CSV file\n",
      "Found file: remind-eu_SSP2-NPi\n",
      "Reading remind-eu_SSP2-NPi as CSV file\n",
      "Found file: remind-eu_SSP2-NPi\n",
      "Reading remind-eu_SSP2-NPi as CSV file\n",
      "Found file: remind-eu_SSP2-NPi\n",
      "Reading remind-eu_SSP2-NPi as CSV file\n",
      "Found file: remind-eu_SSP2-NPi\n",
      "Reading remind-eu_SSP2-NPi as CSV file\n",
      "Found file: remind-eu_SSP2-NPi\n",
      "Reading remind-eu_SSP2-NPi as CSV file\n",
      "Found file: remind-eu_SSP2-NPi\n",
      "Reading remind-eu_SSP2-NPi as CSV file\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing scenarios for all sectors:   0%|    | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> MAJOR anomalies found during steel update: check the change report.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing scenarios for all sectors:  19%|▏| 4/21 [05:22<22:55, 80.90"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> MAJOR anomalies found during steel update: check the change report.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing scenarios for all sectors:  33%|▎| 7/21 [09:35<19:21, 82.95"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> MAJOR anomalies found during steel update: check the change report.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing scenarios for all sectors:  52%|▌| 11/21 [15:44<15:05, 90.5"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> MAJOR anomalies found during steel update: check the change report.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing scenarios for all sectors:  67%|▋| 14/21 [20:37<10:58, 94.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> MAJOR anomalies found during steel update: check the change report.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing scenarios for all sectors:  86%|▊| 18/21 [27:03<04:47, 95.9"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> MAJOR anomalies found during steel update: check the change report.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing scenarios for all sectors: 100%|█| 21/21 [32:14<00:00, 92.1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ndb = NewDatabase(\n",
    "    scenarios=[\n",
    "        {\"model\": \"remind-eu\", \"pathway\": \"SSP2-PkBudg1000\", \"year\": 2023},\n",
    "        {\"model\": \"remind-eu\", \"pathway\": \"SSP2-PkBudg1000\", \"year\": 2025},\n",
    "        {\"model\": \"remind-eu\", \"pathway\": \"SSP2-PkBudg1000\", \"year\": 2030},\n",
    "        {\"model\": \"remind-eu\", \"pathway\": \"SSP2-PkBudg1000\", \"year\": 2035},\n",
    "        {\"model\": \"remind-eu\", \"pathway\": \"SSP2-PkBudg1000\", \"year\": 2037},\n",
    "        {\"model\": \"remind-eu\", \"pathway\": \"SSP2-PkBudg1000\", \"year\": 2040},\n",
    "        {\"model\": \"remind-eu\", \"pathway\": \"SSP2-PkBudg1000\", \"year\": 2045},\n",
    "        \n",
    "        {\"model\": \"remind-eu\", \"pathway\": \"SSP2-PkBudg650\", \"year\": 2023},\n",
    "        {\"model\": \"remind-eu\", \"pathway\": \"SSP2-PkBudg650\", \"year\": 2025},\n",
    "        {\"model\": \"remind-eu\", \"pathway\": \"SSP2-PkBudg650\", \"year\": 2030},\n",
    "        {\"model\": \"remind-eu\", \"pathway\": \"SSP2-PkBudg650\", \"year\": 2035},\n",
    "        {\"model\": \"remind-eu\", \"pathway\": \"SSP2-PkBudg650\", \"year\": 2037},\n",
    "        {\"model\": \"remind-eu\", \"pathway\": \"SSP2-PkBudg650\", \"year\": 2040},\n",
    "        {\"model\": \"remind-eu\", \"pathway\": \"SSP2-PkBudg650\", \"year\": 2045},\n",
    "        \n",
    "        {\"model\": \"remind-eu\", \"pathway\": \"SSP2-NPi\", \"year\": 2023},\n",
    "        {\"model\": \"remind-eu\", \"pathway\": \"SSP2-NPi\", \"year\": 2025},\n",
    "        {\"model\": \"remind-eu\", \"pathway\": \"SSP2-NPi\", \"year\": 2030},\n",
    "        {\"model\": \"remind-eu\", \"pathway\": \"SSP2-NPi\", \"year\": 2035},\n",
    "        {\"model\": \"remind-eu\", \"pathway\": \"SSP2-NPi\", \"year\": 2037},\n",
    "        {\"model\": \"remind-eu\", \"pathway\": \"SSP2-NPi\", \"year\": 2040},\n",
    "        {\"model\": \"remind-eu\", \"pathway\": \"SSP2-NPi\", \"year\": 2045},\n",
    "    ],\n",
    "    source_db=\"ecoinvent-3.10.1-cutoff\",  # <-- name of the database in the BW2 project. Must be a string.\n",
    "    source_version=\"3.10\",  # <-- version of ecoinvent. Can be \"3.8\", \"3.9\" or \"3.10\". Must be a string.\n",
    "    key=premise_key,  # to be requested from the library maintainers if you want to use default scenarios included in `premise`\n",
    "    biosphere_name=\"ecoinvent-3.10.1-biosphere\",  # <-- name of the biosphere database in the BW2 project. Must be a string.\n",
    ")\n",
    "\n",
    "ndb.update()\n",
    "\n",
    "db_names = [\n",
    "    \"ei310_SSP2_PkBudg1000_2023\",\n",
    "    \"ei310_SSP2_PkBudg1000_2025\",\n",
    "    \"ei310_SSP2_PkBudg1000_2030\",\n",
    "    \"ei310_SSP2_PkBudg1000_2035\",\n",
    "    \"ei310_SSP2_PkBudg1000_2037\",\n",
    "    \"ei310_SSP2_PkBudg1000_2040\",\n",
    "    \"ei310_SSP2_PkBudg1000_2045\",\n",
    "    \"ei310_SSP2_PkBudg650_2023\",\n",
    "    \"ei310_SSP2_PkBudg650_2025\",\n",
    "    \"ei310_SSP2_PkBudg650_2030\",\n",
    "    \"ei310_SSP2_PkBudg650_2035\",\n",
    "    \"ei310_SSP2_PkBudg650_2037\",\n",
    "    \"ei310_SSP2_PkBudg650_2040\",\n",
    "    \"ei310_SSP2_PkBudg650_2045\",\n",
    "    \"ei310_SSP2_NPi_2023\",\n",
    "    \"ei310_SSP2_NPi_2025\",\n",
    "    \"ei310_SSP2_NPi_2030\",\n",
    "    \"ei310_SSP2_NPi_2035\",\n",
    "    \"ei310_SSP2_NPi_2037\",\n",
    "    \"ei310_SSP2_NPi_2040\",\n",
    "    \"ei310_SSP2_NPi_2045\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write new database(s) to Brightway.\n",
      "Running all checks...\n",
      "Minor anomalies found: check the change report.\n",
      "\u001b[2m08:41:25+0100\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mNot able to determine geocollections for all datasets. This database is not ready for regionalization.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 23638/40157 [00:55<00:26, 628.96it/s] /Users/timodiepers/anaconda3/envs/premise_v2.3.0dev1/lib/python3.11/site-packages/bw2data/backends/typos.py:90: UserWarning: Possible incorrect exchange key found: Given `Comment` but `comment` is more common\n",
      "  warnings.warn(\n",
      " 64%|██████▍   | 25790/40157 [00:58<00:42, 337.24it/s] /Users/timodiepers/anaconda3/envs/premise_v2.3.0dev1/lib/python3.11/site-packages/bw2data/backends/typos.py:90: UserWarning: Possible incorrect activity key found: Given `Comment` but `comment` is more common\n",
      "  warnings.warn(\n",
      "100%|██████████| 40157/40157 [01:39<00:00, 405.50it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m08:43:05+0100\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mVacuuming database            \u001b[0m\n",
      "Created database: ei310_SSP2_PkBudg1000_2023\n",
      "Running all checks...\n",
      "Minor anomalies found: check the change report.\n",
      "\u001b[2m08:45:00+0100\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mNot able to determine geocollections for all datasets. This database is not ready for regionalization.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 23649/40157 [00:50<00:21, 769.41it/s] /Users/timodiepers/anaconda3/envs/premise_v2.3.0dev1/lib/python3.11/site-packages/bw2data/backends/typos.py:90: UserWarning: Possible incorrect exchange key found: Given `Comment` but `comment` is more common\n",
      "  warnings.warn(\n",
      " 64%|██████▍   | 25785/40157 [00:53<00:43, 331.19it/s] /Users/timodiepers/anaconda3/envs/premise_v2.3.0dev1/lib/python3.11/site-packages/bw2data/backends/typos.py:90: UserWarning: Possible incorrect activity key found: Given `Comment` but `comment` is more common\n",
      "  warnings.warn(\n",
      "100%|██████████| 40157/40157 [01:35<00:00, 421.14it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m08:46:38+0100\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mVacuuming database            \u001b[0m\n",
      "Created database: ei310_SSP2_PkBudg1000_2025\n",
      "Running all checks...\n",
      "Minor anomalies found: check the change report.\n",
      "\u001b[2m08:48:28+0100\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mNot able to determine geocollections for all datasets. This database is not ready for regionalization.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 23567/40162 [01:07<00:23, 714.52it/s] /Users/timodiepers/anaconda3/envs/premise_v2.3.0dev1/lib/python3.11/site-packages/bw2data/backends/typos.py:90: UserWarning: Possible incorrect exchange key found: Given `Comment` but `comment` is more common\n",
      "  warnings.warn(\n",
      " 64%|██████▍   | 25784/40162 [01:10<00:43, 327.42it/s] /Users/timodiepers/anaconda3/envs/premise_v2.3.0dev1/lib/python3.11/site-packages/bw2data/backends/typos.py:90: UserWarning: Possible incorrect activity key found: Given `Comment` but `comment` is more common\n",
      "  warnings.warn(\n",
      "100%|██████████| 40162/40162 [01:48<00:00, 369.47it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m08:50:22+0100\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mVacuuming database            \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created database: ei310_SSP2_PkBudg1000_2030\n",
      "Running all checks...\n",
      "Minor anomalies found: check the change report.\n",
      "\u001b[2m08:52:20+0100\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mNot able to determine geocollections for all datasets. This database is not ready for regionalization.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 23635/40164 [01:17<00:25, 652.37it/s]/Users/timodiepers/anaconda3/envs/premise_v2.3.0dev1/lib/python3.11/site-packages/bw2data/backends/typos.py:90: UserWarning: Possible incorrect exchange key found: Given `Comment` but `comment` is more common\n",
      "  warnings.warn(\n",
      " 64%|██████▍   | 25754/40164 [01:20<00:50, 288.01it/s] /Users/timodiepers/anaconda3/envs/premise_v2.3.0dev1/lib/python3.11/site-packages/bw2data/backends/typos.py:90: UserWarning: Possible incorrect activity key found: Given `Comment` but `comment` is more common\n",
      "  warnings.warn(\n",
      "100%|██████████| 40164/40164 [02:00<00:00, 332.55it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m08:54:29+0100\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mVacuuming database            \u001b[0m\n",
      "Created database: ei310_SSP2_PkBudg1000_2035\n",
      "Running all checks...\n",
      "Minor anomalies found: check the change report.\n",
      "\u001b[2m08:57:00+0100\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mNot able to determine geocollections for all datasets. This database is not ready for regionalization.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 23610/40164 [01:06<02:23, 115.13it/s] /Users/timodiepers/anaconda3/envs/premise_v2.3.0dev1/lib/python3.11/site-packages/bw2data/backends/typos.py:90: UserWarning: Possible incorrect exchange key found: Given `Comment` but `comment` is more common\n",
      "  warnings.warn(\n",
      " 64%|██████▍   | 25773/40164 [01:09<00:49, 292.13it/s] /Users/timodiepers/anaconda3/envs/premise_v2.3.0dev1/lib/python3.11/site-packages/bw2data/backends/typos.py:90: UserWarning: Possible incorrect activity key found: Given `Comment` but `comment` is more common\n",
      "  warnings.warn(\n",
      "100%|██████████| 40164/40164 [01:48<00:00, 369.21it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m08:58:56+0100\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mVacuuming database            \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created database: ei310_SSP2_PkBudg1000_2037\n",
      "Running all checks...\n",
      "Minor anomalies found: check the change report.\n",
      "\u001b[2m09:01:29+0100\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mNot able to determine geocollections for all datasets. This database is not ready for regionalization.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 23568/40164 [00:50<00:24, 666.86it/s]/Users/timodiepers/anaconda3/envs/premise_v2.3.0dev1/lib/python3.11/site-packages/bw2data/backends/typos.py:90: UserWarning: Possible incorrect exchange key found: Given `Comment` but `comment` is more common\n",
      "  warnings.warn(\n",
      " 64%|██████▍   | 25764/40164 [01:10<02:20, 102.26it/s] /Users/timodiepers/anaconda3/envs/premise_v2.3.0dev1/lib/python3.11/site-packages/bw2data/backends/typos.py:90: UserWarning: Possible incorrect activity key found: Given `Comment` but `comment` is more common\n",
      "  warnings.warn(\n",
      "100%|██████████| 40164/40164 [01:50<00:00, 364.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m09:03:27+0100\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mVacuuming database            \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created database: ei310_SSP2_PkBudg1000_2040\n",
      "Running all checks...\n",
      "Minor anomalies found: check the change report.\n",
      "\u001b[2m09:06:22+0100\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mNot able to determine geocollections for all datasets. This database is not ready for regionalization.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 23639/40164 [00:55<00:23, 697.71it/s]/Users/timodiepers/anaconda3/envs/premise_v2.3.0dev1/lib/python3.11/site-packages/bw2data/backends/typos.py:90: UserWarning: Possible incorrect exchange key found: Given `Comment` but `comment` is more common\n",
      "  warnings.warn(\n",
      " 64%|██████▍   | 25762/40164 [00:58<00:58, 246.75it/s] /Users/timodiepers/anaconda3/envs/premise_v2.3.0dev1/lib/python3.11/site-packages/bw2data/backends/typos.py:90: UserWarning: Possible incorrect activity key found: Given `Comment` but `comment` is more common\n",
      "  warnings.warn(\n",
      "100%|██████████| 40164/40164 [01:42<00:00, 393.54it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m09:08:17+0100\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mVacuuming database            \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created database: ei310_SSP2_PkBudg1000_2045\n",
      "Running all checks...\n",
      "Minor anomalies found: check the change report.\n",
      "\u001b[2m09:11:43+0100\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mNot able to determine geocollections for all datasets. This database is not ready for regionalization.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 23596/40157 [00:55<00:25, 662.11it/s]/Users/timodiepers/anaconda3/envs/premise_v2.3.0dev1/lib/python3.11/site-packages/bw2data/backends/typos.py:90: UserWarning: Possible incorrect exchange key found: Given `Comment` but `comment` is more common\n",
      "  warnings.warn(\n",
      " 64%|██████▍   | 25762/40157 [00:59<00:49, 292.53it/s] /Users/timodiepers/anaconda3/envs/premise_v2.3.0dev1/lib/python3.11/site-packages/bw2data/backends/typos.py:90: UserWarning: Possible incorrect activity key found: Given `Comment` but `comment` is more common\n",
      "  warnings.warn(\n",
      "100%|██████████| 40157/40157 [01:38<00:00, 408.45it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m09:13:33+0100\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mVacuuming database            \u001b[0m\n",
      "Created database: ei310_SSP2_PkBudg650_2023\n",
      "Running all checks...\n",
      "Minor anomalies found: check the change report.\n",
      "\u001b[2m09:18:35+0100\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mNot able to determine geocollections for all datasets. This database is not ready for regionalization.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 23639/40157 [00:52<00:23, 717.85it/s]/Users/timodiepers/anaconda3/envs/premise_v2.3.0dev1/lib/python3.11/site-packages/bw2data/backends/typos.py:90: UserWarning: Possible incorrect exchange key found: Given `Comment` but `comment` is more common\n",
      "  warnings.warn(\n",
      " 64%|██████▍   | 25780/40157 [00:55<00:47, 302.10it/s] /Users/timodiepers/anaconda3/envs/premise_v2.3.0dev1/lib/python3.11/site-packages/bw2data/backends/typos.py:90: UserWarning: Possible incorrect activity key found: Given `Comment` but `comment` is more common\n",
      "  warnings.warn(\n",
      "100%|██████████| 40157/40157 [01:37<00:00, 413.45it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m09:20:25+0100\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mVacuuming database            \u001b[0m\n",
      "Created database: ei310_SSP2_PkBudg650_2025\n",
      "Running all checks...\n",
      "Minor anomalies found: check the change report.\n",
      "\u001b[2m09:24:35+0100\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mNot able to determine geocollections for all datasets. This database is not ready for regionalization.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 23639/40162 [00:49<00:23, 710.24it/s]/Users/timodiepers/anaconda3/envs/premise_v2.3.0dev1/lib/python3.11/site-packages/bw2data/backends/typos.py:90: UserWarning: Possible incorrect exchange key found: Given `Comment` but `comment` is more common\n",
      "  warnings.warn(\n",
      " 64%|██████▍   | 25776/40162 [00:52<00:45, 313.64it/s] /Users/timodiepers/anaconda3/envs/premise_v2.3.0dev1/lib/python3.11/site-packages/bw2data/backends/typos.py:90: UserWarning: Possible incorrect activity key found: Given `Comment` but `comment` is more common\n",
      "  warnings.warn(\n",
      "100%|██████████| 40162/40162 [01:30<00:00, 441.42it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m09:26:20+0100\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mVacuuming database            \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created database: ei310_SSP2_PkBudg650_2030\n",
      "Running all checks...\n",
      "Minor anomalies found: check the change report.\n",
      "\u001b[2m09:30:30+0100\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mNot able to determine geocollections for all datasets. This database is not ready for regionalization.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 23639/40164 [00:51<00:23, 712.96it/s]/Users/timodiepers/anaconda3/envs/premise_v2.3.0dev1/lib/python3.11/site-packages/bw2data/backends/typos.py:90: UserWarning: Possible incorrect exchange key found: Given `Comment` but `comment` is more common\n",
      "  warnings.warn(\n",
      " 64%|██████▍   | 25773/40164 [00:54<00:46, 312.28it/s] /Users/timodiepers/anaconda3/envs/premise_v2.3.0dev1/lib/python3.11/site-packages/bw2data/backends/typos.py:90: UserWarning: Possible incorrect activity key found: Given `Comment` but `comment` is more common\n",
      "  warnings.warn(\n",
      "100%|██████████| 40164/40164 [01:34<00:00, 424.18it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m09:32:20+0100\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mVacuuming database            \u001b[0m\n",
      "Created database: ei310_SSP2_PkBudg650_2035\n",
      "Running all checks...\n",
      "Minor anomalies found: check the change report.\n",
      "\u001b[2m09:36:34+0100\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mNot able to determine geocollections for all datasets. This database is not ready for regionalization.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 23630/40164 [00:49<00:23, 717.01it/s] /Users/timodiepers/anaconda3/envs/premise_v2.3.0dev1/lib/python3.11/site-packages/bw2data/backends/typos.py:90: UserWarning: Possible incorrect exchange key found: Given `Comment` but `comment` is more common\n",
      "  warnings.warn(\n",
      " 64%|██████▍   | 25771/40164 [00:52<00:50, 286.22it/s] /Users/timodiepers/anaconda3/envs/premise_v2.3.0dev1/lib/python3.11/site-packages/bw2data/backends/typos.py:90: UserWarning: Possible incorrect activity key found: Given `Comment` but `comment` is more common\n",
      "  warnings.warn(\n",
      "100%|██████████| 40164/40164 [01:30<00:00, 444.17it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m09:38:24+0100\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mVacuuming database            \u001b[0m\n",
      "Created database: ei310_SSP2_PkBudg650_2037\n",
      "Running all checks...\n",
      "Minor anomalies found: check the change report.\n",
      "\u001b[2m09:43:18+0100\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mNot able to determine geocollections for all datasets. This database is not ready for regionalization.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 23642/40164 [00:48<00:23, 695.50it/s]/Users/timodiepers/anaconda3/envs/premise_v2.3.0dev1/lib/python3.11/site-packages/bw2data/backends/typos.py:90: UserWarning: Possible incorrect exchange key found: Given `Comment` but `comment` is more common\n",
      "  warnings.warn(\n",
      " 64%|██████▍   | 25775/40164 [00:52<00:57, 252.07it/s] /Users/timodiepers/anaconda3/envs/premise_v2.3.0dev1/lib/python3.11/site-packages/bw2data/backends/typos.py:90: UserWarning: Possible incorrect activity key found: Given `Comment` but `comment` is more common\n",
      "  warnings.warn(\n",
      "100%|██████████| 40164/40164 [01:32<00:00, 432.39it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m09:45:12+0100\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mVacuuming database            \u001b[0m\n",
      "Created database: ei310_SSP2_PkBudg650_2040\n",
      "Running all checks...\n",
      "Minor anomalies found: check the change report.\n",
      "\u001b[2m09:51:10+0100\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mNot able to determine geocollections for all datasets. This database is not ready for regionalization.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 23626/40164 [01:07<00:24, 663.42it/s]/Users/timodiepers/anaconda3/envs/premise_v2.3.0dev1/lib/python3.11/site-packages/bw2data/backends/typos.py:90: UserWarning: Possible incorrect exchange key found: Given `Comment` but `comment` is more common\n",
      "  warnings.warn(\n",
      " 64%|██████▍   | 25763/40164 [01:11<01:01, 235.25it/s]/Users/timodiepers/anaconda3/envs/premise_v2.3.0dev1/lib/python3.11/site-packages/bw2data/backends/typos.py:90: UserWarning: Possible incorrect activity key found: Given `Comment` but `comment` is more common\n",
      "  warnings.warn(\n",
      "100%|██████████| 40164/40164 [02:00<00:00, 333.20it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m09:53:41+0100\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mVacuuming database            \u001b[0m\n",
      "Created database: ei310_SSP2_PkBudg650_2045\n",
      "Running all checks...\n",
      "Minor anomalies found: check the change report.\n",
      "\u001b[2m09:59:04+0100\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mNot able to determine geocollections for all datasets. This database is not ready for regionalization.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 23564/40157 [00:48<00:23, 702.96it/s] /Users/timodiepers/anaconda3/envs/premise_v2.3.0dev1/lib/python3.11/site-packages/bw2data/backends/typos.py:90: UserWarning: Possible incorrect exchange key found: Given `Comment` but `comment` is more common\n",
      "  warnings.warn(\n",
      " 64%|██████▍   | 25766/40157 [00:51<00:47, 303.25it/s] /Users/timodiepers/anaconda3/envs/premise_v2.3.0dev1/lib/python3.11/site-packages/bw2data/backends/typos.py:90: UserWarning: Possible incorrect activity key found: Given `Comment` but `comment` is more common\n",
      "  warnings.warn(\n",
      "100%|██████████| 40157/40157 [01:29<00:00, 446.20it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m10:01:02+0100\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mVacuuming database            \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created database: ei310_SSP2_NPi_2023\n",
      "Running all checks...\n",
      "Minor anomalies found: check the change report.\n",
      "\u001b[2m10:06:22+0100\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mNot able to determine geocollections for all datasets. This database is not ready for regionalization.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 23638/40157 [00:56<00:23, 693.58it/s]/Users/timodiepers/anaconda3/envs/premise_v2.3.0dev1/lib/python3.11/site-packages/bw2data/backends/typos.py:90: UserWarning: Possible incorrect exchange key found: Given `Comment` but `comment` is more common\n",
      "  warnings.warn(\n",
      " 64%|██████▍   | 25780/40157 [00:59<00:53, 269.89it/s] /Users/timodiepers/anaconda3/envs/premise_v2.3.0dev1/lib/python3.11/site-packages/bw2data/backends/typos.py:90: UserWarning: Possible incorrect activity key found: Given `Comment` but `comment` is more common\n",
      "  warnings.warn(\n",
      "100%|██████████| 40157/40157 [01:45<00:00, 380.26it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m10:08:40+0100\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mVacuuming database            \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created database: ei310_SSP2_NPi_2025\n",
      "Running all checks...\n",
      "Minor anomalies found: check the change report.\n",
      "\u001b[2m10:14:28+0100\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mNot able to determine geocollections for all datasets. This database is not ready for regionalization.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 23593/40158 [00:51<00:28, 581.04it/s]/Users/timodiepers/anaconda3/envs/premise_v2.3.0dev1/lib/python3.11/site-packages/bw2data/backends/typos.py:90: UserWarning: Possible incorrect exchange key found: Given `Comment` but `comment` is more common\n",
      "  warnings.warn(\n",
      " 64%|██████▍   | 25766/40158 [00:55<00:47, 304.33it/s] /Users/timodiepers/anaconda3/envs/premise_v2.3.0dev1/lib/python3.11/site-packages/bw2data/backends/typos.py:90: UserWarning: Possible incorrect activity key found: Given `Comment` but `comment` is more common\n",
      "  warnings.warn(\n",
      "100%|██████████| 40158/40158 [01:38<00:00, 408.80it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m10:16:36+0100\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mVacuuming database            \u001b[0m\n",
      "Created database: ei310_SSP2_NPi_2030\n",
      "Running all checks...\n",
      "Minor anomalies found: check the change report.\n",
      "\u001b[2m10:22:55+0100\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mNot able to determine geocollections for all datasets. This database is not ready for regionalization.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 23637/40161 [00:55<00:28, 570.01it/s]/Users/timodiepers/anaconda3/envs/premise_v2.3.0dev1/lib/python3.11/site-packages/bw2data/backends/typos.py:90: UserWarning: Possible incorrect exchange key found: Given `Comment` but `comment` is more common\n",
      "  warnings.warn(\n",
      " 64%|██████▍   | 25801/40161 [00:58<00:42, 337.59it/s] /Users/timodiepers/anaconda3/envs/premise_v2.3.0dev1/lib/python3.11/site-packages/bw2data/backends/typos.py:90: UserWarning: Possible incorrect activity key found: Given `Comment` but `comment` is more common\n",
      "  warnings.warn(\n",
      "100%|██████████| 40161/40161 [01:40<00:00, 398.29it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m10:25:11+0100\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mVacuuming database            \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created database: ei310_SSP2_NPi_2035\n"
     ]
    }
   ],
   "source": [
    "ndb.write_db_to_brightway(db_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding premise_gwp LCIA method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`premise_gwp` requires the name of your biosphere database.\n",
      "Please enter the name of your biosphere database as it appears in your project.\n",
      "Databases dictionary with 23 objects, including:\n",
      "\tecoinvent-3.10.1-biosphere\n",
      "\tecoinvent-3.10.1-cutoff\n",
      "\tei310_SSP2_NPi_2023\n",
      "\tei310_SSP2_NPi_2025\n",
      "\tei310_SSP2_NPi_2030\n",
      "\tei310_SSP2_NPi_2035\n",
      "\tei310_SSP2_NPi_2037\n",
      "\tei310_SSP2_NPi_2040\n",
      "\tei310_SSP2_NPi_2045\n",
      "\tei310_SSP2_PkBudg1000_2023\n",
      "Use `list(this object)` to get the complete list.\n",
      "Using biosphere database: ecoinvent-3.10.1-biosphere (version (0, 8, 12))\n",
      "Adding ('IPCC 2021', 'climate change', 'GWP 100a, incl. H and bio CO2')\n",
      "Converting to ei 3.10 biosphere names\n",
      "Applying strategy: csv_restore_tuples\n",
      "Applying strategy: csv_numerize\n",
      "Applying strategy: csv_drop_unknown\n",
      "Applying strategy: set_biosphere_type\n",
      "Applying strategy: drop_unspecified_subcategories\n",
      "Applying strategy: link_iterable_by_fields\n",
      "Applying strategy: drop_falsey_uncertainty_fields_but_keep_zeros\n",
      "Applying strategy: convert_uncertainty_types_to_integers\n",
      "Applied 8 strategies in 0.04 seconds\n",
      "Applying strategy: drop_unlinked_cfs\n",
      "Applied 1 strategies in 0.00 seconds\n",
      "Wrote 1 LCIA methods with 208 characterization factors\n",
      "Adding ('IPCC 2021', 'climate change', 'GWP 20a, incl. H')\n",
      "Converting to ei 3.10 biosphere names\n",
      "Applying strategy: csv_restore_tuples\n",
      "Applying strategy: csv_numerize\n",
      "Applying strategy: csv_drop_unknown\n",
      "Applying strategy: set_biosphere_type\n",
      "Applying strategy: drop_unspecified_subcategories\n",
      "Applying strategy: link_iterable_by_fields\n",
      "Applying strategy: drop_falsey_uncertainty_fields_but_keep_zeros\n",
      "Applying strategy: convert_uncertainty_types_to_integers\n",
      "Applied 8 strategies in 0.04 seconds\n",
      "Applying strategy: drop_unlinked_cfs\n",
      "Applied 1 strategies in 0.00 seconds\n",
      "Wrote 1 LCIA methods with 202 characterization factors\n",
      "Adding ('IPCC 2021', 'climate change', 'GWP 20a, incl. H and bio CO2')\n",
      "Converting to ei 3.10 biosphere names\n",
      "Applying strategy: csv_restore_tuples\n",
      "Applying strategy: csv_numerize\n",
      "Applying strategy: csv_drop_unknown\n",
      "Applying strategy: set_biosphere_type\n",
      "Applying strategy: drop_unspecified_subcategories\n",
      "Applying strategy: link_iterable_by_fields\n",
      "Applying strategy: drop_falsey_uncertainty_fields_but_keep_zeros\n",
      "Applying strategy: convert_uncertainty_types_to_integers\n",
      "Applied 8 strategies in 0.08 seconds\n",
      "Applying strategy: drop_unlinked_cfs\n",
      "Applied 1 strategies in 0.00 seconds\n",
      "Wrote 1 LCIA methods with 208 characterization factors\n",
      "Adding ('IPCC 2021', 'climate change', 'GWP 100a, incl. H')\n",
      "Converting to ei 3.10 biosphere names\n",
      "Applying strategy: csv_restore_tuples\n",
      "Applying strategy: csv_numerize\n",
      "Applying strategy: csv_drop_unknown\n",
      "Applying strategy: set_biosphere_type\n",
      "Applying strategy: drop_unspecified_subcategories\n",
      "Applying strategy: link_iterable_by_fields\n",
      "Applying strategy: drop_falsey_uncertainty_fields_but_keep_zeros\n",
      "Applying strategy: convert_uncertainty_types_to_integers\n",
      "Applied 8 strategies in 0.05 seconds\n",
      "Applying strategy: drop_unlinked_cfs\n",
      "Applied 1 strategies in 0.00 seconds\n",
      "Wrote 1 LCIA methods with 202 characterization factors\n"
     ]
    }
   ],
   "source": [
    "from premise_gwp import add_premise_gwp\n",
    "add_premise_gwp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the grid databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation\n",
    "\n",
    "First, we load the background activities from ecoinvent. We write a function for this, as we need to do this for all our different background databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_inputs_from_background(database_name: str):\n",
    "    db = bd.Database(database_name)\n",
    "    return {\n",
    "        \"glass_wool_mat\": db.get(name=\"market for glass wool mat\", location=\"GLO\"),\n",
    "        \"porcelain\": db.get(name=\"market for ceramic tile\", location=\"GLO\"),\n",
    "        \"epoxy\": db.get(name=\"market for epoxy resin, liquid\", location=\"RER\"),\n",
    "        \"brass\": db.get(name=\"market for brass\", location=\"RoW\"),\n",
    "        \"paint\": db.get(name=\"market for electrostatic paint\", location=\"GLO\"),\n",
    "        \"paper\": db.get(name=\"market for paper, melamine impregnated\", location=\"RER\"),\n",
    "        \"rubber\": db.get(name=\"market for synthetic rubber\", location=\"GLO\"),\n",
    "        \"sulfur_hexafluoride\": db.get(\n",
    "            name=\"market for sulfur hexafluoride, liquid\", location=\"RER\"\n",
    "        ),\n",
    "        \"lead\": db.get(name=\"market for lead\", location=\"GLO\"),\n",
    "        \"bronze\": db.get(name=\"market for bronze\", location=\"GLO\"),\n",
    "        \"polypropylene\": db.get(\n",
    "            name=\"market for polypropylene, granulate\", location=\"GLO\"\n",
    "        ),\n",
    "        \"steel_sheet\": db.get(name=\"market for sheet rolling, steel\", location=\"GLO\"),\n",
    "        \"steel_hot_rolled\": db.get(\n",
    "            name=\"market for steel, low-alloyed, hot rolled\", location=\"GLO\"\n",
    "        ),\n",
    "        \"steel_lowalloyed\": db.get(\n",
    "            name=\"market for steel, low-alloyed\", location=\"GLO\"\n",
    "        ),\n",
    "        \"steel_unalloyed\": db.get(name=\"market for steel, unalloyed\", location=\"GLO\"),\n",
    "        \"transformer_oil\": db.get(name=\"market for lubricating oil\", location=\"RER\"),\n",
    "        \"aluminium_wrought_alloy\": db.get(\n",
    "            name=\"market for aluminium, wrought alloy\", location=\"GLO\"\n",
    "        ),\n",
    "        \"aluminium_sheet\": db.get(\n",
    "            name=\"market for sheet rolling, aluminium\", location=\"GLO\"\n",
    "        ),\n",
    "        \"aluminium_cast\": db.get(\n",
    "            name=\"market for aluminium, cast alloy\", location=\"GLO\"\n",
    "        ),\n",
    "        \"copper\": db.get(name=\"market for copper, cathode\", location=\"GLO\"),\n",
    "        \"copper_wire\": db.get(name=\"wire drawing, copper\", location=\"RER\"),\n",
    "        \"glass_fibre\": db.get(name=\"market for glass fibre\", location=\"GLO\"),\n",
    "        \"kraft_paper\": db.get(name=\"market for kraft paper\", location=\"RER\"),\n",
    "        \"softwood\": db.get(\n",
    "            name=\"market for sawnwood, softwood, raw, dried (u=20%)\", location=\"RER\"\n",
    "        ),\n",
    "        \"concrete\": db.get(\n",
    "            name=\"market for concrete, normal strength\", location=\"RoW\"\n",
    "        ),  # density of 2200 kg/m3 assumed for conversion, see Itten et al.\n",
    "        \"zinc\": db.get(name=\"market for zinc\", location=\"GLO\"),\n",
    "        \"waste_concrete\": db.get(\n",
    "            name=\"market for waste concrete\", location=\"Europe without Switzerland\"\n",
    "        ),\n",
    "        \"waste_polyethylene\": db.get(\n",
    "            name=\"market for waste polyethylene\", location=\"DE\"\n",
    "        ),\n",
    "        \"waste_oil\": db.get(\n",
    "            name=\"market for waste mineral oil\", location=\"Europe without Switzerland\"\n",
    "        ),\n",
    "        \"excavation\": db.get(\n",
    "            name=\"market for excavation, hydraulic digger\", location=\"GLO\"\n",
    "        ),\n",
    "        \"polyester_resin\": db.get(\n",
    "            name=\"market for polyester resin, unsaturated\", location=\"RER\"\n",
    "        ),\n",
    "        \"steel_chromium_steel\": db.get(\n",
    "            name=\"market for steel, chromium steel 18/8\", location=\"GLO\"\n",
    "        ),\n",
    "        \"polycarbonate\": db.get(name=\"market for polycarbonate\", location=\"RER\"),\n",
    "        \"wood\": db.get(name=\"market for fibreboard, hard\", location=\"RER\"),\n",
    "        \"cement_unspecified\": db.get(\n",
    "            name=\"market for cement, unspecified\", location=\"Europe without Switzerland\"\n",
    "        ),\n",
    "        \"glass_tube_borosilicate\": db.get(\n",
    "            name=\"market for glass tube, borosilicate\", location=\"GLO\"\n",
    "        ),\n",
    "        \"extrusion_plastic_pipes\": db.get(\n",
    "            name=\"market for extrusion, plastic pipes\", location=\"GLO\"\n",
    "        ),\n",
    "        \"polyethylene\": db.get(\n",
    "            name=\"market for polyethylene, low density, granulate\", location=\"GLO\"\n",
    "        ),\n",
    "        \"copper_wire_drawing\": db.get(\n",
    "            name=\"market for wire drawing, copper\", location=\"GLO\"\n",
    "        ),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metadata for the grid components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (code, name, unit) of grid components\n",
    "COMPONENT_INFO = [\n",
    "    (\"disconnector\", \"Disconnector\", \"unit\"),\n",
    "    (\n",
    "        \"land_cable_oil_cu_150kv\",\n",
    "        \"Land cable, oil insulated, copper, 150kV\",\n",
    "        \"kilometer\",\n",
    "    ),\n",
    "    (\"overhead_line_400kv\", \"Overhead line, 400kV\", \"kilometer\"),\n",
    "    (\"overhead_line_150kv\", \"Overhead line, 150kV\", \"kilometer\"),\n",
    "    (\"overhead_line_10kv\", \"Overhead line, 10kV\", \"kilometer\"),\n",
    "    (\"overhead_line_04kv\", \"Overhead line, 0.4kV\", \"kilometer\"),\n",
    "    (\"overhead_line_HVDC\", \"Overhead line HVDC\", \"kilometer\"),\n",
    "    (\"land_cable_oil_cu_HVDC\", \"Land cable, oil insulated, copper, HVDC\", \"kilometer\"),\n",
    "    (\"transformer_250mva\", \"Transformer, 250MVA\", \"unit\"),\n",
    "    (\"transformer_40mva\", \"Transformer, 40MVA\", \"unit\"),\n",
    "    (\"transformer_315kva\", \"Transformer, 315kVA\", \"unit\"),\n",
    "    (\n",
    "        \"land_cable_vpe_al_04kv\",\n",
    "        \"Land cable, vpe insulated, aluminium, 0.4kV\",\n",
    "        \"kilometer\",\n",
    "    ),\n",
    "    (\n",
    "        \"land_cable_vpe_al_10kv\",\n",
    "        \"Land cable, vpe insulated, aluminium, 10kV\",\n",
    "        \"kilometer\",\n",
    "    ),\n",
    "    (\n",
    "        \"land_cable_vpe_al_10kv\",\n",
    "        \"Land cable, vpe insulated, aluminium, 10kV\",\n",
    "        \"kilometer\",\n",
    "    ),\n",
    "    (\"land_cable_epr_cu_11kv\", \"Land cable, epr insulated, copper, 11kV\", \"kilometer\"),\n",
    "    (\n",
    "        \"land_cable_vpe_al_50kv\",\n",
    "        \"Land cable, vpe insulated, aluminium, 50kV\",\n",
    "        \"kilometer\",\n",
    "    ),\n",
    "    (\"land_cable_vpe_cu_1kv\", \"Land cable, vpe insulated, copper, 1kV\", \"kilometer\"),\n",
    "    (\"gas_insulated_switchgear_420kv\", \"Gas insulated switchgear, 420kV\", \"unit\"),\n",
    "    (\"substation_lv\", \"Substation, LV\", \"unit\"),\n",
    "    (\"substation_mv\", \"Substation, MV\", \"unit\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to help create the grid component activities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_component_nodes(base_database_name: str, component_database_name: str, reset: bool=True):\n",
    "    if reset:\n",
    "        if component_database_name in bd.databases:\n",
    "            del bd.databases[component_database_name]\n",
    "        db_components = bd.Database(component_database_name)\n",
    "        db_components.register()\n",
    "    else :\n",
    "        db_components = bd.Database(component_database_name)\n",
    "    \n",
    "    INPUT_NODES = load_inputs_from_background(base_database_name)\n",
    "    \n",
    "    for code, name, unit in COMPONENT_INFO:\n",
    "        try:\n",
    "            component = db_components.new_node(\n",
    "                code=code, name=name, unit=unit, **{\"reference product\": name}\n",
    "            )\n",
    "        except:\n",
    "            db_components.get(code).delete()\n",
    "            component = db_components.new_node(\n",
    "                code=code, name=name, unit=unit, **{\"reference product\": name}\n",
    "            )\n",
    "            component.save()\n",
    "        component.save()\n",
    "        component.new_edge(\n",
    "            name=component[\"name\"], input=component, amount=1, unit=unit, type=\"production\"\n",
    "        ).save()\n",
    "\n",
    "        inputs_df = pd.read_csv(f\"data/{code}.csv\", sep=\";\", index_col=\"input\")\n",
    "        for input_code, amount, unit in zip(inputs_df.index, inputs_df.amount, inputs_df.unit):\n",
    "            component.new_edge(\n",
    "                input=INPUT_NODES[input_code], amount=amount, type=\"technosphere\"\n",
    "            ).save()\n",
    "\n",
    "        if code == \"gas_insulated_switchgear_420kv\":\n",
    "            sf6_node = bd.get_node(database=\"ecoinvent-3.10.1-biosphere\", name=\"Sulfur hexafluoride\", categories=(\"air\",))\n",
    "            component.new_edge(\n",
    "                input=sf6_node,\n",
    "                amount=28.6, # leakage\n",
    "                type=\"biosphere\",\n",
    "            ).save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need the grid expansion numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid compositions computed successfully.\n",
      "Uncertainty range: 5% in 2023 -> 20% in 2045 (triangular distribution)\n"
     ]
    }
   ],
   "source": [
    "import bw2data as bd\n",
    "\n",
    "# Base share values (central estimates)\n",
    "SHARE_DEFAULTS = {\n",
    "    \"oh\": {  # Overhead lines\n",
    "        \"ehv_ac\": 1.0,\n",
    "        \"ehv_dc\": 0.05,\n",
    "        \"hv\": 0.9474,\n",
    "        \"mv\": 0.1847,\n",
    "        \"lv\": 0.0652,\n",
    "    },\n",
    "    \"al\": {  # Aluminum cables\n",
    "        \"ehv\": 0.0,\n",
    "        \"hv\": 0.5,\n",
    "        \"mv\": 0.75,\n",
    "        \"lv\": 0.75,\n",
    "    }\n",
    "}\n",
    "\n",
    "# Total infrastructure quantities per BASE year (km or units)\n",
    "# Note: Interpolated years (2025, 2030, 2035, 2040) get their totals from distributed_components\n",
    "TOTAL_QUANTITIES = {\n",
    "    2023: {\n",
    "        \"ehv_ac\": 36400, \"ehv_dc\": 2172, \"hv\": 95200, \"mv\": 530200, \"lv\": 1570100,\n",
    "        \"transformer_250mva\": 709, \"transformer_40mva\": 7278, \"transformer_315kva\": 577790,\n",
    "        \"gas_insulated_switchgear_420kv\": 7278, \"substation_mv\": 7278 + 709, \"substation_lv\": 577790,\n",
    "        \"land_cable_epr_cu_11kv\": 117593,\n",
    "    },\n",
    "    2037: {\n",
    "        \"ehv_ac\": 11823, \"ehv_dc\": 17492, \"hv\": 28307, \"mv\": 117593, \"lv\": 375511,\n",
    "        \"transformer_250mva\": 221, \"transformer_40mva\": 1890, \"transformer_315kva\": 133273,\n",
    "        \"gas_insulated_switchgear_420kv\": 1890, \"substation_mv\": (1890+709)/1.58, \"substation_lv\": 133273,\n",
    "        \"land_cable_epr_cu_11kv\": 117593,\n",
    "    },\n",
    "    2045: {\n",
    "        \"ehv_ac\": 250, \"ehv_dc\": 4683, \"hv\": 15096, \"mv\": 64837, \"lv\": 198933,\n",
    "        \"transformer_250mva\": 59, \"transformer_40mva\": 1022, \"transformer_315kva\": 71943,\n",
    "        \"gas_insulated_switchgear_420kv\": 1022, \"substation_mv\": (1022+709)/1.58, \"substation_lv\": 71943,\n",
    "        \"land_cable_epr_cu_11kv\": 117593,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Component to voltage level mapping for parameter lookup\n",
    "# Format: component_key -> (param_type, level)\n",
    "# param_type: \"oh\" = overhead line, \"oh_al\" = underground (aluminum or copper)\n",
    "COMPONENT_LEVEL_MAP = {\n",
    "    \"overhead_line_400kv\": (\"oh\", \"ehv_ac\"),\n",
    "    \"overhead_line_HVDC\": (\"oh\", \"ehv_dc\"),\n",
    "    \"overhead_line_150kv\": (\"oh\", \"hv\"),\n",
    "    \"overhead_line_10kv\": (\"oh\", \"mv\"),\n",
    "    \"overhead_line_04kv\": (\"oh\", \"lv\"),\n",
    "    \"land_cable_oil_cu_HVDC\": (\"ug_cu\", \"ehv_dc\"),  # underground copper (HVDC)\n",
    "    \"land_cable_vpe_al_50kv\": (\"ug_al\", \"hv\"),      # underground aluminum\n",
    "    \"land_cable_oil_cu_150kv\": (\"ug_cu\", \"hv\"),    # underground copper\n",
    "    \"land_cable_vpe_al_10kv\": (\"ug_al\", \"mv\"),\n",
    "    \"land_cable_epr_cu_11kv\": (\"ug_cu\", \"mv\"),\n",
    "    \"land_cable_vpe_al_04kv\": (\"ug_al\", \"lv\"),\n",
    "    \"land_cable_vpe_cu_1kv\": (\"ug_cu\", \"lv\"),\n",
    "}\n",
    "\n",
    "def calculate_uncertainty_range(year):\n",
    "    \"\"\"\n",
    "    Calculate the percentage deviation for uncertainty bounds.\n",
    "    Progressive uncertainty: 5% in 2023, linearly increasing to 20% by 2045.\n",
    "    \"\"\"\n",
    "    min_year, max_year = 2023, 2045\n",
    "    min_uncertainty, max_uncertainty = 0.05, 0.20\n",
    "    \n",
    "    # Clamp year to range\n",
    "    year = max(min_year, min(max_year, year))\n",
    "    \n",
    "    # Linear interpolation\n",
    "    progress = (year - min_year) / (max_year - min_year)\n",
    "    return min_uncertainty + progress * (max_uncertainty - min_uncertainty)\n",
    "\n",
    "\n",
    "def setup_project_parameters(years):\n",
    "    \"\"\"\n",
    "    Registers year-specific parameters in the Brightway project.\n",
    "    Uses triangular distribution (uncertainty_type=5) with progressive uncertainty.\n",
    "    \"\"\"\n",
    "    params = []\n",
    "    \n",
    "    for year in years:\n",
    "        uncertainty_pct = calculate_uncertainty_range(year)\n",
    "        \n",
    "        # Overhead Line Share parameters\n",
    "        for lvl, val in SHARE_DEFAULTS[\"oh\"].items():\n",
    "            if val == 1.0:\n",
    "                minimum = max(0.0, val - uncertainty_pct)\n",
    "                maximum = 1.0\n",
    "            elif val == 0.0:\n",
    "                minimum = 0.0\n",
    "                maximum = min(1.0, val + uncertainty_pct)\n",
    "            else:\n",
    "                minimum = max(0.0, val * (1 - uncertainty_pct))\n",
    "                maximum = min(1.0, val * (1 + uncertainty_pct))\n",
    "            \n",
    "            params.append({\n",
    "                \"name\": f\"share_oh_{lvl}_{year}\",\n",
    "                \"amount\": val,\n",
    "                \"uncertainty type\": 5,  # Triangular distribution\n",
    "                \"loc\": val,\n",
    "                \"minimum\": minimum,\n",
    "                \"maximum\": maximum,\n",
    "            })\n",
    "        \n",
    "        # Aluminum Cable Share parameters\n",
    "        for lvl, val in SHARE_DEFAULTS[\"al\"].items():\n",
    "            if val == 1.0:\n",
    "                minimum = max(0.0, val - uncertainty_pct)\n",
    "                maximum = 1.0\n",
    "            elif val == 0.0:\n",
    "                minimum = 0.0\n",
    "                maximum = min(1.0, val + uncertainty_pct)\n",
    "            else:\n",
    "                minimum = max(0.0, val * (1 - uncertainty_pct))\n",
    "                maximum = min(1.0, val * (1 + uncertainty_pct))\n",
    "            \n",
    "            params.append({\n",
    "                \"name\": f\"share_al_{lvl}_{year}\",\n",
    "                \"amount\": val,\n",
    "                \"uncertainty type\": 5,\n",
    "                \"loc\": val,\n",
    "                \"minimum\": minimum,\n",
    "                \"maximum\": maximum,\n",
    "            })\n",
    "    \n",
    "    # Register parameters in the project\n",
    "    bd.parameters.new_project_parameters(params)\n",
    "    print(f\"Registered {len(params)} parameters for years: {years}\")\n",
    "    return params\n",
    "\n",
    "\n",
    "def get_base_total_for_component(component, current_value, year):\n",
    "    \"\"\"\n",
    "    Calculate the base total (before share multiplication) for a component.\n",
    "    This reverses the share calculation to get the original total.\n",
    "    \n",
    "    For interpolated years, we use the current_value and reverse-engineer the total.\n",
    "    \"\"\"\n",
    "    if component not in COMPONENT_LEVEL_MAP:\n",
    "        return None\n",
    "    \n",
    "    param_type, level = COMPONENT_LEVEL_MAP[component]\n",
    "    oh_share = SHARE_DEFAULTS[\"oh\"].get(level, 0)\n",
    "    al_level = \"ehv\" if level == \"ehv_dc\" else level\n",
    "    al_share = SHARE_DEFAULTS[\"al\"].get(al_level, 0)\n",
    "    \n",
    "    # Reverse the share calculation to get the base total\n",
    "    if param_type == \"oh\":\n",
    "        # current_value = oh_share * total\n",
    "        if oh_share > 0:\n",
    "            return current_value / oh_share\n",
    "        return current_value\n",
    "    elif param_type == \"ug_cu\":\n",
    "        # For HVDC: current_value = (1 - oh_share) * total\n",
    "        # For others: current_value = (1 - oh_share) * (1 - al_share) * total\n",
    "        if level == \"ehv_dc\":\n",
    "            if (1 - oh_share) > 0:\n",
    "                return current_value / (1 - oh_share)\n",
    "        else:\n",
    "            divisor = (1 - oh_share) * (1 - al_share)\n",
    "            if divisor > 0:\n",
    "                return current_value / divisor\n",
    "        return current_value\n",
    "    elif param_type == \"ug_al\":\n",
    "        # current_value = (1 - oh_share) * al_share * total\n",
    "        divisor = (1 - oh_share) * al_share\n",
    "        if divisor > 0:\n",
    "            return current_value / divisor\n",
    "        return current_value\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def get_formula_for_component(component, year, current_value):\n",
    "    \"\"\"\n",
    "    Constructs the formula string for an exchange based on the component type.\n",
    "    \n",
    "    Args:\n",
    "        component: The component key (e.g., \"overhead_line_400kv\")\n",
    "        year: The year for parameter lookup\n",
    "        current_value: The pre-computed value from distributed_components\n",
    "    \n",
    "    Logic:\n",
    "    - Overhead Line: share_oh * total\n",
    "    - Al Cable: (1 - share_oh) * share_al * total  \n",
    "    - Cu Cable: (1 - share_oh) * (1 - share_al) * total\n",
    "    \"\"\"\n",
    "    if component not in COMPONENT_LEVEL_MAP:\n",
    "        return None  # Fixed components (transformers, substations)\n",
    "    \n",
    "    param_type, level = COMPONENT_LEVEL_MAP[component]\n",
    "    \n",
    "    # Calculate the base total from the current value\n",
    "    total = get_base_total_for_component(component, current_value, year)\n",
    "    \n",
    "    if total is None or total == 0:\n",
    "        return None\n",
    "    \n",
    "    # Map level names for AL parameters (ehv_dc uses 'ehv')\n",
    "    al_level = \"ehv\" if level == \"ehv_dc\" else level\n",
    "    \n",
    "    if param_type == \"oh\":\n",
    "        # Pure overhead line: share_oh * total\n",
    "        return f\"share_oh_{level}_{year} * {total}\"\n",
    "    \n",
    "    elif param_type == \"ug_cu\":\n",
    "        if level == \"ehv_dc\":\n",
    "            # HVDC underground: (1 - share_oh) * total (no aluminum option)\n",
    "            return f\"(1 - share_oh_{level}_{year}) * {total}\"\n",
    "        else:\n",
    "            # Copper cable: (1 - share_oh) * (1 - share_al) * total\n",
    "            return f\"(1 - share_oh_{level}_{year}) * (1 - share_al_{al_level}_{year}) * {total}\"\n",
    "    \n",
    "    elif param_type == \"ug_al\":\n",
    "        # Aluminum cable: (1 - share_oh) * share_al * total\n",
    "        return f\"(1 - share_oh_{level}_{year}) * share_al_{al_level}_{year} * {total}\"\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "# Build grid_compositions dictionary with computed values\n",
    "# (These are the deterministic values; formulas will be applied to exchanges)\n",
    "grid_compositions = {}\n",
    "\n",
    "for year in [2023, 2037, 2045]:\n",
    "    q = TOTAL_QUANTITIES[year]\n",
    "    oh = SHARE_DEFAULTS[\"oh\"]\n",
    "    al = SHARE_DEFAULTS[\"al\"]\n",
    "    \n",
    "    grid_compositions[year] = {\n",
    "        # EHV AC\n",
    "        \"overhead_line_400kv\": oh[\"ehv_ac\"] * q[\"ehv_ac\"],\n",
    "        # EHV DC\n",
    "        \"overhead_line_HVDC\": oh[\"ehv_dc\"] * q[\"ehv_dc\"],\n",
    "        \"land_cable_oil_cu_HVDC\": (1 - oh[\"ehv_dc\"]) * q[\"ehv_dc\"],\n",
    "        # HV\n",
    "        \"overhead_line_150kv\": oh[\"hv\"] * q[\"hv\"],\n",
    "        \"land_cable_vpe_al_50kv\": (1 - oh[\"hv\"]) * al[\"hv\"] * q[\"hv\"],\n",
    "        \"land_cable_oil_cu_150kv\": (1 - oh[\"hv\"]) * (1 - al[\"hv\"]) * q[\"hv\"],\n",
    "        \"transformer_250mva\": q[\"transformer_250mva\"],\n",
    "        # MV\n",
    "        \"overhead_line_10kv\": oh[\"mv\"] * q[\"mv\"],\n",
    "        \"land_cable_vpe_al_10kv\": (1 - oh[\"mv\"]) * al[\"mv\"] * q[\"mv\"],\n",
    "        \"land_cable_epr_cu_11kv\": (1 - oh[\"mv\"]) * (1 - al[\"mv\"]) * q[\"land_cable_epr_cu_11kv\"],\n",
    "        \"transformer_40mva\": q[\"transformer_40mva\"],\n",
    "        \"gas_insulated_switchgear_420kv\": q[\"gas_insulated_switchgear_420kv\"],\n",
    "        \"substation_mv\": q[\"substation_mv\"],\n",
    "        # LV\n",
    "        \"overhead_line_04kv\": oh[\"lv\"] * q[\"lv\"],\n",
    "        \"land_cable_vpe_al_04kv\": (1 - oh[\"lv\"]) * al[\"lv\"] * q[\"lv\"],\n",
    "        \"land_cable_vpe_cu_1kv\": (1 - oh[\"lv\"]) * (1 - al[\"lv\"]) * q[\"lv\"],\n",
    "        \"transformer_315kva\": q[\"transformer_315kva\"],\n",
    "        \"substation_lv\": q[\"substation_lv\"],\n",
    "    }\n",
    "\n",
    "print(\"Grid compositions computed successfully.\")\n",
    "print(f\"Uncertainty range: 5% in 2023 -> 20% in 2045 (triangular distribution)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating grid nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid status quo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_status_quo = bd.Database(\"grid_status_quo\")\n",
    "background_2023 = bd.Database(\"ei310_SSP2_NPi_2023\") # common basis: ssp2 base scenario\n",
    "\n",
    "create_component_nodes(background_2023.name, db_status_quo.name)\n",
    "\n",
    "node_code = 'grid_status_quo'\n",
    "node_name = node_code\n",
    "\n",
    "try:\n",
    "    grid = db_status_quo.new_node(code=node_code, name=node_name)\n",
    "except:\n",
    "    existing_node = db_status_quo.get(node_code)\n",
    "    existing_node.delete()\n",
    "    grid = db_status_quo.new_node(code=node_code, name=node_name)\n",
    "grid.save()\n",
    "grid.new_edge(input=grid, amount=1, type='production').save()\n",
    "for key, value in grid_compositions[2023].items():\n",
    "    grid.new_edge(input=db_status_quo.get(key), amount=value, type='technosphere').save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregated material nodes for status quo, needed for the sankey diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "material_exchanges = {\n",
    "    'aluminium': {},\n",
    "    'steel': {},\n",
    "    'concrete': {},\n",
    "    'copper': {},\n",
    "    'plastics': {}\n",
    "}\n",
    "\n",
    "grid = bd.get_node(code=\"grid_status_quo\")\n",
    "for component_exchange in grid.technosphere():\n",
    "    for material_exchange in component_exchange.input.technosphere():\n",
    "        name = material_exchange.input['name'].lower()\n",
    "        key = None\n",
    "        if \"aluminium\" in name:\n",
    "            key = 'aluminium'\n",
    "        elif \"steel\" in name or \"iron\" in name:\n",
    "            key = 'steel'\n",
    "        elif \"concrete\" in name:\n",
    "            key = 'concrete'\n",
    "        elif \"copper\" in name:\n",
    "            key = 'copper'\n",
    "        elif \"polyethylene\" in name or \"polypropylene\" in name or \"plastic\" in name:\n",
    "            key = 'plastics'\n",
    "\n",
    "        total_amount = material_exchange.amount * component_exchange.amount\n",
    "        \n",
    "        if key:\n",
    "            if material_exchange.input in material_exchanges[key]:\n",
    "                material_exchanges[key][material_exchange.input] += total_amount\n",
    "            else:\n",
    "                material_exchanges[key][material_exchange.input] = total_amount\n",
    "        else:\n",
    "            if material_exchange.input in material_exchanges.setdefault('other', {}):\n",
    "                material_exchanges['other'][material_exchange.input] += total_amount\n",
    "            else:\n",
    "                material_exchanges['other'][material_exchange.input] = total_amount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_material_nodes = []\n",
    "for name, subexchanges in material_exchanges.items():\n",
    "    try:\n",
    "        mat = db_status_quo.new_node(\n",
    "            name=f\"aggregated material: {name}\"\n",
    "        )\n",
    "    except:\n",
    "        db_status_quo.get(name).delete()\n",
    "        mat = db_status_quo.new_node(\n",
    "            name=f\"aggregated material: {name}\"\n",
    "        )\n",
    "    mat.save()\n",
    "    aggregated_material_nodes.append(mat)\n",
    "    mat.new_exchange(input=mat, amount=1, type=\"production\").save()\n",
    "    for node, value in subexchanges.items():\n",
    "        mat.new_exchange(input=node, amount=value, type=\"technosphere\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid expansion nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributing grid expansion periods to sub-expansion periods that can be linked to prospective databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current year\n",
    "current_year = 2023\n",
    "expansion_period_1 = 2037 - current_year\n",
    "expansion_period_2 = 2045 - 2037\n",
    "\n",
    "# Target years for the distribution\n",
    "years_2037 = [2023, 2025, 2030, 2035, 2037]\n",
    "timespan_2037 = years_2037[-1] - years_2037[0]\n",
    "years_2045 = [2037, 2040, 2045]\n",
    "timespan_2045 = years_2045[-1] - years_2045[0]\n",
    "\n",
    "distributed_components = {}\n",
    "for index, year in enumerate(years_2037):\n",
    "    if year < 2037:\n",
    "        year_data = {}\n",
    "        duration = years_2037[index+1] - year \n",
    "        factor = duration / timespan_2037\n",
    "        \n",
    "        year_data = {}\n",
    "        for component, total_value in grid_compositions[2037].items():\n",
    "            year_data[component] = total_value * factor\n",
    "        distributed_components[year] = year_data\n",
    "        \n",
    "for index, year in enumerate(years_2045):\n",
    "    if year < 2045:\n",
    "        year_data = {}\n",
    "        duration = years_2045[index+1] - year \n",
    "        factor = duration / timespan_2045\n",
    "        \n",
    "        year_data = {}\n",
    "        for component, total_value in grid_compositions[2045].items():\n",
    "            year_data[component] = total_value * factor\n",
    "        distributed_components[year] = year_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the results for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json.dump(distributed_components, open(\"data/distributed_components.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making sure the distributed numbers sum up correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_distributed_sums(components, distributed_data):\n",
    "    original_totals = {}\n",
    "    for year in components:\n",
    "        if year != 2023:\n",
    "            for component, value in components[year].items():\n",
    "                original_totals[component] = original_totals.get(component, 0) + value\n",
    "\n",
    "    distributed_sums = {component: 0 for component in original_totals.keys()}\n",
    "    for year in distributed_data.values():\n",
    "        for component, value in year.items():\n",
    "            distributed_sums[component] += value\n",
    "\n",
    "    all_passed = True\n",
    "    for component, total in original_totals.items():\n",
    "        if not round(distributed_sums[component], 1) == round(total, 1):\n",
    "            print(f\"Test failed for {component}: distributed sum {distributed_sums[component]} != original total {total}\")\n",
    "            all_passed = False\n",
    "    assert all_passed\n",
    "\n",
    "test_result = test_distributed_sums(grid_compositions, distributed_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To link the prospective grid activities to the correct background databases, we need the info what database represents what year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_db_time_mapping = {\n",
    "    'ei310_SSP2_NPi_2023': 2023,\n",
    "    'ei310_SSP2_NPi_2025': 2025,\n",
    "    'ei310_SSP2_NPi_2030': 2030,\n",
    "    'ei310_SSP2_NPi_2035': 2035,\n",
    "    'ei310_SSP2_NPi_2037': 2037,\n",
    "    'ei310_SSP2_NPi_2040': 2040,\n",
    "    #    \n",
    "    'ei310_SSP2_PkBudg1000_2023': 2023,\n",
    "    'ei310_SSP2_PkBudg1000_2025': 2025,\n",
    "    'ei310_SSP2_PkBudg1000_2030': 2030,\n",
    "    'ei310_SSP2_PkBudg1000_2035': 2035,\n",
    "    'ei310_SSP2_PkBudg1000_2037': 2037,\n",
    "    'ei310_SSP2_PkBudg1000_2040': 2040,\n",
    "    #\n",
    "    'ei310_SSP2_PkBudg650_2023': 2023,\n",
    "    'ei310_SSP2_PkBudg650_2025': 2025,\n",
    "    'ei310_SSP2_PkBudg650_2030': 2030,\n",
    "    'ei310_SSP2_PkBudg650_2035': 2035,\n",
    "    'ei310_SSP2_PkBudg650_2037': 2037,\n",
    "    'ei310_SSP2_PkBudg650_2040': 2040,\n",
    "}\n",
    "\n",
    "ends_of_expansion_periods = {\n",
    "    2023: 2025,\n",
    "    2025: 2030,\n",
    "    2030: 2035,\n",
    "    2035: 2037,\n",
    "    2037: 2040,\n",
    "    2040: 2045,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the prospective grid activities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered 54 parameters for years: [2023, 2025, 2030, 2035, 2037, 2040]\n",
      "Cleaned up old group: grid_share_parameters\n",
      "Cleaned up old group: grid_expansion_prospective_params\n",
      "Created 18 grid expansion nodes with parameterized exchanges.\n"
     ]
    }
   ],
   "source": [
    "# --- Setup Parameters First ---\n",
    "# Get all unique years from the mapping\n",
    "all_years = sorted(set(background_db_time_mapping.values()))\n",
    "setup_project_parameters(all_years)\n",
    "\n",
    "# --- Clean up old parameter groups ---\n",
    "from bw2data.parameters import Group, ActivityParameter, ParameterizedExchange\n",
    "\n",
    "# Remove old/stale parameter groups that might cause conflicts\n",
    "old_groups = [\"grid_share_parameters\", \"grid_expansion_prospective_params\", \"grid_expansion_static_params\"]\n",
    "for group_name in old_groups:\n",
    "    try:\n",
    "        group = Group.get(name=group_name)\n",
    "        # Delete associated activity parameters\n",
    "        ActivityParameter.delete().where(ActivityParameter.group == group_name).execute()\n",
    "        # Delete associated parameterized exchanges\n",
    "        ParameterizedExchange.delete().where(ParameterizedExchange.group == group_name).execute()\n",
    "        # Delete the group\n",
    "        group.delete_instance()\n",
    "        print(f\"Cleaned up old group: {group_name}\")\n",
    "    except:\n",
    "        pass  # Group doesn't exist, that's fine\n",
    "\n",
    "# --- Create Grid Expansion Database with Parameterized Exchanges ---\n",
    "nodes_prospective_expansion = []\n",
    "db_expansion = bd.Database(\"grid_expansion_prospective\")\n",
    "if db_expansion.name in bd.databases:\n",
    "    del bd.databases[db_expansion.name]\n",
    "db_expansion.register()\n",
    "\n",
    "# Use database-specific parameter group\n",
    "param_group_prospective = \"grid_expansion_prospective_params\"\n",
    "\n",
    "for db_name, year in background_db_time_mapping.items():\n",
    "    scenario = db_name.split(\"_\")[2]\n",
    "    db_components = bd.Database(f\"grid_components_{scenario}_{year}\")\n",
    "    if year != 2023:\n",
    "        db_background = bd.Database(db_name)\n",
    "    else:\n",
    "        db_background = bd.Database(\"ei310_SSP2_NPi_2023\") # choose same basis for status quo\n",
    "\n",
    "    create_component_nodes(db_background.name, db_components.name)\n",
    "\n",
    "    node_code = f\"grid_{scenario}_{ends_of_expansion_periods[year]}\"\n",
    "    node_name = node_code\n",
    "\n",
    "    try:\n",
    "        grid = db_expansion.new_node(code=node_code, name=node_name)\n",
    "    except:\n",
    "        existing_node = db_expansion.get(node_code)\n",
    "        existing_node.delete()\n",
    "        grid = db_expansion.new_node(code=node_code, name=node_name)\n",
    "    grid.save()\n",
    "\n",
    "    nodes_prospective_expansion.append(grid)\n",
    "\n",
    "    grid.new_edge(input=grid, amount=1, type=\"production\").save()\n",
    "    \n",
    "    # Create exchanges with formulas for parameterized components\n",
    "    for key, value in distributed_components[year].items():\n",
    "        formula = get_formula_for_component(key, year, value)\n",
    "        \n",
    "        if formula:\n",
    "            # Create exchange with formula for parameterized components\n",
    "            edge = grid.new_edge(\n",
    "                input=db_components.get(key), \n",
    "                amount=value,  # Default amount\n",
    "                type=\"technosphere\",\n",
    "                formula=formula,\n",
    "            )\n",
    "        else:\n",
    "            # Fixed components (transformers, substations) - no formula\n",
    "            edge = grid.new_edge(\n",
    "                input=db_components.get(key), \n",
    "                amount=value, \n",
    "                type=\"technosphere\"\n",
    "            )\n",
    "        edge.save()\n",
    "    \n",
    "    # Add this activity to the database-specific parameter group\n",
    "    bd.parameters.add_exchanges_to_group(param_group_prospective, grid)\n",
    "\n",
    "# Recalculate only the specific group we just created\n",
    "ActivityParameter.recalculate(param_group_prospective)\n",
    "print(f\"Created {len(nodes_prospective_expansion)} grid expansion nodes with parameterized exchanges.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Static expansion for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 6 static expansion nodes with parameterized exchanges.\n"
     ]
    }
   ],
   "source": [
    "# Static expansion for comparison (also parameterized)\n",
    "nodes_static_expansion = []\n",
    "\n",
    "db_static_expansion = bd.Database(f\"grid_expansion_static\")\n",
    "if db_static_expansion.name in bd.databases:\n",
    "    del bd.databases[db_static_expansion.name]\n",
    "db_static_expansion.register()\n",
    "\n",
    "# Use database-specific parameter group\n",
    "param_group_static = \"grid_expansion_static_params\"\n",
    "\n",
    "years = [\n",
    "    2023,\n",
    "    2025,\n",
    "    2030,\n",
    "    2035,\n",
    "    2037,\n",
    "    2040,\n",
    "]\n",
    "\n",
    "for year in years:\n",
    "    node_code = f\"grid_static_{ends_of_expansion_periods[year]}\"\n",
    "    node_name = node_code\n",
    "    try:\n",
    "        grid = db_static_expansion.new_node(code=node_code, name=node_name)\n",
    "    except:\n",
    "        existing_node = db_static_expansion.get(node_code)\n",
    "        existing_node.delete()\n",
    "        grid = db_static_expansion.new_node(code=node_code, name=node_name)\n",
    "    grid.save()\n",
    "\n",
    "    nodes_static_expansion.append(grid)\n",
    "\n",
    "    grid.new_edge(input=grid, amount=1, type=\"production\").save()\n",
    "    \n",
    "    for key, value in distributed_components[year].items():\n",
    "        formula = get_formula_for_component(key, year, value)\n",
    "        \n",
    "        if formula:\n",
    "            # Create exchange with formula for parameterized components\n",
    "            edge = grid.new_edge(\n",
    "                input=db_status_quo.get(key), \n",
    "                amount=value,\n",
    "                type=\"technosphere\",\n",
    "                formula=formula,\n",
    "            )\n",
    "        else:\n",
    "            # Fixed components - no formula\n",
    "            edge = grid.new_edge(\n",
    "                input=db_status_quo.get(key), \n",
    "                amount=value, \n",
    "                type=\"technosphere\"\n",
    "            )\n",
    "        edge.save()\n",
    "    \n",
    "    # Add to database-specific parameter group\n",
    "    bd.parameters.add_exchanges_to_group(param_group_static, grid)\n",
    "\n",
    "# Recalculate only this specific group\n",
    "ActivityParameter.recalculate(param_group_static)\n",
    "print(f\"Created {len(nodes_static_expansion)} static expansion nodes with parameterized exchanges.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add electricity mixes for comparison of emissions per kWh electricity\n",
    "\n",
    "We base the assessment on the German market for electricity, low voltage and remove all grid-related exchanges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_mix_lv_2023 = bd.get_node(database=\"ei310_SSP2_NPi_2023\", name=\"market group for electricity, low voltage\", location=\"DEU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_mix_mv_2023 = bd.get_node(database=\"ei310_SSP2_NPi_2023\", name=\"market group for electricity, low voltage\", location=\"DEU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"new_mixes\" in bd.databases:\n",
    "    del bd.databases[\"new_mixes\"]\n",
    "db_new_mixes = bd.Database(\"new_mixes\")\n",
    "db_new_mixes.register()\n",
    "\n",
    "dbs = [\"ei310_SSP2_NPi_2023\", \"ei310_SSP2_NPi_2045\", \"ei310_SSP2_PkBudg1000_2045\", \"ei310_SSP2_PkBudg650_2045\"]\n",
    "\n",
    "for db, scenario, year in zip(dbs, [\"SS2_NPi\", \"SSP2_NPi\", \"SSP2-PkBudg1000\", \"SSP2-PkBudg650\"], [2023, 2045, 2045, 2045]):\n",
    "    # MV LEVEL\n",
    "    old_mix_mv_2023 = bd.get_node(database=db, name=\"market group for electricity, medium voltage\", location=\"DEU\")\n",
    "    activity_mv_name = f'market group for electricity, medium voltage DE {year} {scenario} NO GRID'\n",
    "    activity_mv_code = f'el_mv_{year}_DE_{scenario}'\n",
    "\n",
    "    try:\n",
    "        new_mix_mv = db_new_mixes.new_node(code=activity_mv_code, name=activity_mv_name)\n",
    "    except:\n",
    "        existing_activity = db_new_mixes.get(activity_mv_code)\n",
    "        existing_activity.delete()\n",
    "        new_mix_mv = db_new_mixes.new_activity(code=activity_mv_code, name=activity_mv_name, unit='kilowatt hour', location=\"DEU\", **{'reference product': activity_mv_name})\n",
    "    new_mix_mv.save()\n",
    "\n",
    "    for exc in old_mix_mv_2023.exchanges():\n",
    "        if 'transmission network' in exc['name'] or 'sulfur hexafluoride' in exc['name'].lower():\n",
    "            continue\n",
    "        if exc['type'] == 'production':\n",
    "            new_mix_mv.new_edge(input=new_mix_mv.key, amount=1, type='production').save()\n",
    "        elif exc[\"name\"] == \"market group for electricity, medium voltage\":\n",
    "            new_mix_mv.new_exchange(input=new_mix_mv, amount=exc['amount'], type=exc['type']).save()\n",
    "        else:\n",
    "            new_mix_mv.new_exchange(input=exc['input'], amount=exc['amount'], type=exc['type']).save()\n",
    "\n",
    "    # LV LEVEL\n",
    "    old_mix_lv = bd.get_node(database=db, name=\"market group for electricity, low voltage\", location=\"DEU\") \n",
    "    activity_lv_name = f'market group for electricity, low voltage DE {year} {scenario} NO GRID'\n",
    "    activity_lv_code = f'el_lv_{year}_DE_{scenario}'\n",
    "\n",
    "    try:\n",
    "        new_mix_lv = db_new_mixes.new_node(code=activity_lv_code, name=activity_lv_name)\n",
    "    except:\n",
    "        existing_activity = db_new_mixes.get(activity_lv_code)\n",
    "        existing_activity.delete()\n",
    "        new_mix_lv = db_new_mixes.new_activity(code=activity_lv_code, name=activity_lv_name, unit='kilowatt hour', location=\"DEU\", **{'reference product': activity_lv_name})\n",
    "    new_mix_lv.save()\n",
    "\n",
    "    for exc in old_mix_lv.exchanges():\n",
    "        if 'distribution network' in exc['name'] or 'sulfur hexafluoride' in exc['name'].lower():\n",
    "            continue\n",
    "        if exc['type'] == 'production':\n",
    "            new_mix_lv.new_edge(input=new_mix_lv.key, amount=1, type='production').save()\n",
    "        elif exc[\"name\"] == \"market group for electricity, low voltage\":\n",
    "            new_mix_lv.new_exchange(input=new_mix_lv, amount=exc['amount'], type=exc['type']).save()\n",
    "        elif exc[\"name\"] == \"market group for electricity, medium voltage\":\n",
    "            new_mix_lv.new_exchange(input=new_mix_mv, amount=exc['amount'], type=exc['type']).save()\n",
    "        else:\n",
    "            new_mix_lv.new_exchange(input=exc['input'], amount=exc['amount'], type=exc['type']).save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add additional scenario for recycled material shares in Aluminium Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Scrap] Scaling RER treatment of aluminium scrap, ...: 0.0253 -> 0.0306\n",
      "  [Scrap] Scaling RoW treatment of aluminium scrap, ...: 0.0551 -> 0.0666\n",
      "  [Scrap] Scaling RoW treatment of aluminium scrap, ...: 0.2217 -> 0.2680\n",
      "  [Scrap] Scaling RER treatment of aluminium scrap, ...: 0.0019 -> 0.0023\n",
      "  [Prim] Scaling GLO aluminium ingot, primary, to a...: 0.6960 -> 0.6325\n",
      "  [Scrap] Scaling RER treatment of aluminium scrap, ...: 0.0253 -> 0.0318\n",
      "  [Scrap] Scaling RoW treatment of aluminium scrap, ...: 0.0551 -> 0.0693\n",
      "  [Scrap] Scaling RoW treatment of aluminium scrap, ...: 0.2217 -> 0.2790\n",
      "  [Scrap] Scaling RER treatment of aluminium scrap, ...: 0.0019 -> 0.0024\n",
      "  [Prim] Scaling GLO aluminium ingot, primary, to a...: 0.6960 -> 0.6175\n",
      "  [Scrap] Scaling RER treatment of aluminium scrap, ...: 0.0253 -> 0.0349\n",
      "  [Scrap] Scaling RoW treatment of aluminium scrap, ...: 0.0551 -> 0.0761\n",
      "  [Scrap] Scaling RoW treatment of aluminium scrap, ...: 0.2217 -> 0.3063\n",
      "  [Scrap] Scaling RER treatment of aluminium scrap, ...: 0.0019 -> 0.0027\n",
      "  [Prim] Scaling GLO aluminium ingot, primary, to a...: 0.6960 -> 0.5800\n",
      "  [Scrap] Scaling RER treatment of aluminium scrap, ...: 0.0253 -> 0.0366\n",
      "  [Scrap] Scaling RoW treatment of aluminium scrap, ...: 0.0551 -> 0.0797\n",
      "  [Scrap] Scaling RoW treatment of aluminium scrap, ...: 0.2217 -> 0.3209\n",
      "  [Scrap] Scaling RER treatment of aluminium scrap, ...: 0.0019 -> 0.0028\n",
      "  [Prim] Scaling GLO aluminium ingot, primary, to a...: 0.6960 -> 0.5600\n",
      "  [Scrap] Scaling RER treatment of aluminium scrap, ...: 0.0253 -> 0.0369\n",
      "  [Scrap] Scaling RoW treatment of aluminium scrap, ...: 0.0551 -> 0.0804\n",
      "  [Scrap] Scaling RoW treatment of aluminium scrap, ...: 0.2217 -> 0.3238\n",
      "  [Scrap] Scaling RER treatment of aluminium scrap, ...: 0.0019 -> 0.0028\n",
      "  [Prim] Scaling GLO aluminium ingot, primary, to a...: 0.6960 -> 0.5560\n",
      "  [Scrap] Scaling RER treatment of aluminium scrap, ...: 0.0253 -> 0.0374\n",
      "  [Scrap] Scaling RoW treatment of aluminium scrap, ...: 0.0551 -> 0.0815\n",
      "  [Scrap] Scaling RoW treatment of aluminium scrap, ...: 0.2217 -> 0.3282\n",
      "  [Scrap] Scaling RER treatment of aluminium scrap, ...: 0.0019 -> 0.0029\n",
      "  [Prim] Scaling GLO aluminium ingot, primary, to a...: 0.6960 -> 0.5500\n",
      "  [Scrap] Scaling RER treatment of aluminium scrap, ...: 0.0253 -> 0.0312\n",
      "  [Scrap] Scaling RoW treatment of aluminium scrap, ...: 0.0551 -> 0.0681\n",
      "  [Scrap] Scaling RoW treatment of aluminium scrap, ...: 0.2217 -> 0.2741\n",
      "  [Scrap] Scaling RER treatment of aluminium scrap, ...: 0.0019 -> 0.0024\n",
      "  [Prim] Scaling GLO aluminium ingot, primary, to a...: 0.6960 -> 0.6242\n",
      "  [Scrap] Scaling RER treatment of aluminium scrap, ...: 0.0253 -> 0.0328\n",
      "  [Scrap] Scaling RoW treatment of aluminium scrap, ...: 0.0551 -> 0.0714\n",
      "  [Scrap] Scaling RoW treatment of aluminium scrap, ...: 0.2217 -> 0.2875\n",
      "  [Scrap] Scaling RER treatment of aluminium scrap, ...: 0.0019 -> 0.0025\n",
      "  [Prim] Scaling GLO aluminium ingot, primary, to a...: 0.6960 -> 0.6058\n",
      "  [Scrap] Scaling RER treatment of aluminium scrap, ...: 0.0253 -> 0.0366\n",
      "  [Scrap] Scaling RoW treatment of aluminium scrap, ...: 0.0551 -> 0.0797\n",
      "  [Scrap] Scaling RoW treatment of aluminium scrap, ...: 0.2217 -> 0.3209\n",
      "  [Scrap] Scaling RER treatment of aluminium scrap, ...: 0.0019 -> 0.0028\n",
      "  [Prim] Scaling GLO aluminium ingot, primary, to a...: 0.6960 -> 0.5600\n",
      "  [Scrap] Scaling RER treatment of aluminium scrap, ...: 0.0253 -> 0.0391\n",
      "  [Scrap] Scaling RoW treatment of aluminium scrap, ...: 0.0551 -> 0.0851\n",
      "  [Scrap] Scaling RoW treatment of aluminium scrap, ...: 0.2217 -> 0.3428\n",
      "  [Scrap] Scaling RER treatment of aluminium scrap, ...: 0.0019 -> 0.0030\n",
      "  [Prim] Scaling GLO aluminium ingot, primary, to a...: 0.6960 -> 0.5300\n",
      "  [Scrap] Scaling RER treatment of aluminium scrap, ...: 0.0253 -> 0.0394\n",
      "  [Scrap] Scaling RoW treatment of aluminium scrap, ...: 0.0551 -> 0.0859\n",
      "  [Scrap] Scaling RoW treatment of aluminium scrap, ...: 0.2217 -> 0.3457\n",
      "  [Scrap] Scaling RER treatment of aluminium scrap, ...: 0.0019 -> 0.0030\n",
      "  [Prim] Scaling GLO aluminium ingot, primary, to a...: 0.6960 -> 0.5260\n",
      "  [Scrap] Scaling RER treatment of aluminium scrap, ...: 0.0253 -> 0.0399\n",
      "  [Scrap] Scaling RoW treatment of aluminium scrap, ...: 0.0551 -> 0.0870\n",
      "  [Scrap] Scaling RoW treatment of aluminium scrap, ...: 0.2217 -> 0.3501\n",
      "  [Scrap] Scaling RER treatment of aluminium scrap, ...: 0.0019 -> 0.0030\n",
      "  [Prim] Scaling GLO aluminium ingot, primary, to a...: 0.6960 -> 0.5200\n",
      "  [Scrap] Scaling RER treatment of aluminium scrap, ...: 0.0253 -> 0.0309\n",
      "  [Scrap] Scaling RoW treatment of aluminium scrap, ...: 0.0551 -> 0.0673\n",
      "  [Scrap] Scaling RoW treatment of aluminium scrap, ...: 0.2217 -> 0.2711\n",
      "  [Scrap] Scaling RER treatment of aluminium scrap, ...: 0.0019 -> 0.0024\n",
      "  [Prim] Scaling GLO aluminium ingot, primary, to a...: 0.6960 -> 0.6283\n",
      "  [Scrap] Scaling RER treatment of aluminium scrap, ...: 0.0253 -> 0.0323\n",
      "  [Scrap] Scaling RoW treatment of aluminium scrap, ...: 0.0551 -> 0.0703\n",
      "  [Scrap] Scaling RoW treatment of aluminium scrap, ...: 0.2217 -> 0.2832\n",
      "  [Scrap] Scaling RER treatment of aluminium scrap, ...: 0.0019 -> 0.0025\n",
      "  [Prim] Scaling GLO aluminium ingot, primary, to a...: 0.6960 -> 0.6117\n",
      "  [Scrap] Scaling RER treatment of aluminium scrap, ...: 0.0253 -> 0.0357\n",
      "  [Scrap] Scaling RoW treatment of aluminium scrap, ...: 0.0551 -> 0.0779\n",
      "  [Scrap] Scaling RoW treatment of aluminium scrap, ...: 0.2217 -> 0.3136\n",
      "  [Scrap] Scaling RER treatment of aluminium scrap, ...: 0.0019 -> 0.0027\n",
      "  [Prim] Scaling GLO aluminium ingot, primary, to a...: 0.6960 -> 0.5700\n",
      "  [Scrap] Scaling RER treatment of aluminium scrap, ...: 0.0253 -> 0.0374\n",
      "  [Scrap] Scaling RoW treatment of aluminium scrap, ...: 0.0551 -> 0.0815\n",
      "  [Scrap] Scaling RoW treatment of aluminium scrap, ...: 0.2217 -> 0.3282\n",
      "  [Scrap] Scaling RER treatment of aluminium scrap, ...: 0.0019 -> 0.0029\n",
      "  [Prim] Scaling GLO aluminium ingot, primary, to a...: 0.6960 -> 0.5500\n",
      "  [Scrap] Scaling RER treatment of aluminium scrap, ...: 0.0253 -> 0.0384\n",
      "  [Scrap] Scaling RoW treatment of aluminium scrap, ...: 0.0551 -> 0.0837\n",
      "  [Scrap] Scaling RoW treatment of aluminium scrap, ...: 0.2217 -> 0.3370\n",
      "  [Scrap] Scaling RER treatment of aluminium scrap, ...: 0.0019 -> 0.0029\n",
      "  [Prim] Scaling GLO aluminium ingot, primary, to a...: 0.6960 -> 0.5380\n",
      "  [Scrap] Scaling RER treatment of aluminium scrap, ...: 0.0253 -> 0.0399\n",
      "  [Scrap] Scaling RoW treatment of aluminium scrap, ...: 0.0551 -> 0.0870\n",
      "  [Scrap] Scaling RoW treatment of aluminium scrap, ...: 0.2217 -> 0.3501\n",
      "  [Scrap] Scaling RER treatment of aluminium scrap, ...: 0.0019 -> 0.0030\n",
      "  [Prim] Scaling GLO aluminium ingot, primary, to a...: 0.6960 -> 0.5200\n"
     ]
    }
   ],
   "source": [
    "\n",
    "shares_secondary_alu = { # from IAI Report \"Aluminium Sector Greenhouse Gas Pathways to 2050\"\n",
    "    \"SSP2-PkBudg650\": { # 1.5°C scenario from original report\n",
    "        2018: 0.33,\n",
    "        2030: 0.43,\n",
    "        2035: 0.45,\n",
    "        2040: 0.48,\n",
    "        2045: 0.51,\n",
    "    },\n",
    "    \"SSP2-PkBudg1000\": { # <2°C scenario from original report\n",
    "        2018: 0.33,\n",
    "        2030: 0.44,\n",
    "        2035: 0.47,\n",
    "        2040: 0.48,\n",
    "        2045: 0.50,\n",
    "    },\n",
    "    \"SSP2-NPi\": { # BAU scenario from original report\n",
    "        2018: 0.33,\n",
    "        2030: 0.42,\n",
    "        2035: 0.44,\n",
    "        2040: 0.45,\n",
    "        2045: 0.46,\n",
    "    }\n",
    "}\n",
    "\n",
    "def extract_scenario_from_db_name(db_name: str) -> str:\n",
    "    parts = db_name.split(\"_\")\n",
    "    scenario_part = parts[1]  # e.g., \"SSP2\"\n",
    "    scenario_type_part = parts[2]  # e.g., \"NPi\", \"PkBudg1000\", \"PkBudg650\"\n",
    "    return f\"{scenario_part}-{scenario_type_part}\"\n",
    "\n",
    "def interpolate_scrap_share(year: int, scenario: str) -> float:\n",
    "    shares = shares_secondary_alu[scenario]\n",
    "    years = sorted(shares.keys()) \n",
    "    \n",
    "    if year in shares:\n",
    "        return shares[year]\n",
    "    \n",
    "    for i in range(len(years) - 1):\n",
    "        if years[i] < year < years[i + 1]:\n",
    "            year_start = years[i]\n",
    "            year_end = years[i + 1]\n",
    "            share_start = shares[year_start]\n",
    "            share_end = shares[year_end]\n",
    "            # Linear interpolation\n",
    "            interpolated_share = share_start + (share_end - share_start) * ((year - year_start) / (year_end - year_start))\n",
    "            return interpolated_share\n",
    "    \n",
    "\n",
    "for db_name, year in background_db_time_mapping.items():    \n",
    "    # 1. Group the exchanges\n",
    "    alu = bd.Database(db_name).get(name=\"market for aluminium, wrought alloy\", location=\"GLO\")\n",
    "    scrap_exchanges = [exc for exc in alu.technosphere() if \"treatment of aluminium scrap\" in exc.input['name'].lower()]\n",
    "    primary_exchanges = [exc for exc in alu.technosphere() if \"aluminium ingot, primary\" in exc.input['name'].lower()]\n",
    "    \n",
    "    # 2. Get current sub-totals to determine internal proportions\n",
    "    current_scrap_total = sum(exc.amount for exc in scrap_exchanges)\n",
    "    current_primary_total = sum(exc.amount for exc in primary_exchanges)\n",
    "    \n",
    "    # 3. Define the new target shares (Target Total = 1)\n",
    "    new_scrap_share = interpolate_scrap_share(year=year, scenario=extract_scenario_from_db_name(db_name))\n",
    "    new_primary_share = 1 - new_scrap_share\n",
    "    \n",
    "    # 4. Scale Scrap Exchanges\n",
    "    if current_scrap_total > 0:\n",
    "        for exc in scrap_exchanges:\n",
    "            proportion = exc.amount / current_scrap_total\n",
    "            new_amount = proportion * new_scrap_share\n",
    "            print(f\"  [Scrap] Scaling {exc.input['location']} {exc.input['name'][:30]}...: {exc.amount:.4f} -> {new_amount:.4f}\")\n",
    "            exc[\"amount\"] = new_amount\n",
    "            exc.save()\n",
    "\n",
    "    # 5. Scale Primary Exchanges\n",
    "    if current_primary_total > 0:\n",
    "        for exc in primary_exchanges:\n",
    "            proportion = exc.amount / current_primary_total\n",
    "            new_amount = proportion * new_primary_share\n",
    "            print(f\"  [Prim] Scaling {exc.input['location']} {exc.input['name'][:30]}...: {exc.amount:.4f} -> {new_amount:.4f}\")\n",
    "            exc[\"amount\"] = new_amount\n",
    "            exc.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating materials for faster monte carlo runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_dbs = [db for db in bd.databases if db.startswith(\"agg_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m12:00:51+0100\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mRemoving database node(s) from calculation setup uncertainty\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for db in agg_dbs:\n",
    "    del bd.databases[db]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timodiepers/anaconda3/envs/premise/lib/python3.11/site-packages/bw2calc/lca_base.py:127: SparseEfficiencyWarning: splu converted its input to CSC format\n",
      "  self.solver = factorized(self.technosphere_matrix)\n",
      "/Users/timodiepers/anaconda3/envs/premise/lib/python3.11/site-packages/scikits/umfpack/umfpack.py:736: UmfpackWarning: (almost) singular matrix! (estimated cond. number: 4.24e+12)\n",
      "  warnings.warn(msg, UmfpackWarning)\n",
      "/Users/timodiepers/anaconda3/envs/premise/lib/python3.11/site-packages/scikits/umfpack/umfpack.py:736: UmfpackWarning: (almost) singular matrix! (estimated cond. number: 2.13e+12)\n",
      "  warnings.warn(msg, UmfpackWarning)\n",
      "/Users/timodiepers/anaconda3/envs/premise/lib/python3.11/site-packages/scikits/umfpack/umfpack.py:736: UmfpackWarning: (almost) singular matrix! (estimated cond. number: 2.10e+12)\n",
      "  warnings.warn(msg, UmfpackWarning)\n",
      "/Users/timodiepers/anaconda3/envs/premise/lib/python3.11/site-packages/scikits/umfpack/umfpack.py:736: UmfpackWarning: (almost) singular matrix! (estimated cond. number: 1.16e+12)\n",
      "  warnings.warn(msg, UmfpackWarning)\n",
      "/Users/timodiepers/anaconda3/envs/premise/lib/python3.11/site-packages/scikits/umfpack/umfpack.py:736: UmfpackWarning: (almost) singular matrix! (estimated cond. number: 2.34e+12)\n",
      "  warnings.warn(msg, UmfpackWarning)\n",
      "/Users/timodiepers/anaconda3/envs/premise/lib/python3.11/site-packages/scikits/umfpack/umfpack.py:736: UmfpackWarning: (almost) singular matrix! (estimated cond. number: 2.57e+12)\n",
      "  warnings.warn(msg, UmfpackWarning)\n",
      "/Users/timodiepers/anaconda3/envs/premise/lib/python3.11/site-packages/scikits/umfpack/umfpack.py:736: UmfpackWarning: (almost) singular matrix! (estimated cond. number: 1.08e+13)\n",
      "  warnings.warn(msg, UmfpackWarning)\n",
      "/Users/timodiepers/anaconda3/envs/premise/lib/python3.11/site-packages/scikits/umfpack/umfpack.py:736: UmfpackWarning: (almost) singular matrix! (estimated cond. number: 1.07e+13)\n",
      "  warnings.warn(msg, UmfpackWarning)\n"
     ]
    }
   ],
   "source": [
    "import bw2calc as bc\n",
    "\n",
    "method = ('IPCC 2021', 'climate change', 'GWP 100a, incl. H and bio CO2')\n",
    "co2 = bd.get_node(database=\"ecoinvent-3.10.1-biosphere\", name=\"Carbon dioxide, fossil\", categories=(\"air\",))\n",
    "\n",
    "for db_name in background_db_time_mapping.keys():\n",
    "    bg_inputs = list(load_inputs_from_background(db_name).values())\n",
    "    new_db = bd.Database(\"agg_\" + db_name)\n",
    "    new_db.register()\n",
    "    lca = bc.LCA({bg_inputs[0]: 1}, method=method)\n",
    "    lca.lci(factorize=True)\n",
    "    for inp in bg_inputs:\n",
    "        lca.redo_lcia(demand={inp.id: 1})\n",
    "        agg_act = new_db.new_node(name=\"agg\" + inp['name'])\n",
    "        agg_act.save()\n",
    "        agg_act.new_edge(input=agg_act, amount=1, type=\"production\").save()\n",
    "        agg_act.new_edge(input=co2, amount=lca.score, type=\"biosphere\").save()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_dbs = [ \n",
    " 'grid_components_NPi_2023',\n",
    " 'grid_components_NPi_2025',\n",
    " 'grid_components_NPi_2030',\n",
    " 'grid_components_NPi_2035',\n",
    " 'grid_components_NPi_2037',\n",
    " 'grid_components_NPi_2040',\n",
    " 'grid_components_PkBudg1000_2023',\n",
    " 'grid_components_PkBudg1000_2025',\n",
    " 'grid_components_PkBudg1000_2030',\n",
    " 'grid_components_PkBudg1000_2035',\n",
    " 'grid_components_PkBudg1000_2037',\n",
    " 'grid_components_PkBudg1000_2040',\n",
    " 'grid_components_PkBudg650_2023',\n",
    " 'grid_components_PkBudg650_2025',\n",
    " 'grid_components_PkBudg650_2030',\n",
    " 'grid_components_PkBudg650_2035',\n",
    " 'grid_components_PkBudg650_2037',\n",
    " 'grid_components_PkBudg650_2040',]\n",
    "\n",
    "# for db in comp_dbs:\n",
    "#     del bd.databases[\"agg_\" + db]\n",
    "    \n",
    "for db_name in comp_dbs:\n",
    "    new_db = bd.Database(\"agg_\" + db_name)\n",
    "    new_db.register()\n",
    "    for act in bd.Database(db_name):\n",
    "        try:\n",
    "            new_db.get(name=\"agg\" + act['name'])\n",
    "            print(f\"Aggregated activity for {act['name'], db_name} already exists, skipping...\")\n",
    "            continue\n",
    "        except:\n",
    "            pass\n",
    "        new_act = new_db.new_node(name=\"agg\" + act['name'])\n",
    "        new_act.save()\n",
    "        new_act.new_edge(input=new_act, amount=1, type=\"production\").save()\n",
    "        for exc in act.technosphere():\n",
    "            try:\n",
    "                agg_input = bd.get_node(database=\"agg_\" + exc.input['database'], name=\"agg\" + exc.input['name'])\n",
    "            except:\n",
    "                print(f\"Input {exc.input['name']} not found in agg_{exc.input['database']}, skipping...\")\n",
    "                raise\n",
    "            new_act.new_edge(input=agg_input, amount=exc.amount, type=\"technosphere\").save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created aggregated grid expansion nodes with parameterized exchanges.\n"
     ]
    }
   ],
   "source": [
    "if \"agg_grid_expansion_prospective\" in bd.databases:\n",
    "    del bd.databases[\"agg_grid_expansion_prospective\"]\n",
    "\n",
    "# Clean up old parameter group if it exists\n",
    "from bw2data.parameters import Group, ActivityParameter, ParameterizedExchange\n",
    "agg_param_group = \"agg_grid_expansion_prospective_params\"\n",
    "try:\n",
    "    group = Group.get(name=agg_param_group)\n",
    "    ActivityParameter.delete().where(ActivityParameter.group == agg_param_group).execute()\n",
    "    ParameterizedExchange.delete().where(ParameterizedExchange.group == agg_param_group).execute()\n",
    "    group.delete_instance()\n",
    "    print(f\"Cleaned up old group: {agg_param_group}\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "new_db = bd.Database(\"agg_grid_expansion_prospective\")\n",
    "new_db.register()\n",
    "\n",
    "old_expansion_db = bd.Database(\"grid_expansion_prospective\")\n",
    "\n",
    "for act in old_expansion_db:\n",
    "    new_act = new_db.new_node(name=\"agg\" + act['name'])  # grid node for a specific year\n",
    "    new_act.save()\n",
    "    new_act.new_edge(input=new_act, amount=1, type=\"production\").save()\n",
    "    \n",
    "    has_formula = False\n",
    "    for exc in act.technosphere():\n",
    "        agg_input = bd.get_node(database=\"agg_\" + exc.input['database'], name=\"agg\" + exc.input['name'])\n",
    "        \n",
    "        # Copy the formula if it exists, along with the amount\n",
    "        if exc.get(\"formula\"):\n",
    "            new_act.new_edge(\n",
    "                input=agg_input, \n",
    "                amount=exc.amount, \n",
    "                type=\"technosphere\",\n",
    "                formula=exc[\"formula\"]\n",
    "            ).save()\n",
    "            has_formula = True\n",
    "        else:\n",
    "            new_act.new_edge(\n",
    "                input=agg_input, \n",
    "                amount=exc.amount, \n",
    "                type=\"technosphere\"\n",
    "            ).save()\n",
    "    \n",
    "    # Add to parameter group if this activity has any formulas\n",
    "    if has_formula:\n",
    "        bd.parameters.add_exchanges_to_group(agg_param_group, new_act)\n",
    "\n",
    "# Recalculate the aggregated parameter group\n",
    "try:\n",
    "    ActivityParameter.recalculate(agg_param_group)\n",
    "    print(f\"Created aggregated grid expansion nodes with parameterized exchanges.\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: Parameter recalculation skipped or failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding uncertainty info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup\n",
    "for sub_period in bd.Database(\"agg_grid_expansion_prospective\"):\n",
    "        for comp_exc in sub_period.technosphere():\n",
    "            for prop in [\"uncertainty type\", \"loc\", \"scale\", \"shape\", \"minimum\", \"maximum\"]:\n",
    "                if comp_exc.get(prop):\n",
    "                    del comp_exc[prop]\n",
    "            comp_exc.save()\n",
    "            for mat_exc in comp_exc.input.technosphere():\n",
    "                for prop in [\"uncertainty type\", \"loc\", \"scale\", \"shape\", \"minimum\", \"maximum\"]:\n",
    "                    if mat_exc.get(prop):\n",
    "                        del mat_exc[prop]\n",
    "                mat_exc.save()\n",
    "                for bios_exc in mat_exc.input.biosphere():\n",
    "                    for prop in [\"uncertainty type\", \"loc\", \"scale\", \"shape\", \"minimum\", \"maximum\"]:\n",
    "                        if bios_exc.get(prop):\n",
    "                            del bios_exc[prop]\n",
    "                    bios_exc.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uncertainty_factor(year, base_uncertainty=0.1, final_uncertainty=0.35, start_year=2023, end_year=2045):\n",
    "        years_total = end_year - start_year\n",
    "        years_elapsed = year - start_year\n",
    "        uncertainty_factor = base_uncertainty + (final_uncertainty - base_uncertainty) * (years_elapsed / years_total)\n",
    "        return uncertainty_factor\n",
    "    \n",
    "for sub_period in bd.Database(\"agg_grid_expansion_prospective\"):\n",
    "    \n",
    "    if not sub_period[\"name\"].startswith(\"agggrid_\"):\n",
    "        continue\n",
    "    \n",
    "    year = int(sub_period['name'].split(\"_\")[-1])\n",
    "    for comp_exc in sub_period.technosphere(): # GRID EXPANSION NUMBERS\n",
    "        comp_exc[\"uncertainty type\"] = 5 # Triangular distribution\n",
    "        comp_exc[\"loc\"] = comp_exc.amount\n",
    "        comp_exc[\"scale\"] = np.nan\n",
    "        comp_exc[\"shape\"] = np.nan\n",
    "        mini = comp_exc.amount - comp_exc.amount * get_uncertainty_factor(year, base_uncertainty=0.05, final_uncertainty=0.20)\n",
    "        maxi = comp_exc.amount + comp_exc.amount * get_uncertainty_factor(year, base_uncertainty=0.05, final_uncertainty=0.20)\n",
    "        if comp_exc.amount < 0:\n",
    "            mini, maxi = maxi, mini  # swap for negative amounts\n",
    "        assert mini <= maxi, f\"Minimum {mini} is not less than maximum {maxi} for component {comp_exc.input['name']} in year {year}\"\n",
    "        comp_exc[\"minimum\"] = mini\n",
    "        comp_exc[\"maximum\"] = maxi\n",
    "        comp_exc.save()\n",
    "        \n",
    "        for mat_exc in comp_exc.input.technosphere(): # COMPONENT MATERIAL DEMANDS\n",
    "            mat_exc[\"uncertainty type\"] = 5 # Triangular distribution\n",
    "            mat_exc[\"loc\"] = mat_exc.amount\n",
    "            mat_exc[\"scale\"] = np.nan\n",
    "            mat_exc[\"shape\"] = np.nan\n",
    "            mini = mat_exc.amount - mat_exc.amount * get_uncertainty_factor(year, base_uncertainty=0.01, final_uncertainty=0.05)\n",
    "            maxi = mat_exc.amount + mat_exc.amount * get_uncertainty_factor(year, base_uncertainty=0.01, final_uncertainty=0.05)\n",
    "            if mat_exc.amount < 0:\n",
    "                mini, maxi = maxi, mini  # swap for negative amounts\n",
    "            assert mini <= maxi, f\"Minimum {mini} is not less than maximum {maxi} for material {mat_exc.input['name']} in year {year}\"\n",
    "            mat_exc[\"minimum\"] = mini\n",
    "            mat_exc[\"maximum\"] = maxi\n",
    "            mat_exc.save()\n",
    "            \n",
    "            for emission in mat_exc.input.biosphere(): # MATERIAL GHG EMISSIONS\n",
    "                emission[\"uncertainty type\"] = 5 # Triangular distribution\n",
    "                emission[\"loc\"] = emission.amount\n",
    "                emission[\"scale\"] = np.nan\n",
    "                emission[\"shape\"] = np.nan\n",
    "                mini = emission.amount - emission.amount * get_uncertainty_factor(year, base_uncertainty=0.05, final_uncertainty=0.2)\n",
    "                maxi = emission.amount + emission.amount * get_uncertainty_factor(year, base_uncertainty=0.05, final_uncertainty=0.2)\n",
    "                if emission.amount < 0:\n",
    "                    mini, maxi = maxi, mini  # swap for negative amounts\n",
    "                assert mini <= maxi, f\"Minimum {mini} is not less than maximum {maxi} for emission {emission.input['name']} in year {year}\"\n",
    "                emission[\"minimum\"] = mini\n",
    "                emission[\"maximum\"] = maxi\n",
    "                emission.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating aggregated grid expansion nodes over all expansion periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in db:\n",
    "    if \"expansion\" in node['name']:\n",
    "        node.delete()\n",
    "\n",
    "exp_npi = db.new_node(name=\"expansion_NPi\")\n",
    "exp_npi.new_edge(input=exp_npi, amount=1, type=\"production\").save()\n",
    "\n",
    "exp_pkbudg1000 = db.new_node(name=\"expansion_PkBudg1000\")\n",
    "exp_pkbudg1000.new_edge(input=exp_pkbudg1000, amount=1, type=\"production\").save()\n",
    "\n",
    "exp_pkbudg650 = db.new_node(name=\"expansion_PkBudg650\")\n",
    "exp_pkbudg650.new_edge(input=exp_pkbudg650, amount=1, type=\"production\").save()\n",
    "\n",
    "for node in db:\n",
    "    if node['name'].startswith(\"agggrid_NPi_\"):\n",
    "        exp_npi.new_edge(input=node, amount=1, type=\"technosphere\").save()\n",
    "    elif node['name'].startswith(\"agggrid_PkBudg1000_\"):\n",
    "        exp_pkbudg1000.new_edge(input=node, amount=1, type=\"technosphere\").save()\n",
    "    elif node['name'].startswith(\"agggrid_PkBudg650_\"):\n",
    "        exp_pkbudg650.new_edge(input=node, amount=1, type=\"technosphere\").save()\n",
    "\n",
    "exp_npi.save()\n",
    "exp_pkbudg1000.save()\n",
    "exp_pkbudg650.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "premise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
